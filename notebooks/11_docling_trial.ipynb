{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install docling"
      ],
      "metadata": {
        "id": "bnxFefI2On33",
        "outputId": "d265324e-9b2f-44ae-da71-1f523d980e65",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "bnxFefI2On33",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting docling\n",
            "  Downloading docling-2.28.4-py3-none-any.whl.metadata (10.0 kB)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.11/dist-packages (from docling) (4.13.3)\n",
            "Requirement already satisfied: certifi>=2024.7.4 in /usr/local/lib/python3.11/dist-packages (from docling) (2025.1.31)\n",
            "Collecting docling-core<3.0.0,>=2.24.1 (from docling-core[chunking]<3.0.0,>=2.24.1->docling)\n",
            "  Downloading docling_core-2.25.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting docling-ibm-models<4.0.0,>=3.4.0 (from docling)\n",
            "  Downloading docling_ibm_models-3.4.1-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting docling-parse<5.0.0,>=4.0.0 (from docling)\n",
            "  Downloading docling_parse-4.0.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Collecting easyocr<2.0,>=1.7 (from docling)\n",
            "  Downloading easyocr-1.7.2-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from docling)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: huggingface_hub<1,>=0.23 in /usr/local/lib/python3.11/dist-packages (from docling) (0.30.1)\n",
            "Requirement already satisfied: lxml<6.0.0,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from docling) (5.3.1)\n",
            "Collecting marko<3.0.0,>=2.1.2 (from docling)\n",
            "  Downloading marko-2.1.3-py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: openpyxl<4.0.0,>=3.1.5 in /usr/local/lib/python3.11/dist-packages (from docling) (3.1.5)\n",
            "Requirement already satisfied: pandas<3.0.0,>=2.1.4 in /usr/local/lib/python3.11/dist-packages (from docling) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0.0,>=10.0.0 in /usr/local/lib/python3.11/dist-packages (from docling) (11.1.0)\n",
            "Requirement already satisfied: pluggy<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from docling) (1.5.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from docling) (2.11.1)\n",
            "Collecting pydantic-settings<3.0.0,>=2.3.0 (from docling)\n",
            "  Downloading pydantic_settings-2.8.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting pylatexenc<3.0,>=2.10 (from docling)\n",
            "  Downloading pylatexenc-2.10.tar.gz (162 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.6/162.6 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pypdfium2<5.0.0,>=4.30.0 (from docling)\n",
            "  Downloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-docx<2.0.0,>=1.1.2 (from docling)\n",
            "  Downloading python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting python-pptx<2.0.0,>=1.0.2 (from docling)\n",
            "  Downloading python_pptx-1.0.2-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from docling) (2.32.3)\n",
            "Collecting rtree<2.0.0,>=1.3.0 (from docling)\n",
            "  Downloading rtree-1.4.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: scipy<2.0.0,>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from docling) (1.14.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.65.0 in /usr/local/lib/python3.11/dist-packages (from docling) (4.67.1)\n",
            "Collecting typer<0.13.0,>=0.12.5 (from docling)\n",
            "  Downloading typer-0.12.5-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->docling) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->docling) (4.13.0)\n",
            "Collecting jsonref<2.0.0,>=1.1.0 (from docling-core<3.0.0,>=2.24.1->docling-core[chunking]<3.0.0,>=2.24.1->docling)\n",
            "  Downloading jsonref-1.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: jsonschema<5.0.0,>=4.16.0 in /usr/local/lib/python3.11/dist-packages (from docling-core<3.0.0,>=2.24.1->docling-core[chunking]<3.0.0,>=2.24.1->docling) (4.23.0)\n",
            "Collecting latex2mathml<4.0.0,>=3.77.0 (from docling-core<3.0.0,>=2.24.1->docling-core[chunking]<3.0.0,>=2.24.1->docling)\n",
            "  Downloading latex2mathml-3.77.0-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.1 in /usr/local/lib/python3.11/dist-packages (from docling-core<3.0.0,>=2.24.1->docling-core[chunking]<3.0.0,>=2.24.1->docling) (6.0.2)\n",
            "Requirement already satisfied: tabulate<0.10.0,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from docling-core<3.0.0,>=2.24.1->docling-core[chunking]<3.0.0,>=2.24.1->docling) (0.9.0)\n",
            "Collecting semchunk<3.0.0,>=2.2.0 (from docling-core[chunking]<3.0.0,>=2.24.1->docling)\n",
            "  Downloading semchunk-2.2.2-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.11/dist-packages (from docling-core[chunking]<3.0.0,>=2.24.1->docling) (4.50.3)\n",
            "Collecting jsonlines<4.0.0,>=3.1.0 (from docling-ibm-models<4.0.0,>=3.4.0->docling)\n",
            "  Downloading jsonlines-3.1.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.24.4 in /usr/local/lib/python3.11/dist-packages (from docling-ibm-models<4.0.0,>=3.4.0->docling) (2.0.2)\n",
            "Requirement already satisfied: opencv-python-headless<5.0.0.0,>=4.6.0.66 in /usr/local/lib/python3.11/dist-packages (from docling-ibm-models<4.0.0,>=3.4.0->docling) (4.11.0.86)\n",
            "Requirement already satisfied: safetensors<1,>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from safetensors[torch]<1,>=0.4.3->docling-ibm-models<4.0.0,>=3.4.0->docling) (0.5.3)\n",
            "Requirement already satisfied: torch<3.0.0,>=2.2.2 in /usr/local/lib/python3.11/dist-packages (from docling-ibm-models<4.0.0,>=3.4.0->docling) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision<1,>=0 in /usr/local/lib/python3.11/dist-packages (from docling-ibm-models<4.0.0,>=3.4.0->docling) (0.21.0+cu124)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (from easyocr<2.0,>=1.7->docling) (0.25.2)\n",
            "Collecting python-bidi (from easyocr<2.0,>=1.7->docling)\n",
            "  Downloading python_bidi-0.6.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.11/dist-packages (from easyocr<2.0,>=1.7->docling) (2.0.7)\n",
            "Collecting pyclipper (from easyocr<2.0,>=1.7->docling)\n",
            "  Downloading pyclipper-1.3.0.post6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n",
            "Collecting ninja (from easyocr<2.0,>=1.7->docling)\n",
            "  Downloading ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub<1,>=0.23->docling) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub<1,>=0.23->docling) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub<1,>=0.23->docling) (24.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl<4.0.0,>=3.1.5->docling) (2.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0.0,>=2.1.4->docling) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0.0,>=2.1.4->docling) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0.0,>=2.1.4->docling) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.0.0->docling) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.0.0->docling) (2.33.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.0.0->docling) (0.4.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.3.0->docling)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting XlsxWriter>=0.5.7 (from python-pptx<2.0.0,>=1.0.2->docling)\n",
            "  Downloading XlsxWriter-3.2.2-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.32.2->docling) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.32.2->docling) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.32.2->docling) (2.3.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<0.13.0,>=0.12.5->docling) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<0.13.0,>=0.12.5->docling) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<0.13.0,>=0.12.5->docling) (13.9.4)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonlines<4.0.0,>=3.1.0->docling-ibm-models<4.0.0,>=3.4.0->docling) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.16.0->docling-core<3.0.0,>=2.24.1->docling-core[chunking]<3.0.0,>=2.24.1->docling) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.16.0->docling-core<3.0.0,>=2.24.1->docling-core[chunking]<3.0.0,>=2.24.1->docling) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.16.0->docling-core<3.0.0,>=2.24.1->docling-core[chunking]<3.0.0,>=2.24.1->docling) (0.24.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0.0,>=2.1.4->docling) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<0.13.0,>=0.12.5->docling) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<0.13.0,>=0.12.5->docling) (2.18.0)\n",
            "Collecting mpire[dill] (from semchunk<3.0.0,>=2.2.0->docling-core[chunking]<3.0.0,>=2.24.1->docling)\n",
            "  Downloading mpire-2.10.2-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.2.2->docling-ibm-models<4.0.0,>=3.4.0->docling) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.2.2->docling-ibm-models<4.0.0,>=3.4.0->docling) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<3.0.0,>=2.2.2->docling-ibm-models<4.0.0,>=3.4.0->docling)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<3.0.0,>=2.2.2->docling-ibm-models<4.0.0,>=3.4.0->docling)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<3.0.0,>=2.2.2->docling-ibm-models<4.0.0,>=3.4.0->docling)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3.0.0,>=2.2.2->docling-ibm-models<4.0.0,>=3.4.0->docling)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<3.0.0,>=2.2.2->docling-ibm-models<4.0.0,>=3.4.0->docling)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<3.0.0,>=2.2.2->docling-ibm-models<4.0.0,>=3.4.0->docling)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<3.0.0,>=2.2.2->docling-ibm-models<4.0.0,>=3.4.0->docling)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3.0.0,>=2.2.2->docling-ibm-models<4.0.0,>=3.4.0->docling)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3.0.0,>=2.2.2->docling-ibm-models<4.0.0,>=3.4.0->docling)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.2.2->docling-ibm-models<4.0.0,>=3.4.0->docling) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.2.2->docling-ibm-models<4.0.0,>=3.4.0->docling) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.2.2->docling-ibm-models<4.0.0,>=3.4.0->docling) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3.0.0,>=2.2.2->docling-ibm-models<4.0.0,>=3.4.0->docling)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.2.2->docling-ibm-models<4.0.0,>=3.4.0->docling) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.2.2->docling-ibm-models<4.0.0,>=3.4.0->docling) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3.0.0,>=2.2.2->docling-ibm-models<4.0.0,>=3.4.0->docling) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.34.0->docling-core[chunking]<3.0.0,>=2.24.1->docling) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.34.0->docling-core[chunking]<3.0.0,>=2.24.1->docling) (0.21.1)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image->easyocr<2.0,>=1.7->docling) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image->easyocr<2.0,>=1.7->docling) (2025.3.30)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image->easyocr<2.0,>=1.7->docling) (0.4)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<0.13.0,>=0.12.5->docling) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3.0.0,>=2.2.2->docling-ibm-models<4.0.0,>=3.4.0->docling) (3.0.2)\n",
            "Collecting multiprocess>=0.70.15 (from mpire[dill]->semchunk<3.0.0,>=2.2.0->docling-core[chunking]<3.0.0,>=2.24.1->docling)\n",
            "  Downloading multiprocess-0.70.17-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting dill>=0.3.9 (from multiprocess>=0.70.15->mpire[dill]->semchunk<3.0.0,>=2.2.0->docling-core[chunking]<3.0.0,>=2.24.1->docling)\n",
            "  Downloading dill-0.3.9-py3-none-any.whl.metadata (10 kB)\n",
            "Downloading docling-2.28.4-py3-none-any.whl (160 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.4/160.4 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docling_core-2.25.0-py3-none-any.whl (121 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.9/121.9 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docling_ibm_models-3.4.1-py3-none-any.whl (80 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.9/80.9 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docling_parse-4.0.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.1/15.1 MB\u001b[0m \u001b[31m94.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading easyocr-1.7.2-py3-none-any.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m100.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading marko-2.1.3-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.8.1-py3-none-any.whl (30 kB)\n",
            "Downloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m91.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_pptx-1.0.2-py3-none-any.whl (472 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m472.8/472.8 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rtree-1.4.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (541 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m541.1/541.1 kB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typer-0.12.5-py3-none-any.whl (47 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.3/47.3 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonlines-3.1.0-py3-none-any.whl (8.6 kB)\n",
            "Downloading jsonref-1.1.0-py3-none-any.whl (9.4 kB)\n",
            "Downloading latex2mathml-3.77.0-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.7/73.7 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading semchunk-2.2.2-py3-none-any.whl (10 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m113.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m85.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m81.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading XlsxWriter-3.2.2-py3-none-any.whl (165 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m165.1/165.1 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.8/422.8 kB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyclipper-1.3.0.post6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (969 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m969.6/969.6 kB\u001b[0m \u001b[31m60.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_bidi-0.6.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (292 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m292.9/292.9 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.17-py311-none-any.whl (144 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.3/144.3 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mpire-2.10.2-py3-none-any.whl (272 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m272.8/272.8 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.9-py3-none-any.whl (119 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.4/119.4 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pylatexenc\n",
            "  Building wheel for pylatexenc (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pylatexenc: filename=pylatexenc-2.10-py3-none-any.whl size=136817 sha256=d86f3033893fa87754f91aa97adeb5c83c2c4ad6dbb3840b13dd4b00577dc965\n",
            "  Stored in directory: /root/.cache/pip/wheels/b1/7a/33/9fdd892f784ed4afda62b685ae3703adf4c91aa0f524c28f03\n",
            "Successfully built pylatexenc\n",
            "Installing collected packages: python-bidi, pylatexenc, pyclipper, filetype, XlsxWriter, rtree, python-dotenv, python-docx, pypdfium2, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, ninja, mpire, marko, latex2mathml, jsonref, jsonlines, dill, python-pptx, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, typer, pydantic-settings, nvidia-cusolver-cu12, semchunk, docling-core, docling-parse, easyocr, docling-ibm-models, docling\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from docling.document_converter import DocumentConverter"
      ],
      "metadata": {
        "id": "Ktli86j3OZwc"
      },
      "id": "Ktli86j3OZwc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "source = \"https://arxiv.org/pdf/2408.09869\"  # document per local path or URL\n",
        "converter = DocumentConverter()\n",
        "\n",
        "result = converter.convert(source)\n",
        "\n",
        "print(result.document.export_to_markdown())\n",
        "# output: ## Docling Technical Report [...]\""
      ],
      "metadata": {
        "id": "tk6pK27GOjNs",
        "outputId": "b113fe67-2565-4341-9e84-748d306bb712",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "tk6pK27GOjNs",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<!-- image -->\n",
            "\n",
            "## Docling Technical Report\n",
            "\n",
            "Version 1.0\n",
            "\n",
            "Christoph Auer Maksym Lysak Ahmed Nassar Michele Dolfi Nikolaos Livathinos Panos Vagenas Cesar Berrospi Ramis Matteo Omenetti Fabian Lindlbauer Kasper Dinkla Lokesh Mishra Yusik Kim Shubham Gupta Rafael Teixeira de Lima Valery Weber Lucas Morin Ingmar Meijer Viktor Kuropiatnyk Peter W. J. Staar\n",
            "\n",
            "AI4K Group, IBM Research R¨ uschlikon, Switzerland\n",
            "\n",
            "## Abstract\n",
            "\n",
            "This technical report introduces Docling , an easy to use, self-contained, MITlicensed open-source package for PDF document conversion. It is powered by state-of-the-art specialized AI models for layout analysis (DocLayNet) and table structure recognition (TableFormer), and runs efficiently on commodity hardware in a small resource budget. The code interface allows for easy extensibility and addition of new features and models.\n",
            "\n",
            "## 1 Introduction\n",
            "\n",
            "Converting PDF documents back into a machine-processable format has been a major challenge for decades due to their huge variability in formats, weak standardization and printing-optimized characteristic, which discards most structural features and metadata. With the advent of LLMs and popular application patterns such as retrieval-augmented generation (RAG), leveraging the rich content embedded in PDFs has become ever more relevant. In the past decade, several powerful document understanding solutions have emerged on the market, most of which are commercial software, cloud offerings [3] and most recently, multi-modal vision-language models. As of today, only a handful of open-source tools cover PDF conversion, leaving a significant feature and quality gap to proprietary solutions.\n",
            "\n",
            "With Docling , we open-source a very capable and efficient document conversion tool which builds on the powerful, specialized AI models and datasets for layout analysis and table structure recognition we developed and presented in the recent past [12, 13, 9]. Docling is designed as a simple, self-contained python library with permissive license, running entirely locally on commodity hardware. Its code architecture allows for easy extensibility and addition of new features and models.\n",
            "\n",
            "Here is what Docling delivers today:\n",
            "\n",
            "- · Converts PDF documents to JSON or Markdown format, stable and lightning fast\n",
            "- · Understands detailed page layout, reading order, locates figures and recovers table structures\n",
            "- · Extracts metadata from the document, such as title, authors, references and language\n",
            "- · Optionally applies OCR, e.g. for scanned PDFs\n",
            "- · Can be configured to be optimal for batch-mode (i.e high throughput, low time-to-solution) or interactive mode (compromise on efficiency, low time-to-solution)\n",
            "- · Can leverage different accelerators (GPU, MPS, etc).\n",
            "\n",
            "## 2 Getting Started\n",
            "\n",
            "To use Docling, you can simply install the docling package from PyPI. Documentation and examples are available in our GitHub repository at github.com/DS4SD/docling. All required model assets 1 are downloaded to a local huggingface datasets cache on first use, unless you choose to pre-install the model assets in advance.\n",
            "\n",
            "Docling provides an easy code interface to convert PDF documents from file system, URLs or binary streams, and retrieve the output in either JSON or Markdown format. For convenience, separate methods are offered to convert single documents or batches of documents. A basic usage example is illustrated below. Further examples are available in the Doclign code repository.\n",
            "\n",
            "```\n",
            "from docling.document_converter import DocumentConverter Large\n",
            "```\n",
            "\n",
            "```\n",
            "source = \"https://arxiv.org/pdf/2206.01062\" # PDF path or URL converter = DocumentConverter() result = converter.convert_single(source) print(result.render_as_markdown()) # output: \"## DocLayNet: A Human -Annotated Dataset for Document -Layout Analysis [...]\"\n",
            "```\n",
            "\n",
            "Optionally, you can configure custom pipeline features and runtime options, such as turning on or off features (e.g. OCR, table structure recognition), enforcing limits on the input document size, and defining the budget of CPU threads. Advanced usage examples and options are documented in the README file. Docling also provides a Dockerfile to demonstrate how to install and run it inside a container.\n",
            "\n",
            "## 3 Processing pipeline\n",
            "\n",
            "Docling implements a linear pipeline of operations, which execute sequentially on each given document (see Fig. 1). Each document is first parsed by a PDF backend, which retrieves the programmatic text tokens, consisting of string content and its coordinates on the page, and also renders a bitmap image of each page to support downstream operations. Then, the standard model pipeline applies a sequence of AI models independently on every page in the document to extract features and content, such as layout and table structures. Finally, the results from all pages are aggregated and passed through a post-processing stage, which augments metadata, detects the document language, infers reading-order and eventually assembles a typed document object which can be serialized to JSON or Markdown.\n",
            "\n",
            "## 3.1 PDF backends\n",
            "\n",
            "Two basic requirements to process PDF documents in our pipeline are a) to retrieve all text content and their geometric coordinates on each page and b) to render the visual representation of each page as it would appear in a PDF viewer. Both these requirements are encapsulated in Docling's PDF backend interface. While there are several open-source PDF parsing libraries available for python, we faced major obstacles with all of them for different reasons, among which were restrictive\n",
            "\n",
            "1 see huggingface.co/ds4sd/docling-models/\n",
            "\n",
            "Figure 1: Sketch of Docling's default processing pipeline. The inner part of the model pipeline is easily customizable and extensible.\n",
            "\n",
            "<!-- image -->\n",
            "\n",
            "licensing (e.g. pymupdf [7]), poor speed or unrecoverable quality issues, such as merged text cells across far-apart text tokens or table columns (pypdfium, PyPDF) [15, 14].\n",
            "\n",
            "We therefore decided to provide multiple backend choices, and additionally open-source a custombuilt PDF parser, which is based on the low-level qpdf [4] library. It is made available in a separate package named docling-parse and powers the default PDF backend in Docling. As an alternative, we provide a PDF backend relying on pypdfium , which may be a safe backup choice in certain cases, e.g. if issues are seen with particular font encodings.\n",
            "\n",
            "## 3.2 AI models\n",
            "\n",
            "As part of Docling, we initially release two highly capable AI models to the open-source community, which have been developed and published recently by our team. The first model is a layout analysis model, an accurate object-detector for page elements [13]. The second model is TableFormer [12, 9], a state-of-the-art table structure recognition model. We provide the pre-trained weights (hosted on huggingface) and a separate package for the inference code as docling-ibm-models . Both models are also powering the open-access deepsearch-experience, our cloud-native service for knowledge exploration tasks.\n",
            "\n",
            "## Layout Analysis Model\n",
            "\n",
            "Our layout analysis model is an object-detector which predicts the bounding-boxes and classes of various elements on the image of a given page. Its architecture is derived from RT-DETR [16] and re-trained on DocLayNet [13], our popular human-annotated dataset for document-layout analysis, among other proprietary datasets. For inference, our implementation relies on the onnxruntime [5].\n",
            "\n",
            "The Docling pipeline feeds page images at 72 dpi resolution, which can be processed on a single CPU with sub-second latency. All predicted bounding-box proposals for document elements are post-processed to remove overlapping proposals based on confidence and size, and then intersected with the text tokens in the PDF to group them into meaningful and complete units such as paragraphs, section titles, list items, captions, figures or tables.\n",
            "\n",
            "## Table Structure Recognition\n",
            "\n",
            "The TableFormer model [12], first published in 2022 and since refined with a custom structure token language [9], is a vision-transformer model for table structure recovery. It can predict the logical row and column structure of a given table based on an input image, and determine which table cells belong to column headers, row headers or the table body. Compared to earlier approaches, TableFormer handles many characteristics of tables, such as partial or no borderlines, empty cells, rows or columns, cell spans and hierarchy both on column-heading or row-heading level, tables with inconsistent indentation or alignment and other complexities. For inference, our implementation relies on PyTorch [2].\n",
            "\n",
            "The Docling pipeline feeds all table objects detected in the layout analysis to the TableFormer model, by providing an image-crop of the table and the included text cells. TableFormer structure predictions are matched back to the PDF cells in post-processing to avoid expensive re-transcription text in the table image. Typical tables require between 2 and 6 seconds to be processed on a standard CPU, strongly depending on the amount of included table cells.\n",
            "\n",
            "## OCR\n",
            "\n",
            "Docling provides optional support for OCR, for example to cover scanned PDFs or content in bitmaps images embedded on a page. In our initial release, we rely on EasyOCR [1], a popular thirdparty OCR library with support for many languages. Docling, by default, feeds a high-resolution page image (216 dpi) to the OCR engine, to allow capturing small print detail in decent quality. While EasyOCR delivers reasonable transcription quality, we observe that it runs fairly slow on CPU (upwards of 30 seconds per page).\n",
            "\n",
            "We are actively seeking collaboration from the open-source community to extend Docling with additional OCR backends and speed improvements.\n",
            "\n",
            "## 3.3 Assembly\n",
            "\n",
            "In the final pipeline stage, Docling assembles all prediction results produced on each page into a well-defined datatype that encapsulates a converted document, as defined in the auxiliary package docling-core . The generated document object is passed through a post-processing model which leverages several algorithms to augment features, such as detection of the document language, correcting the reading order, matching figures with captions and labelling metadata such as title, authors and references. The final output can then be serialized to JSON or transformed into a Markdown representation at the users request.\n",
            "\n",
            "## 3.4 Extensibility\n",
            "\n",
            "Docling provides a straight-forward interface to extend its capabilities, namely the model pipeline. A model pipeline constitutes the central part in the processing, following initial document parsing and preceding output assembly, and can be fully customized by sub-classing from an abstract baseclass ( BaseModelPipeline ) or cloning the default model pipeline. This effectively allows to fully customize the chain of models, add or replace models, and introduce additional pipeline configuration parameters. To use a custom model pipeline, the custom pipeline class to instantiate can be provided as an argument to the main document conversion methods. We invite everyone in the community to propose additional or alternative models and improvements.\n",
            "\n",
            "Implementations of model classes must satisfy the python Callable interface. The \\_\\_call\\_\\_ method must accept an iterator over page objects, and produce another iterator over the page objects which were augmented with the additional features predicted by the model, by extending the provided PagePredictions data model accordingly.\n",
            "\n",
            "## 4 Performance\n",
            "\n",
            "In this section, we establish some reference numbers for the processing speed of Docling and the resource budget it requires. All tests in this section are run with default options on our standard test set distributed with Docling, which consists of three papers from arXiv and two IBM Redbooks, with a total of 225 pages. Measurements were taken using both available PDF backends on two different hardware systems: one MacBook Pro M3 Max, and one bare-metal server running Ubuntu 20.04 LTS on an Intel Xeon E5-2690 CPU. For reproducibility, we fixed the thread budget (through setting OMP NUM THREADS environment variable ) once to 4 (Docling default) and once to 16 (equal to full core count on the test hardware). All results are shown in Table 1.\n",
            "\n",
            "If you need to run Docling in very low-resource environments, please consider configuring the pypdfium backend. While it is faster and more memory efficient than the default docling-parse backend, it will come at the expense of worse quality results, especially in table structure recovery.\n",
            "\n",
            "Establishing GPU acceleration support for the AI models is currently work-in-progress and largely untested, but may work implicitly when CUDA is available and discovered by the onnxruntime and\n",
            "\n",
            "torch runtimes backing the Docling pipeline. We will deliver updates on this topic at in a future version of this report.\n",
            "\n",
            "Table 1: Runtime characteristics of Docling with the standard model pipeline and settings, on our test dataset of 225 pages, on two different systems. OCR is disabled. We show the time-to-solution (TTS), computed throughput in pages per second, and the peak memory used (resident set size) for both the Docling-native PDF backend and for the pypdfium backend, using 4 and 16 threads.\n",
            "\n",
            "| CPU                              | Thread budget   | native backend   | native backend   | native backend   | pypdfium backend   | pypdfium backend   | pypdfium backend   |\n",
            "|----------------------------------|-----------------|------------------|------------------|------------------|--------------------|--------------------|--------------------|\n",
            "|                                  |                 | TTS              | Pages/s          | Mem              | TTS                | Pages/s            | Mem                |\n",
            "| Apple M3 Max                     | 4               | 177 s 167 s      | 1.27 1.34        | 6.20 GB          | 103 s 92 s         | 2.18 2.45          | 2.56 GB            |\n",
            "| (16 cores) Intel(R) Xeon E5-2690 | 16 4 16         | 375 s 244 s      | 0.60 0.92        | 6.16 GB          | 239 s 143 s        | 0.94 1.57          | 2.42 GB            |\n",
            "\n",
            "## 5 Applications\n",
            "\n",
            "Thanks to the high-quality, richly structured document conversion achieved by Docling, its output qualifies for numerous downstream applications. For example, Docling can provide a base for detailed enterprise document search, passage retrieval or classification use-cases, or support knowledge extraction pipelines, allowing specific treatment of different structures in the document, such as tables, figures, section structure or references. For popular generative AI application patterns, such as retrieval-augmented generation (RAG), we provide quackling , an open-source package which capitalizes on Docling's feature-rich document output to enable document-native optimized vector embedding and chunking. It plugs in seamlessly with LLM frameworks such as LlamaIndex [8]. Since Docling is fast, stable and cheap to run, it also makes for an excellent choice to build document-derived datasets. With its powerful table structure recognition, it provides significant benefit to automated knowledge-base construction [11, 10]. Docling is also integrated within the open IBM data prep kit [6], which implements scalable data transforms to build large-scale multi-modal training datasets.\n",
            "\n",
            "## 6 Future work and contributions\n",
            "\n",
            "Docling is designed to allow easy extension of the model library and pipelines. In the future, we plan to extend Docling with several more models, such as a figure-classifier model, an equationrecognition model, a code-recognition model and more. This will help improve the quality of conversion for specific types of content, as well as augment extracted document metadata with additional information. Further investment into testing and optimizing GPU acceleration as well as improving the Docling-native PDF backend are on our roadmap, too.\n",
            "\n",
            "We encourage everyone to propose or implement additional features and models, and will gladly take your inputs and contributions under review . The codebase of Docling is open for use and contribution, under the MIT license agreement and in alignment with our contributing guidelines included in the Docling repository. If you use Docling in your projects, please consider citing this technical report.\n",
            "\n",
            "## References\n",
            "\n",
            "- [1] J. AI. Easyocr: Ready-to-use ocr with 80+ supported languages. https://github.com/ JaidedAI/EasyOCR , 2024. Version: 1.7.0.\n",
            "- [2] J. Ansel, E. Yang, H. He, N. Gimelshein, A. Jain, M. Voznesensky, B. Bao, P. Bell, D. Berard, E. Burovski, G. Chauhan, A. Chourdia, W. Constable, A. Desmaison, Z. DeVito, E. Ellison, W. Feng, J. Gong, M. Gschwind, B. Hirsh, S. Huang, K. Kalambarkar, L. Kirsch, M. Lazos, M. Lezcano, Y. Liang, J. Liang, Y. Lu, C. Luk, B. Maher, Y. Pan, C. Puhrsch, M. Reso, M. Saroufim, M. Y. Siraichi, H. Suk, M. Suo, P. Tillet, E. Wang, X. Wang, W. Wen, S. Zhang, X. Zhao, K. Zhou, R. Zou, A. Mathews, G. Chanan, P. Wu, and S. Chintala. Pytorch 2: Faster\n",
            "\n",
            "machine learning through dynamic python bytecode transformation and graph compilation. In Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2 (ASPLOS '24) . ACM, 4 2024. doi: 10.1145/3620665.3640366. URL https://pytorch.org/assets/pytorch2-2.pdf .\n",
            "\n",
            "- [3] C. Auer, M. Dolfi, A. Carvalho, C. B. Ramis, and P. W. Staar. Delivering document conversion as a cloud service with high throughput and responsiveness. In 2022 IEEE 15th International Conference on Cloud Computing (CLOUD) , pages 363-373. IEEE, 2022.\n",
            "- [4] J. Berkenbilt. Qpdf: A content-preserving pdf document transformer, 2024. URL https: //github.com/qpdf/qpdf .\n",
            "- [5] O. R. developers. Onnx runtime. https://onnxruntime.ai/ , 2024. Version: 1.18.1.\n",
            "- [6] IBM. Data Prep Kit: a community project to democratize and accelerate unstructured data preparation for LLM app developers, 2024. URL https://github.com/IBM/ data-prep-kit .\n",
            "- [7] A. S. Inc. PyMuPDF, 2024. URL https://github.com/pymupdf/PyMuPDF .\n",
            "- [8] J. Liu. LlamaIndex, 11 2022. URL https://github.com/jerryjliu/llama\\_index .\n",
            "- [9] M. Lysak, A. Nassar, N. Livathinos, C. Auer, and P. Staar. Optimized Table Tokenization for Table Structure Recognition. In Document Analysis and Recognition - ICDAR 2023: 17th International Conference, San Jos´ e, CA, USA, August 21-26, 2023, Proceedings, Part II , pages 37-50, Berlin, Heidelberg, Aug. 2023. Springer-Verlag. ISBN 978-3-031-41678-1. doi: 10. 1007/978-3-031-41679-8 3. URL https://doi.org/10.1007/978-3-031-41679-8\\_3 .\n",
            "- [10] L. Mishra, S. Dhibi, Y. Kim, C. Berrospi Ramis, S. Gupta, M. Dolfi, and P. Staar. Statements: Universal information extraction from tables with large language models for ESG KPIs. In D. Stammbach, J. Ni, T. Schimanski, K. Dutia, A. Singh, J. Bingler, C. Christiaen, N. Kushwaha, V. Muccione, S. A. Vaghefi, and M. Leippold, editors, Proceedings of the 1st Workshop on Natural Language Processing Meets Climate Change (ClimateNLP 2024) , pages 193-214, Bangkok, Thailand, Aug. 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024.climatenlp-1.15 .\n",
            "- [11] L. Morin, V. Weber, G. I. Meijer, F. Yu, and P. W. J. Staar. Patcid: an open-access dataset of chemical structures in patent documents. Nature Communications , 15(1):6532, August 2024. ISSN 2041-1723. doi: 10.1038/s41467-024-50779-y. URL https://doi.org/10.1038/ s41467-024-50779-y .\n",
            "- [12] A. Nassar, N. Livathinos, M. Lysak, and P. Staar. Tableformer: Table structure understanding with transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4614-4623, 2022.\n",
            "- [13] B. Pfitzmann, C. Auer, M. Dolfi, A. S. Nassar, and P. Staar. Doclaynet: a large humanannotated dataset for document-layout segmentation. pages 3743-3751, 2022.\n",
            "- [14] pypdf Maintainers. pypdf: A Pure-Python PDF Library, 2024. URL https://github.com/ py-pdf/pypdf .\n",
            "- [15] P. Team. PyPDFium2: Python bindings for PDFium, 2024. URL https://github.com/ pypdfium2-team/pypdfium2 .\n",
            "- [16] Y. Zhao, W. Lv, S. Xu, J. Wei, G. Wang, Q. Dang, Y. Liu, and J. Chen. Detrs beat yolos on real-time object detection, 2023.\n",
            "\n",
            "## Appendix\n",
            "\n",
            "In this section, we illustrate a few examples of Docling's output in Markdown and JSON.\n",
            "\n",
            "## DocLayNet: A Large Human-Annotated Dataset for Document-Layout Analysis\n",
            "\n",
            "## DocLayNet: A Large Human-Annotated Dataset for Document-Layout Analysis\n",
            "\n",
            "Birgit Pfitzmann IBM Research Rueschlikon, Switzerland bpf@zurich.ibm.com\n",
            "\n",
            "Christoph Auer IBM Research Rueschlikon, Switzerland cau@zurich.ibm.com\n",
            "\n",
            "Michele Dolfi IBM Research Rueschlikon, Switzerland dol@zurich.ibm.com\n",
            "\n",
            "Ahmed S. Nassar IBM Research Rueschlikon, Switzerland ahn@zurich.ibm.com\n",
            "\n",
            "Peter Staar IBM Research Rueschlikon, Switzerland taa@zurich.ibm.com\n",
            "\n",
            "## ABSTRACT\n",
            "\n",
            "Accurate document layout analysis is a key requirement for highquality PDF document conversion. With the recent availability of public, large ground-truth datasets such as PubLayNet and DocBank, deep-learning models have proven to be very effective at layout detection and segmentation. While these datasets are of adequate size to train such models, they severely lack in layout variability since they are sourced from scientific article repositories such as PubMed and arXiv only. Consequently, the accuracy of the layout segmentation drops significantly when these models are applied on more challenging and diverse layouts. In this paper, we present DocLayNet , a new, publicly available, document-layout annotation dataset in COCO format. It contains 80863 manually annotated pages from diverse data sources to represent a wide variability in layouts. For each PDF page, the layout annotations provide labelled bounding-boxes with a choice of 11 distinct classes. DocLayNet also provides a subset of double- and triple-annotated pages to determine the inter-annotator agreement. In multiple experiments, we provide baseline accuracy scores (in mAP) for a set of popular object detection models. We also demonstrate that these models fall approximately 10% behind the inter-annotator agreement. Furthermore, we provide evidence that DocLayNet is of sufficient size. Lastly, we compare models trained on PubLayNet, DocBank and DocLayNet, showing that layout predictions of the DocLayNettrained models are more robust and thus the preferred choice for general-purpose document-layout analysis.\n",
            "\n",
            "## CCS CONCEPTS\n",
            "\n",
            "· Informationsystems → Documentstructure ; · Appliedcomputing → Document analysis ; · Computing methodologies → Machine learning Computer vision ; ; Object detection ;\n",
            "\n",
            "Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). KDD '22, August 14-18, 2022, Washington, DC, USA © 2022 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-9385-0/22/08. https://doi.org/10.1145/3534678.3539043\n",
            "\n",
            "Birgit Pfitzmann IBM Research Rueschlikon, Switzerland bpf@zurich.ibm.com\n",
            "\n",
            "Christoph Auer IBM Research Rueschlikon, Switzerland cau@zurich.ibm.com\n",
            "\n",
            "Michele Dolfi IBM Research Rueschlikon, Switzerland dol@zurich.ibm.com\n",
            "\n",
            "Ahmed S. Nassar IBM Research Rueschlikon, Switzerland ahn@zurich.ibm.com\n",
            "\n",
            "Peter Staar IBM Research Rueschlikon, Switzerland taa@zurich.ibm.com\n",
            "\n",
            "## ABSTRACT\n",
            "\n",
            "Accurate document layout analysis is a key requirement for highquality PDF document conversion. With the recent availability of public, large groundtruth datasets such as PubLayNet and DocBank, deep-learning models have proven to be very effective at layout detection and segmentation. While these datasets are of adequate size to train such models, they severely lack in layout variability since they are sourced from scientific article repositories such as PubMed and arXiv only. Consequently, the accuracy of the layout segmentation drops significantly when these models are applied on more challenging and diverse layouts. In this paper, we present DocLayNet , a new, publicly available, document-layout annotation dataset in COCO format. It contains 80863 manually annotated pages from diverse data sources to represent a wide variability in layouts. For each PDF page, the layout annotations provide labelled bounding-boxes with a choice of 11 distinct classes. DocLayNet also provides a subset of double- and triple-annotated pages to determine the inter-annotator agreement. In multiple experiments, we provide baseline accuracy scores (in mAP) for a set of popular object detection models. We also demonstrate that these models fall approximately 10% behind the inter-annotator agreement. Furthermore, we provide evidence that DocLayNet is of sufficient size. Lastly, we compare models trained on PubLayNet, DocBank and DocLayNet, showing that layout predictions of the DocLayNettrained models are more robust and thus the preferred choice for general-purpose document-layout analysis.\n",
            "\n",
            "## CCS CONCEPTS\n",
            "\n",
            "Æ Information systems → Document structure ; Æ Applied computing → Document analysis ; Æ Computing methodologies → Machine learning ; Computer vision ; Object detection ;\n",
            "\n",
            "Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s).\n",
            "\n",
            "KDD '22, August 14-18, 2022, Washington, DC, USA ' 2022 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-9385-0/22/08. https://doi.org/10.1145/3534678.3539043\n",
            "\n",
            "Figure 1: Four examples of complex page layouts across different document categories\n",
            "\n",
            "## KEYWORDS\n",
            "\n",
            "PDF document conversion, layout segmentation, object-detection, data set, Machine Learning\n",
            "\n",
            "## ACM Reference Format:\n",
            "\n",
            "Birgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S. Nassar, and Peter Staar. 2022. DocLayNet: A Large Human-Annotated Dataset for DocumentLayout Analysis. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD '22), August 14-18, 2022, Washington, DC, USA. ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/ 3534678.3539043\n",
            "\n",
            "<!-- image -->\n",
            "\n",
            "<!-- image -->\n",
            "\n",
            "AGL Energy Limited  ABN 74 1\n",
            "\n",
            "5 061 375\n",
            "\n",
            "<!-- image -->\n",
            "\n",
            "<!-- image -->\n",
            "\n",
            "Figure 1: Four examples of complex page layouts across different document categories\n",
            "\n",
            "## KEYWORDS\n",
            "\n",
            "PDF document conversion, layout segmentation, object-detection, data set, Machine Learning\n",
            "\n",
            "## ACMReference Format:\n",
            "\n",
            "Birgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S. Nassar, and Peter Staar. 2022. DocLayNet: A Large Human-Annotated Dataset for DocumentLayout Analysis. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD '22), August 14-18, 2022, Washington, DC, USA. ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/ 3534678.3539043\n",
            "\n",
            "1 INTRODUCTION\n",
            "\n",
            "Despite the substantial improvements achieved with machine-learning (ML) approaches and deep neural networks in recent years, document conversion remains a challenging problem, as demonstrated by the numerous public competitions held on this topic [1-4]. The challenge originates from the huge variability in PDF documents regarding layout, language and formats (scanned, programmatic or a combination of both). Engineering a single ML model that can be applied on all types of documents and provides high-quality layout segmentation remains to this day extremely challenging [5]. To highlight the variability in document layouts, we show a few example documents from the DocLayNet dataset in Figure 1. Figure 2: Title page of the DocLayNet paper (arxiv.org/pdf/2206.01062) - left PDF, right rendered Markdown. If recognized, metadata such as authors are appearing first under the title. Text content inside figures is currently dropped, the caption is retained and linked to the figure in the JSON representation (not shown).\n",
            "\n",
            "KDD '22, August 14-18, 2022, Washington, DC, USA Birgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S. Nassar, and Peter Staar\n",
            "\n",
            "Table 2: Prediction performance (mAP@0.5-0.95) of object detection networks on DocLayNet test set. The MRCNN (Mask R-CNN) and FRCNN (Faster R-CNN) models with ResNet-50 or ResNet-101 backbone were trained based on the network architectures from the detectron2 model zoo (Mask R-CNN R50, R101-FPN 3x, Faster R-CNN R101-FPN 3x), with default configurations. The YOLO implementation utilized was YOLOv5x6 [13]. All models were initialised using pre-trained weights from the COCO 2017 dataset.\n",
            "\n",
            "|                                                                                                        | human                                                                   | MRCNN R50 R101                                                                                                          | FRCNN R101                                                  | YOLO v5x6                                                   |\n",
            "|--------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------|-------------------------------------------------------------|\n",
            "| Caption Footnote Formula List-item Page-footer Page-header Picture Section-header Table Text Title All | 84-89 83-91 83-85 87-88 93-94 85-89 69-71 83-84 77-81 84-86 60-72 82-83 | 68.4 71.5 70.9 71.8 60.1 63.4 81.2 80.8 61.6 59.3 71.9 70.0 71.7 72.7 67.6 69.3 82.2 82.9 84.6 85.8 76.7 80.4 72.4 73.5 | 70.1 73.7 63.5 81.0 58.9 72.0 72.0 68.4 82.2 85.4 79.9 73.4 | 77.7 77.2 66.2 86.2 61.1 67.9 77.1 74.6 86.3 88.1 82.7 76.8 |\n",
            "\n",
            "to avoid this at any cost in order to have clear, unbiased baseline numbers for human document-layout annotation. Third, we introduced the feature of snapping boxes around text segments to obtain a pixel-accurate annotation and again reduce time and effort. The CCS annotation tool automatically shrinks every user-drawn box to the minimum bounding-box around the enclosed text-cells for all purely text-based segments, which excludes only Table and Picture . For the latter, we instructed annotation staff to minimise inclusion of surrounding whitespace while including all graphical lines. A downside of snapping boxes to enclosed text cells is that some wrongly parsed PDF pages cannot be annotated correctly and need to be skipped. Fourth, we established a way to flag pages as rejected for cases where no valid annotation according to the label guidelines could be achieved. Example cases for this would be PDF pages that render incorrectly or contain layouts that are impossible to capture with non-overlapping rectangles. Such rejected pages are not contained in the final dataset. With all these measures in place, experienced annotation staff managed to annotate a single page in a typical timeframe of 20s to 60s, depending on its complexity.\n",
            "\n",
            "## 5 EXPERIMENTS\n",
            "\n",
            "The primary goal of DocLayNet is to obtain high-quality ML models capable of accurate document-layout analysis on a wide variety of challenging layouts. As discussed in Section 2, object detection models are currently the easiest to use, due to the standardisation of ground-truth data in COCO format [16] and the availability of general frameworks such as detectron2 [17]. Furthermore, baseline numbers in PubLayNet and DocBank were obtained using standard object detection models such as Mask R-CNN and Faster R-CNN. As such, we will relate to these object detection methods in this\n",
            "\n",
            "<!-- image -->\n",
            "\n",
            "Third, Ccs label qu Oolines achienec Exanole\n",
            "\n",
            "## EXPERIMENTS\n",
            "\n",
            "chalenongayouls ground-vuth dawa such WC\n",
            "\n",
            "<!-- image -->\n",
            "\n",
            "Figure 5: Prediction performance (mAP@0.5-0.95) of a Mask R-CNNnetworkwithResNet50backbonetrainedonincreasing fractions of the DocLayNet dataset. The learning curve flattens around the 80% mark, indicating that increasing the size of the DocLayNet dataset with similar data will not yield significantly better predictions.\n",
            "\n",
            "paper and leave the detailed evaluation of more recent methods mentioned in Section 2 for future work.\n",
            "\n",
            "In this section, we will present several aspects related to the performance of object detection models on DocLayNet. Similarly as in PubLayNet, we will evaluate the quality of their predictions using mean average precision (mAP) with 10 overlaps that range from 0.5 to 0.95 in steps of 0.05 (mAP@0.5-0.95). These scores are computed by leveraging the evaluation code provided by the COCO API [16].\n",
            "\n",
            "## Baselines for Object Detection\n",
            "\n",
            "In Table 2, we present baseline experiments (given in mAP) on Mask R-CNN [12], Faster R-CNN [11], and YOLOv5 [13]. Both training and evaluation were performed on RGB images with dimensions of 1025 × 1025 pixels. For training, we only used one annotation in case of redundantly annotated pages. As one can observe, the variation in mAP between the models is rather low, but overall between 6 and 10% lower than the mAP computed from the pairwise human annotations on triple-annotated pages. This gives a good indication that the DocLayNet dataset poses a worthwhile challenge for the research community to close the gap between human recognition and ML approaches. It is interesting to see that Mask R-CNN and Faster R-CNN produce very comparable mAP scores, indicating that pixel-based image segmentation derived from bounding-boxes does not help to obtain better predictions. On the other hand, the more recent Yolov5x model does very well and even out-performs humans on selected labels such as Text , Table and Picture . This is not entirely surprising, as Text , Table and Picture are abundant and the most visually distinctive in a document.\n",
            "\n",
            "## Baselines for Object Detection\n",
            "\n",
            "mak enbrel dacuont\n",
            "\n",
            "Figure 3: Page 6 of the DocLayNet paper. If recognized, metadata such as authors are appearing first under the title. Elements recognized as page headers or footers are suppressed in Markdown to deliver uninterrupted content in reading order. Tables are inserted in reading order. The paragraph in '5. Experiments' wrapping over the column end is broken up in two and interrupted by the table.\n",
            "\n",
            "KDD '22, August 14-18, 2022, Washington, DC, USA\n",
            "\n",
            "Birgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S. Nassar, and Peter Staar\n",
            "\n",
            "Table 1: DocLayNet dataset overview. Along with the frequency of each class label, we present the relative occurrence (as %\n",
            "\n",
            "between pairwise annotations from the triple-annotated pages, from which we obtain accuracy ranges.\n",
            "\n",
            "of row 'Total') in the train, test and validation sets. The inter-annotator agreement is computed as the mAP@0.5-0.95 metric\n",
            "\n",
            "<!-- image -->\n",
            "\n",
            "<!-- image -->\n",
            "\n",
            "| class label    | Count   | % of Total   | % of Total   | % of Total   | triple inter-annotator mAP @0.5-0.95 (%)   | triple inter-annotator mAP @0.5-0.95 (%)   | triple inter-annotator mAP @0.5-0.95 (%)   | triple inter-annotator mAP @0.5-0.95 (%)   | triple inter-annotator mAP @0.5-0.95 (%)   | triple inter-annotator mAP @0.5-0.95 (%)   | triple inter-annotator mAP @0.5-0.95 (%)   |\n",
            "|----------------|---------|--------------|--------------|--------------|--------------------------------------------|--------------------------------------------|--------------------------------------------|--------------------------------------------|--------------------------------------------|--------------------------------------------|--------------------------------------------|\n",
            "| class label    | Count   | Train        | Test         | Val          | All                                        | Fin                                        | Man                                        | Sci                                        | Law                                        | Pat                                        | Ten                                        |\n",
            "| Caption        | 22524   | 2.04         | 1.77         | 2.32         | 84-89                                      | 40-61                                      | 86-92                                      | 94-99                                      | 95-99                                      | 69-78                                      | n/a                                        |\n",
            "| Footnote       | 6318    | 0.60         | 0.31         | 0.58         | 83-91                                      | n/a                                        | 100                                        | 62-88                                      | 85-94                                      | n/a                                        | 82-97                                      |\n",
            "| Formula        | 25027   | 2.25         | 1.90         | 2.96         | 83-85                                      | n/a                                        | n/a                                        | 84-87                                      | 86-96                                      | n/a                                        | n/a                                        |\n",
            "| List-item      | 185660  | 17.19        | 13.34        | 15.82        | 87-88                                      | 74-83                                      | 90-92                                      | 97-97                                      | 81-85                                      | 75-88                                      | 93-95                                      |\n",
            "| Page-footer    | 70878   | 6.51         | 5.58         | 6.00         | 93-94                                      | 88-90                                      | 95-96                                      | 100                                        | 92-97                                      | 100                                        | 96-98                                      |\n",
            "| Page-header    | 58022   | 5.10         | 6.70         | 5.06         | 85-89                                      | 66-76                                      | 90-94                                      | 98-100                                     | 91-92                                      | 97-99                                      | 81-86                                      |\n",
            "| Picture        | 45976   | 4.21         | 2.78         | 5.31         | 69-71                                      | 56-59                                      | 82-86                                      | 69-82                                      | 80-95                                      | 66-71                                      | 59-76                                      |\n",
            "| Section-header | 142884  | 12.60        | 15.77        | 12.85        | 83-84                                      | 76-81                                      | 90-92                                      | 94-95                                      | 87-94                                      | 69-73                                      | 78-86                                      |\n",
            "| Table          | 34733   | 3.20         | 2.27         | 3.60         | 77-81                                      | 75-80                                      | 83-86                                      | 98-99                                      | 58-80                                      | 79-84                                      | 70-85                                      |\n",
            "| Text           | 510377  | 45.82        | 49.28        | 45.00        | 84-86                                      | 81-86                                      | 88-93                                      | 89-93                                      | 87-92                                      | 71-79                                      | 87-95                                      |\n",
            "| Title          | 5071    | 0.47         | 0.30         | 0.50         | 60-72                                      | 24-63                                      | 50-63                                      | 94-100                                     | 82-96                                      | 68-79                                      | 24-56                                      |\n",
            "| Total          | 1107470 | 941123       | 99816        | 66531        | 82-83                                      | 71-74                                      | 79-81                                      | 89-94                                      | 86-91                                      | 71-76                                      | 68-85                                      |\n",
            "\n",
            "<!-- image -->\n",
            "\n",
            "include publication repositories such as arXiv\n",
            "\n",
            "Table 1: DocLayNet dataset overview. Along with the frequency of each class label, we present the relative occurrence (as % of row \"Total\") in the train, test and validation sets. The inter-annotator agreement is computed as the mAP@0.5-0.95 metric between pairwise annotations from the triple-\n",
            "\n",
            "annotated pages, from which we obtain accuracy ranges.\n",
            "\n",
            "<!-- image -->\n",
            "\n",
            "|                       |         | %of Total   | %of Total   | %of Total   | triple inter- annotator mAP @ 0.5-0.95 (%)   | triple inter- annotator mAP @ 0.5-0.95 (%)   | triple inter- annotator mAP @ 0.5-0.95 (%)   | triple inter- annotator mAP @ 0.5-0.95 (%)   | triple inter- annotator mAP @ 0.5-0.95 (%)   | triple inter- annotator mAP @ 0.5-0.95 (%)   | triple inter- annotator mAP @ 0.5-0.95 (%)   |\n",
            "|-----------------------|---------|-------------|-------------|-------------|----------------------------------------------|----------------------------------------------|----------------------------------------------|----------------------------------------------|----------------------------------------------|----------------------------------------------|----------------------------------------------|\n",
            "| class label           | Count   | Train       | Test        | Val         | All                                          | Fin                                          | Man                                          | Sci                                          | Law                                          | Pat                                          | Ten                                          |\n",
            "| Caption               | 22524   | 2.04        | 1.77        | 2.32        | 84-89                                        | 40-61                                        | 86-92                                        | 94-99                                        | 95-99                                        | 69-78                                        | n/a                                          |\n",
            "| Footnote              | 6318    | 0.60        | 0.31        | 0.58        | 83-91                                        | n/a                                          | 100                                          | 62-88                                        | 85-94                                        | n/a                                          | 82-97                                        |\n",
            "| Formula               | 25027   | 2.25        | 1.90        | 2.96        | 83-85                                        | n/a                                          | n/a                                          | 84-87                                        | 86-96                                        | n/a                                          | n/a                                          |\n",
            "| List-item             | 185660  | 17.19       | 13.34       | 15.82       | 87-88                                        | 74-83                                        | 90-92                                        | 97-97                                        | 81-85                                        | 75-88                                        | 93-95                                        |\n",
            "| Page- footer          | 70878   | 6.51        | 5.58        | 6.00        | 93-94                                        | 88-90                                        | 95-96                                        | 100                                          | 92-97                                        | 100                                          | 96-98                                        |\n",
            "| Page- header offices, | 58022   | 5.10        | 6.70        | 5.06        | 85-89                                        | 66-76                                        | 90-94                                        | 98-100                                       | 91-92                                        | 97-99                                        | 81-86                                        |\n",
            "| Picture               | 45976   | 4.21        | 2.78        | 5.31        | 69-71                                        | 56-59                                        | 82-86                                        | 69-82                                        | 80-95                                        | 66-71                                        | 59-76                                        |\n",
            "| Section- header not   | 142884  | 12.60       | 15.77       | 12.85       | 83-84                                        | 76-81                                        | 90-92                                        | 94-95                                        | 87-94                                        | 69-73                                        | 78-86                                        |\n",
            "| Table                 | 34733   | 3.20        | 2.27        | 3.60        | 77-81                                        | 75-80                                        | 83-86                                        | 98-99                                        | 58-80                                        | 79-84                                        | 70-85                                        |\n",
            "| Text                  | 510377  | 45.82       | 49.28       | 45.00       | 84-86                                        | 81-86                                        | 88-93                                        | 89-93                                        | 87-92                                        | 71-79                                        | 87-95                                        |\n",
            "| Title [22], a         | 5071    | 0.47        | 0.30        | 0.50        | 60-72                                        | 24-63                                        | 50-63                                        | 94-100                                       | 82-96                                        | 68-79                                        | 24-56                                        |\n",
            "| Total in-             | 1107470 | 941123      | 99816       | 66531       | 82-83                                        | 71-74                                        | 79-81                                        | 89-94                                        | 86-91                                        | 71-76                                        | 68-85                                        |\n",
            "\n",
            "3\n",
            "\n",
            ",\n",
            "\n",
            "government offices,\n",
            "\n",
            "We reviewed the col-\n",
            "\n",
            ",\n",
            "\n",
            "Page-\n",
            "\n",
            "Title and\n",
            "\n",
            ".\n",
            "\n",
            "page. Specificity ensures that the choice of label is not ambiguous,\n",
            "\n",
            "<!-- image -->\n",
            "\n",
            "we distributed the annotation workload and performed continuous be annotated. We refrained from class labels that are very specific\n",
            "\n",
            "only. For phases three and four, a group of 40 dedicated annotators while coverage ensures that all meaningful items on a page can\n",
            "\n",
            "quality controls. Phase one and two required a small team of experts to a document category, such as\n",
            "\n",
            "Abstract in the\n",
            "\n",
            "Scientific Articles were assembled and supervised.\n",
            "\n",
            "category. We also avoided class labels that are tightly linked to the\n",
            "\n",
            "Phase 1: Data selection and preparation.\n",
            "\n",
            "Our inclusion cri-\n",
            "\n",
            "Author\n",
            "\n",
            "Affiliation\n",
            "\n",
            "teria for documents were described in Section 3. A large effort went into ensuring that all documents are free to use. The data sources in DocBank, are often only distinguishable by discriminating on 3 https://arxiv.org/ Figure 4: Table 1 from the DocLayNet paper in the original PDF (A), as rendered Markdown (B) and in JSON representation (C). Spanning table cells, such as the multi-column header 'triple interannotator mAP@0.5-0.95 (%)', is repeated for each column in the Markdown representation (B), which guarantees that every data point can be traced back to row and column headings only by its grid coordinates in the table. In the JSON representation, the span information is reflected in the fields of each table cell (C).\n",
            "\n",
            "semantics of the text. Labels such as and\n",
            "\n",
            ",\n",
            "\n",
            "as seen\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "doc_data = result.document.export_to_dict()\n"
      ],
      "metadata": {
        "id": "uaavd0ZRWVZV"
      },
      "id": "uaavd0ZRWVZV",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PvlgeYAHbpW2"
      },
      "id": "PvlgeYAHbpW2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "self_ref: Stands for \"self-reference\". Its value ('#/texts/6') acts as a unique identifier for this specific text block within the overall document structure. It likely means this is the 7th element (index 6) within a list named 'texts'.\n",
        "parent: Indicates the structural element that contains this text block. The value ({'$ref': '#/body'}) points to the main 'body' of the document, meaning this text block is a direct child of the document body.\n",
        "children: This key holds a list of any sub-elements contained within this text block. Here, it's an empty list ([]), signifying that this text block itself doesn't have further nested structures like lists, figures, or tables defined directly inside it in this representation.\n",
        "content_layer: Specifies the general category or layer this content belongs to. 'body' means it's part of the main content flow of the document, distinct from elements like headers, footers, page numbers (furniture), etc.\n",
        "label: Provides a more specific semantic classification for this block. 'text' indicates it's recognized as a standard paragraph or block of text. Other labels might be 'section_header', 'list_item', 'table', 'figure', etc.\n",
        "prov: Short for \"provenance\". This key holds metadata about the origin and location of this text block in the source PDF document. It's a list (usually with one entry for a single block) containing a dictionary with:\n",
        "page_no: The page number in the original PDF where this text was found (page 1).\n",
        "bbox: The \"bounding box\" coordinates defining the rectangular area occupied by this text on the page (l: left, t: top, r: right, b: bottom). The coord_origin indicates the reference point for these coordinates (bottom-left corner of the page).\n",
        "charspan: Likely refers to the start (0) and end (431) character indices of this text within some internal representation or possibly within the orig string.\n",
        "orig: Holds the \"original\" text content as it might have been extracted initially, possibly preserving specific line breaks or formatting closer to the PDF source.\n",
        "text: Contains the processed or cleaned text content intended for general use. In this example, the text and orig values are identical, but in other cases, text might have normalized whitespace, removed hyphens from line breaks, etc."
      ],
      "metadata": {
        "id": "NWrRFhCkbvQd"
      },
      "id": "NWrRFhCkbvQd"
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'doc_data' holds the original dictionary from the first message\n",
        "all_text = doc_data['texts']\n",
        "\n",
        "for i, text_item in enumerate(all_text):\n",
        "    # Use .get('level') which returns None if 'level' key doesn't exist\n",
        "    level = text_item.get('level')\n",
        "    print(i, text_item['text'], level, text_item['text'], text_item['parent'])  # parent = group means this text is not in the body. it's originating from another body (like bullet points)"
      ],
      "metadata": {
        "id": "3HXpkq1JXmG8",
        "outputId": "798f4fff-f45c-4a4b-fb5a-cce470077ad0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "3HXpkq1JXmG8",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 arXiv:2408.09869v5  [cs.CL]  9 Dec 2024 None arXiv:2408.09869v5  [cs.CL]  9 Dec 2024 {'$ref': '#/body'}\n",
            "1 Docling Technical Report 1 Docling Technical Report {'$ref': '#/body'}\n",
            "2 Version 1.0 None Version 1.0 {'$ref': '#/groups/0'}\n",
            "3 Christoph Auer Maksym Lysak Ahmed Nassar Michele Dolfi Nikolaos Livathinos Panos Vagenas Cesar Berrospi Ramis Matteo Omenetti Fabian Lindlbauer Kasper Dinkla Lokesh Mishra Yusik Kim Shubham Gupta Rafael Teixeira de Lima Valery Weber Lucas Morin Ingmar Meijer Viktor Kuropiatnyk Peter W. J. Staar None Christoph Auer Maksym Lysak Ahmed Nassar Michele Dolfi Nikolaos Livathinos Panos Vagenas Cesar Berrospi Ramis Matteo Omenetti Fabian Lindlbauer Kasper Dinkla Lokesh Mishra Yusik Kim Shubham Gupta Rafael Teixeira de Lima Valery Weber Lucas Morin Ingmar Meijer Viktor Kuropiatnyk Peter W. J. Staar {'$ref': '#/body'}\n",
            "4 AI4K Group, IBM Research R¨ uschlikon, Switzerland None AI4K Group, IBM Research R¨ uschlikon, Switzerland {'$ref': '#/groups/1'}\n",
            "5 Abstract 1 Abstract {'$ref': '#/body'}\n",
            "6 This technical report introduces Docling , an easy to use, self-contained, MITlicensed open-source package for PDF document conversion. It is powered by state-of-the-art specialized AI models for layout analysis (DocLayNet) and table structure recognition (TableFormer), and runs efficiently on commodity hardware in a small resource budget. The code interface allows for easy extensibility and addition of new features and models. None This technical report introduces Docling , an easy to use, self-contained, MITlicensed open-source package for PDF document conversion. It is powered by state-of-the-art specialized AI models for layout analysis (DocLayNet) and table structure recognition (TableFormer), and runs efficiently on commodity hardware in a small resource budget. The code interface allows for easy extensibility and addition of new features and models. {'$ref': '#/body'}\n",
            "7 1 Introduction 1 1 Introduction {'$ref': '#/body'}\n",
            "8 Converting PDF documents back into a machine-processable format has been a major challenge for decades due to their huge variability in formats, weak standardization and printing-optimized characteristic, which discards most structural features and metadata. With the advent of LLMs and popular application patterns such as retrieval-augmented generation (RAG), leveraging the rich content embedded in PDFs has become ever more relevant. In the past decade, several powerful document understanding solutions have emerged on the market, most of which are commercial software, cloud offerings [3] and most recently, multi-modal vision-language models. As of today, only a handful of open-source tools cover PDF conversion, leaving a significant feature and quality gap to proprietary solutions. None Converting PDF documents back into a machine-processable format has been a major challenge for decades due to their huge variability in formats, weak standardization and printing-optimized characteristic, which discards most structural features and metadata. With the advent of LLMs and popular application patterns such as retrieval-augmented generation (RAG), leveraging the rich content embedded in PDFs has become ever more relevant. In the past decade, several powerful document understanding solutions have emerged on the market, most of which are commercial software, cloud offerings [3] and most recently, multi-modal vision-language models. As of today, only a handful of open-source tools cover PDF conversion, leaving a significant feature and quality gap to proprietary solutions. {'$ref': '#/body'}\n",
            "9 With Docling , we open-source a very capable and efficient document conversion tool which builds on the powerful, specialized AI models and datasets for layout analysis and table structure recognition we developed and presented in the recent past [12, 13, 9]. Docling is designed as a simple, self-contained python library with permissive license, running entirely locally on commodity hardware. Its code architecture allows for easy extensibility and addition of new features and models. None With Docling , we open-source a very capable and efficient document conversion tool which builds on the powerful, specialized AI models and datasets for layout analysis and table structure recognition we developed and presented in the recent past [12, 13, 9]. Docling is designed as a simple, self-contained python library with permissive license, running entirely locally on commodity hardware. Its code architecture allows for easy extensibility and addition of new features and models. {'$ref': '#/body'}\n",
            "10 Docling Technical Report None Docling Technical Report {'$ref': '#/body'}\n",
            "11 1 None 1 {'$ref': '#/body'}\n",
            "12 Here is what Docling delivers today: None Here is what Docling delivers today: {'$ref': '#/body'}\n",
            "13 · Converts PDF documents to JSON or Markdown format, stable and lightning fast None · Converts PDF documents to JSON or Markdown format, stable and lightning fast {'$ref': '#/groups/2'}\n",
            "14 · Understands detailed page layout, reading order, locates figures and recovers table structures None · Understands detailed page layout, reading order, locates figures and recovers table structures {'$ref': '#/groups/2'}\n",
            "15 · Extracts metadata from the document, such as title, authors, references and language None · Extracts metadata from the document, such as title, authors, references and language {'$ref': '#/groups/2'}\n",
            "16 · Optionally applies OCR, e.g. for scanned PDFs None · Optionally applies OCR, e.g. for scanned PDFs {'$ref': '#/groups/2'}\n",
            "17 · Can be configured to be optimal for batch-mode (i.e high throughput, low time-to-solution) or interactive mode (compromise on efficiency, low time-to-solution) None · Can be configured to be optimal for batch-mode (i.e high throughput, low time-to-solution) or interactive mode (compromise on efficiency, low time-to-solution) {'$ref': '#/groups/2'}\n",
            "18 · Can leverage different accelerators (GPU, MPS, etc). None · Can leverage different accelerators (GPU, MPS, etc). {'$ref': '#/groups/2'}\n",
            "19 2 Getting Started 1 2 Getting Started {'$ref': '#/body'}\n",
            "20 To use Docling, you can simply install the docling package from PyPI. Documentation and examples are available in our GitHub repository at github.com/DS4SD/docling. All required model assets 1 are downloaded to a local huggingface datasets cache on first use, unless you choose to pre-install the model assets in advance. None To use Docling, you can simply install the docling package from PyPI. Documentation and examples are available in our GitHub repository at github.com/DS4SD/docling. All required model assets 1 are downloaded to a local huggingface datasets cache on first use, unless you choose to pre-install the model assets in advance. {'$ref': '#/body'}\n",
            "21 Docling provides an easy code interface to convert PDF documents from file system, URLs or binary streams, and retrieve the output in either JSON or Markdown format. For convenience, separate methods are offered to convert single documents or batches of documents. A basic usage example is illustrated below. Further examples are available in the Doclign code repository. None Docling provides an easy code interface to convert PDF documents from file system, URLs or binary streams, and retrieve the output in either JSON or Markdown format. For convenience, separate methods are offered to convert single documents or batches of documents. A basic usage example is illustrated below. Further examples are available in the Doclign code repository. {'$ref': '#/body'}\n",
            "22 from docling.document_converter import DocumentConverter Large None from docling.document_converter import DocumentConverter Large {'$ref': '#/body'}\n",
            "23 source = \"https://arxiv.org/pdf/2206.01062\" # PDF path or URL converter = DocumentConverter() result = converter.convert_single(source) print(result.render_as_markdown()) # output: \"## DocLayNet: A Human -Annotated Dataset for Document -Layout Analysis [...]\" None source = \"https://arxiv.org/pdf/2206.01062\" # PDF path or URL converter = DocumentConverter() result = converter.convert_single(source) print(result.render_as_markdown()) # output: \"## DocLayNet: A Human -Annotated Dataset for Document -Layout Analysis [...]\" {'$ref': '#/body'}\n",
            "24 Optionally, you can configure custom pipeline features and runtime options, such as turning on or off features (e.g. OCR, table structure recognition), enforcing limits on the input document size, and defining the budget of CPU threads. Advanced usage examples and options are documented in the README file. Docling also provides a Dockerfile to demonstrate how to install and run it inside a container. None Optionally, you can configure custom pipeline features and runtime options, such as turning on or off features (e.g. OCR, table structure recognition), enforcing limits on the input document size, and defining the budget of CPU threads. Advanced usage examples and options are documented in the README file. Docling also provides a Dockerfile to demonstrate how to install and run it inside a container. {'$ref': '#/body'}\n",
            "25 3 Processing pipeline 1 3 Processing pipeline {'$ref': '#/body'}\n",
            "26 Docling implements a linear pipeline of operations, which execute sequentially on each given document (see Fig. 1). Each document is first parsed by a PDF backend, which retrieves the programmatic text tokens, consisting of string content and its coordinates on the page, and also renders a bitmap image of each page to support downstream operations. Then, the standard model pipeline applies a sequence of AI models independently on every page in the document to extract features and content, such as layout and table structures. Finally, the results from all pages are aggregated and passed through a post-processing stage, which augments metadata, detects the document language, infers reading-order and eventually assembles a typed document object which can be serialized to JSON or Markdown. None Docling implements a linear pipeline of operations, which execute sequentially on each given document (see Fig. 1). Each document is first parsed by a PDF backend, which retrieves the programmatic text tokens, consisting of string content and its coordinates on the page, and also renders a bitmap image of each page to support downstream operations. Then, the standard model pipeline applies a sequence of AI models independently on every page in the document to extract features and content, such as layout and table structures. Finally, the results from all pages are aggregated and passed through a post-processing stage, which augments metadata, detects the document language, infers reading-order and eventually assembles a typed document object which can be serialized to JSON or Markdown. {'$ref': '#/body'}\n",
            "27 3.1 PDF backends 1 3.1 PDF backends {'$ref': '#/body'}\n",
            "28 Two basic requirements to process PDF documents in our pipeline are a) to retrieve all text content and their geometric coordinates on each page and b) to render the visual representation of each page as it would appear in a PDF viewer. Both these requirements are encapsulated in Docling's PDF backend interface. While there are several open-source PDF parsing libraries available for python, we faced major obstacles with all of them for different reasons, among which were restrictive None Two basic requirements to process PDF documents in our pipeline are a) to retrieve all text content and their geometric coordinates on each page and b) to render the visual representation of each page as it would appear in a PDF viewer. Both these requirements are encapsulated in Docling's PDF backend interface. While there are several open-source PDF parsing libraries available for python, we faced major obstacles with all of them for different reasons, among which were restrictive {'$ref': '#/body'}\n",
            "29 1 see huggingface.co/ds4sd/docling-models/ None 1 see huggingface.co/ds4sd/docling-models/ {'$ref': '#/body'}\n",
            "30 2 None 2 {'$ref': '#/body'}\n",
            "31 Figure 1: Sketch of Docling's default processing pipeline. The inner part of the model pipeline is easily customizable and extensible. None Figure 1: Sketch of Docling's default processing pipeline. The inner part of the model pipeline is easily customizable and extensible. {'$ref': '#/pictures/1'}\n",
            "32 Layout None Layout {'$ref': '#/pictures/1'}\n",
            "33 Analysis None Analysis {'$ref': '#/pictures/1'}\n",
            "34 Serialize as None Serialize as {'$ref': '#/pictures/1'}\n",
            "35 JSON None JSON {'$ref': '#/pictures/1'}\n",
            "36 or Markdown None or Markdown {'$ref': '#/pictures/1'}\n",
            "37 {;} None {;} {'$ref': '#/pictures/1'}\n",
            "38 Parse None Parse {'$ref': '#/pictures/1'}\n",
            "39 PDF pages None PDF pages {'$ref': '#/pictures/1'}\n",
            "40 Table None Table {'$ref': '#/pictures/1'}\n",
            "41 Structure None Structure {'$ref': '#/pictures/1'}\n",
            "42 OCR None OCR {'$ref': '#/pictures/1'}\n",
            "43 Model Pipeline None Model Pipeline {'$ref': '#/pictures/1'}\n",
            "44 Assemble results, None Assemble results, {'$ref': '#/pictures/1'}\n",
            "45 Apply document None Apply document {'$ref': '#/pictures/1'}\n",
            "46 post-processing None post-processing {'$ref': '#/pictures/1'}\n",
            "47 licensing (e.g. pymupdf [7]), poor speed or unrecoverable quality issues, such as merged text cells across far-apart text tokens or table columns (pypdfium, PyPDF) [15, 14]. None licensing (e.g. pymupdf [7]), poor speed or unrecoverable quality issues, such as merged text cells across far-apart text tokens or table columns (pypdfium, PyPDF) [15, 14]. {'$ref': '#/body'}\n",
            "48 We therefore decided to provide multiple backend choices, and additionally open-source a custombuilt PDF parser, which is based on the low-level qpdf [4] library. It is made available in a separate package named docling-parse and powers the default PDF backend in Docling. As an alternative, we provide a PDF backend relying on pypdfium , which may be a safe backup choice in certain cases, e.g. if issues are seen with particular font encodings. None We therefore decided to provide multiple backend choices, and additionally open-source a custombuilt PDF parser, which is based on the low-level qpdf [4] library. It is made available in a separate package named docling-parse and powers the default PDF backend in Docling. As an alternative, we provide a PDF backend relying on pypdfium , which may be a safe backup choice in certain cases, e.g. if issues are seen with particular font encodings. {'$ref': '#/body'}\n",
            "49 3.2 AI models 1 3.2 AI models {'$ref': '#/body'}\n",
            "50 As part of Docling, we initially release two highly capable AI models to the open-source community, which have been developed and published recently by our team. The first model is a layout analysis model, an accurate object-detector for page elements [13]. The second model is TableFormer [12, 9], a state-of-the-art table structure recognition model. We provide the pre-trained weights (hosted on huggingface) and a separate package for the inference code as docling-ibm-models . Both models are also powering the open-access deepsearch-experience, our cloud-native service for knowledge exploration tasks. None As part of Docling, we initially release two highly capable AI models to the open-source community, which have been developed and published recently by our team. The first model is a layout analysis model, an accurate object-detector for page elements [13]. The second model is TableFormer [12, 9], a state-of-the-art table structure recognition model. We provide the pre-trained weights (hosted on huggingface) and a separate package for the inference code as docling-ibm-models . Both models are also powering the open-access deepsearch-experience, our cloud-native service for knowledge exploration tasks. {'$ref': '#/body'}\n",
            "51 Layout Analysis Model 1 Layout Analysis Model {'$ref': '#/body'}\n",
            "52 Our layout analysis model is an object-detector which predicts the bounding-boxes and classes of various elements on the image of a given page. Its architecture is derived from RT-DETR [16] and re-trained on DocLayNet [13], our popular human-annotated dataset for document-layout analysis, among other proprietary datasets. For inference, our implementation relies on the onnxruntime [5]. None Our layout analysis model is an object-detector which predicts the bounding-boxes and classes of various elements on the image of a given page. Its architecture is derived from RT-DETR [16] and re-trained on DocLayNet [13], our popular human-annotated dataset for document-layout analysis, among other proprietary datasets. For inference, our implementation relies on the onnxruntime [5]. {'$ref': '#/body'}\n",
            "53 The Docling pipeline feeds page images at 72 dpi resolution, which can be processed on a single CPU with sub-second latency. All predicted bounding-box proposals for document elements are post-processed to remove overlapping proposals based on confidence and size, and then intersected with the text tokens in the PDF to group them into meaningful and complete units such as paragraphs, section titles, list items, captions, figures or tables. None The Docling pipeline feeds page images at 72 dpi resolution, which can be processed on a single CPU with sub-second latency. All predicted bounding-box proposals for document elements are post-processed to remove overlapping proposals based on confidence and size, and then intersected with the text tokens in the PDF to group them into meaningful and complete units such as paragraphs, section titles, list items, captions, figures or tables. {'$ref': '#/body'}\n",
            "54 Table Structure Recognition 1 Table Structure Recognition {'$ref': '#/body'}\n",
            "55 The TableFormer model [12], first published in 2022 and since refined with a custom structure token language [9], is a vision-transformer model for table structure recovery. It can predict the logical row and column structure of a given table based on an input image, and determine which table cells belong to column headers, row headers or the table body. Compared to earlier approaches, TableFormer handles many characteristics of tables, such as partial or no borderlines, empty cells, rows or columns, cell spans and hierarchy both on column-heading or row-heading level, tables with inconsistent indentation or alignment and other complexities. For inference, our implementation relies on PyTorch [2]. None The TableFormer model [12], first published in 2022 and since refined with a custom structure token language [9], is a vision-transformer model for table structure recovery. It can predict the logical row and column structure of a given table based on an input image, and determine which table cells belong to column headers, row headers or the table body. Compared to earlier approaches, TableFormer handles many characteristics of tables, such as partial or no borderlines, empty cells, rows or columns, cell spans and hierarchy both on column-heading or row-heading level, tables with inconsistent indentation or alignment and other complexities. For inference, our implementation relies on PyTorch [2]. {'$ref': '#/body'}\n",
            "56 3 None 3 {'$ref': '#/body'}\n",
            "57 The Docling pipeline feeds all table objects detected in the layout analysis to the TableFormer model, by providing an image-crop of the table and the included text cells. TableFormer structure predictions are matched back to the PDF cells in post-processing to avoid expensive re-transcription text in the table image. Typical tables require between 2 and 6 seconds to be processed on a standard CPU, strongly depending on the amount of included table cells. None The Docling pipeline feeds all table objects detected in the layout analysis to the TableFormer model, by providing an image-crop of the table and the included text cells. TableFormer structure predictions are matched back to the PDF cells in post-processing to avoid expensive re-transcription text in the table image. Typical tables require between 2 and 6 seconds to be processed on a standard CPU, strongly depending on the amount of included table cells. {'$ref': '#/body'}\n",
            "58 OCR 1 OCR {'$ref': '#/body'}\n",
            "59 Docling provides optional support for OCR, for example to cover scanned PDFs or content in bitmaps images embedded on a page. In our initial release, we rely on EasyOCR [1], a popular thirdparty OCR library with support for many languages. Docling, by default, feeds a high-resolution page image (216 dpi) to the OCR engine, to allow capturing small print detail in decent quality. While EasyOCR delivers reasonable transcription quality, we observe that it runs fairly slow on CPU (upwards of 30 seconds per page). None Docling provides optional support for OCR, for example to cover scanned PDFs or content in bitmaps images embedded on a page. In our initial release, we rely on EasyOCR [1], a popular thirdparty OCR library with support for many languages. Docling, by default, feeds a high-resolution page image (216 dpi) to the OCR engine, to allow capturing small print detail in decent quality. While EasyOCR delivers reasonable transcription quality, we observe that it runs fairly slow on CPU (upwards of 30 seconds per page). {'$ref': '#/body'}\n",
            "60 We are actively seeking collaboration from the open-source community to extend Docling with additional OCR backends and speed improvements. None We are actively seeking collaboration from the open-source community to extend Docling with additional OCR backends and speed improvements. {'$ref': '#/body'}\n",
            "61 3.3 Assembly 1 3.3 Assembly {'$ref': '#/body'}\n",
            "62 In the final pipeline stage, Docling assembles all prediction results produced on each page into a well-defined datatype that encapsulates a converted document, as defined in the auxiliary package docling-core . The generated document object is passed through a post-processing model which leverages several algorithms to augment features, such as detection of the document language, correcting the reading order, matching figures with captions and labelling metadata such as title, authors and references. The final output can then be serialized to JSON or transformed into a Markdown representation at the users request. None In the final pipeline stage, Docling assembles all prediction results produced on each page into a well-defined datatype that encapsulates a converted document, as defined in the auxiliary package docling-core . The generated document object is passed through a post-processing model which leverages several algorithms to augment features, such as detection of the document language, correcting the reading order, matching figures with captions and labelling metadata such as title, authors and references. The final output can then be serialized to JSON or transformed into a Markdown representation at the users request. {'$ref': '#/body'}\n",
            "63 3.4 Extensibility 1 3.4 Extensibility {'$ref': '#/body'}\n",
            "64 Docling provides a straight-forward interface to extend its capabilities, namely the model pipeline. A model pipeline constitutes the central part in the processing, following initial document parsing and preceding output assembly, and can be fully customized by sub-classing from an abstract baseclass ( BaseModelPipeline ) or cloning the default model pipeline. This effectively allows to fully customize the chain of models, add or replace models, and introduce additional pipeline configuration parameters. To use a custom model pipeline, the custom pipeline class to instantiate can be provided as an argument to the main document conversion methods. We invite everyone in the community to propose additional or alternative models and improvements. None Docling provides a straight-forward interface to extend its capabilities, namely the model pipeline. A model pipeline constitutes the central part in the processing, following initial document parsing and preceding output assembly, and can be fully customized by sub-classing from an abstract baseclass ( BaseModelPipeline ) or cloning the default model pipeline. This effectively allows to fully customize the chain of models, add or replace models, and introduce additional pipeline configuration parameters. To use a custom model pipeline, the custom pipeline class to instantiate can be provided as an argument to the main document conversion methods. We invite everyone in the community to propose additional or alternative models and improvements. {'$ref': '#/body'}\n",
            "65 Implementations of model classes must satisfy the python Callable interface. The __call__ method must accept an iterator over page objects, and produce another iterator over the page objects which were augmented with the additional features predicted by the model, by extending the provided PagePredictions data model accordingly. None Implementations of model classes must satisfy the python Callable interface. The __call__ method must accept an iterator over page objects, and produce another iterator over the page objects which were augmented with the additional features predicted by the model, by extending the provided PagePredictions data model accordingly. {'$ref': '#/body'}\n",
            "66 4 Performance 1 4 Performance {'$ref': '#/body'}\n",
            "67 In this section, we establish some reference numbers for the processing speed of Docling and the resource budget it requires. All tests in this section are run with default options on our standard test set distributed with Docling, which consists of three papers from arXiv and two IBM Redbooks, with a total of 225 pages. Measurements were taken using both available PDF backends on two different hardware systems: one MacBook Pro M3 Max, and one bare-metal server running Ubuntu 20.04 LTS on an Intel Xeon E5-2690 CPU. For reproducibility, we fixed the thread budget (through setting OMP NUM THREADS environment variable ) once to 4 (Docling default) and once to 16 (equal to full core count on the test hardware). All results are shown in Table 1. None In this section, we establish some reference numbers for the processing speed of Docling and the resource budget it requires. All tests in this section are run with default options on our standard test set distributed with Docling, which consists of three papers from arXiv and two IBM Redbooks, with a total of 225 pages. Measurements were taken using both available PDF backends on two different hardware systems: one MacBook Pro M3 Max, and one bare-metal server running Ubuntu 20.04 LTS on an Intel Xeon E5-2690 CPU. For reproducibility, we fixed the thread budget (through setting OMP NUM THREADS environment variable ) once to 4 (Docling default) and once to 16 (equal to full core count on the test hardware). All results are shown in Table 1. {'$ref': '#/body'}\n",
            "68 If you need to run Docling in very low-resource environments, please consider configuring the pypdfium backend. While it is faster and more memory efficient than the default docling-parse backend, it will come at the expense of worse quality results, especially in table structure recovery. None If you need to run Docling in very low-resource environments, please consider configuring the pypdfium backend. While it is faster and more memory efficient than the default docling-parse backend, it will come at the expense of worse quality results, especially in table structure recovery. {'$ref': '#/body'}\n",
            "69 Establishing GPU acceleration support for the AI models is currently work-in-progress and largely untested, but may work implicitly when CUDA is available and discovered by the onnxruntime and None Establishing GPU acceleration support for the AI models is currently work-in-progress and largely untested, but may work implicitly when CUDA is available and discovered by the onnxruntime and {'$ref': '#/body'}\n",
            "70 4 None 4 {'$ref': '#/body'}\n",
            "71 torch runtimes backing the Docling pipeline. We will deliver updates on this topic at in a future version of this report. None torch runtimes backing the Docling pipeline. We will deliver updates on this topic at in a future version of this report. {'$ref': '#/body'}\n",
            "72 Table 1: Runtime characteristics of Docling with the standard model pipeline and settings, on our test dataset of 225 pages, on two different systems. OCR is disabled. We show the time-to-solution (TTS), computed throughput in pages per second, and the peak memory used (resident set size) for both the Docling-native PDF backend and for the pypdfium backend, using 4 and 16 threads. None Table 1: Runtime characteristics of Docling with the standard model pipeline and settings, on our test dataset of 225 pages, on two different systems. OCR is disabled. We show the time-to-solution (TTS), computed throughput in pages per second, and the peak memory used (resident set size) for both the Docling-native PDF backend and for the pypdfium backend, using 4 and 16 threads. {'$ref': '#/tables/0'}\n",
            "73 5 Applications 1 5 Applications {'$ref': '#/body'}\n",
            "74 Thanks to the high-quality, richly structured document conversion achieved by Docling, its output qualifies for numerous downstream applications. For example, Docling can provide a base for detailed enterprise document search, passage retrieval or classification use-cases, or support knowledge extraction pipelines, allowing specific treatment of different structures in the document, such as tables, figures, section structure or references. For popular generative AI application patterns, such as retrieval-augmented generation (RAG), we provide quackling , an open-source package which capitalizes on Docling's feature-rich document output to enable document-native optimized vector embedding and chunking. It plugs in seamlessly with LLM frameworks such as LlamaIndex [8]. Since Docling is fast, stable and cheap to run, it also makes for an excellent choice to build document-derived datasets. With its powerful table structure recognition, it provides significant benefit to automated knowledge-base construction [11, 10]. Docling is also integrated within the open IBM data prep kit [6], which implements scalable data transforms to build large-scale multi-modal training datasets. None Thanks to the high-quality, richly structured document conversion achieved by Docling, its output qualifies for numerous downstream applications. For example, Docling can provide a base for detailed enterprise document search, passage retrieval or classification use-cases, or support knowledge extraction pipelines, allowing specific treatment of different structures in the document, such as tables, figures, section structure or references. For popular generative AI application patterns, such as retrieval-augmented generation (RAG), we provide quackling , an open-source package which capitalizes on Docling's feature-rich document output to enable document-native optimized vector embedding and chunking. It plugs in seamlessly with LLM frameworks such as LlamaIndex [8]. Since Docling is fast, stable and cheap to run, it also makes for an excellent choice to build document-derived datasets. With its powerful table structure recognition, it provides significant benefit to automated knowledge-base construction [11, 10]. Docling is also integrated within the open IBM data prep kit [6], which implements scalable data transforms to build large-scale multi-modal training datasets. {'$ref': '#/body'}\n",
            "75 6 Future work and contributions 1 6 Future work and contributions {'$ref': '#/body'}\n",
            "76 Docling is designed to allow easy extension of the model library and pipelines. In the future, we plan to extend Docling with several more models, such as a figure-classifier model, an equationrecognition model, a code-recognition model and more. This will help improve the quality of conversion for specific types of content, as well as augment extracted document metadata with additional information. Further investment into testing and optimizing GPU acceleration as well as improving the Docling-native PDF backend are on our roadmap, too. None Docling is designed to allow easy extension of the model library and pipelines. In the future, we plan to extend Docling with several more models, such as a figure-classifier model, an equationrecognition model, a code-recognition model and more. This will help improve the quality of conversion for specific types of content, as well as augment extracted document metadata with additional information. Further investment into testing and optimizing GPU acceleration as well as improving the Docling-native PDF backend are on our roadmap, too. {'$ref': '#/body'}\n",
            "77 We encourage everyone to propose or implement additional features and models, and will gladly take your inputs and contributions under review . The codebase of Docling is open for use and contribution, under the MIT license agreement and in alignment with our contributing guidelines included in the Docling repository. If you use Docling in your projects, please consider citing this technical report. None We encourage everyone to propose or implement additional features and models, and will gladly take your inputs and contributions under review . The codebase of Docling is open for use and contribution, under the MIT license agreement and in alignment with our contributing guidelines included in the Docling repository. If you use Docling in your projects, please consider citing this technical report. {'$ref': '#/body'}\n",
            "78 References 1 References {'$ref': '#/body'}\n",
            "79 [1] J. AI. Easyocr: Ready-to-use ocr with 80+ supported languages. https://github.com/ JaidedAI/EasyOCR , 2024. Version: 1.7.0. None [1] J. AI. Easyocr: Ready-to-use ocr with 80+ supported languages. https://github.com/ JaidedAI/EasyOCR , 2024. Version: 1.7.0. {'$ref': '#/groups/3'}\n",
            "80 [2] J. Ansel, E. Yang, H. He, N. Gimelshein, A. Jain, M. Voznesensky, B. Bao, P. Bell, D. Berard, E. Burovski, G. Chauhan, A. Chourdia, W. Constable, A. Desmaison, Z. DeVito, E. Ellison, W. Feng, J. Gong, M. Gschwind, B. Hirsh, S. Huang, K. Kalambarkar, L. Kirsch, M. Lazos, M. Lezcano, Y. Liang, J. Liang, Y. Lu, C. Luk, B. Maher, Y. Pan, C. Puhrsch, M. Reso, M. Saroufim, M. Y. Siraichi, H. Suk, M. Suo, P. Tillet, E. Wang, X. Wang, W. Wen, S. Zhang, X. Zhao, K. Zhou, R. Zou, A. Mathews, G. Chanan, P. Wu, and S. Chintala. Pytorch 2: Faster None [2] J. Ansel, E. Yang, H. He, N. Gimelshein, A. Jain, M. Voznesensky, B. Bao, P. Bell, D. Berard, E. Burovski, G. Chauhan, A. Chourdia, W. Constable, A. Desmaison, Z. DeVito, E. Ellison, W. Feng, J. Gong, M. Gschwind, B. Hirsh, S. Huang, K. Kalambarkar, L. Kirsch, M. Lazos, M. Lezcano, Y. Liang, J. Liang, Y. Lu, C. Luk, B. Maher, Y. Pan, C. Puhrsch, M. Reso, M. Saroufim, M. Y. Siraichi, H. Suk, M. Suo, P. Tillet, E. Wang, X. Wang, W. Wen, S. Zhang, X. Zhao, K. Zhou, R. Zou, A. Mathews, G. Chanan, P. Wu, and S. Chintala. Pytorch 2: Faster {'$ref': '#/groups/3'}\n",
            "81 5 None 5 {'$ref': '#/body'}\n",
            "82 machine learning through dynamic python bytecode transformation and graph compilation. In Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2 (ASPLOS '24) . ACM, 4 2024. doi: 10.1145/3620665.3640366. URL https://pytorch.org/assets/pytorch2-2.pdf . None machine learning through dynamic python bytecode transformation and graph compilation. In Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2 (ASPLOS '24) . ACM, 4 2024. doi: 10.1145/3620665.3640366. URL https://pytorch.org/assets/pytorch2-2.pdf . {'$ref': '#/body'}\n",
            "83 [3] C. Auer, M. Dolfi, A. Carvalho, C. B. Ramis, and P. W. Staar. Delivering document conversion as a cloud service with high throughput and responsiveness. In 2022 IEEE 15th International Conference on Cloud Computing (CLOUD) , pages 363-373. IEEE, 2022. None [3] C. Auer, M. Dolfi, A. Carvalho, C. B. Ramis, and P. W. Staar. Delivering document conversion as a cloud service with high throughput and responsiveness. In 2022 IEEE 15th International Conference on Cloud Computing (CLOUD) , pages 363-373. IEEE, 2022. {'$ref': '#/groups/4'}\n",
            "84 [4] J. Berkenbilt. Qpdf: A content-preserving pdf document transformer, 2024. URL https: //github.com/qpdf/qpdf . None [4] J. Berkenbilt. Qpdf: A content-preserving pdf document transformer, 2024. URL https: //github.com/qpdf/qpdf . {'$ref': '#/groups/4'}\n",
            "85 [5] O. R. developers. Onnx runtime. https://onnxruntime.ai/ , 2024. Version: 1.18.1. None [5] O. R. developers. Onnx runtime. https://onnxruntime.ai/ , 2024. Version: 1.18.1. {'$ref': '#/groups/4'}\n",
            "86 [6] IBM. Data Prep Kit: a community project to democratize and accelerate unstructured data preparation for LLM app developers, 2024. URL https://github.com/IBM/ data-prep-kit . None [6] IBM. Data Prep Kit: a community project to democratize and accelerate unstructured data preparation for LLM app developers, 2024. URL https://github.com/IBM/ data-prep-kit . {'$ref': '#/groups/4'}\n",
            "87 [7] A. S. Inc. PyMuPDF, 2024. URL https://github.com/pymupdf/PyMuPDF . None [7] A. S. Inc. PyMuPDF, 2024. URL https://github.com/pymupdf/PyMuPDF . {'$ref': '#/groups/4'}\n",
            "88 [8] J. Liu. LlamaIndex, 11 2022. URL https://github.com/jerryjliu/llama_index . None [8] J. Liu. LlamaIndex, 11 2022. URL https://github.com/jerryjliu/llama_index . {'$ref': '#/groups/4'}\n",
            "89 [9] M. Lysak, A. Nassar, N. Livathinos, C. Auer, and P. Staar. Optimized Table Tokenization for Table Structure Recognition. In Document Analysis and Recognition - ICDAR 2023: 17th International Conference, San Jos´ e, CA, USA, August 21-26, 2023, Proceedings, Part II , pages 37-50, Berlin, Heidelberg, Aug. 2023. Springer-Verlag. ISBN 978-3-031-41678-1. doi: 10. 1007/978-3-031-41679-8 3. URL https://doi.org/10.1007/978-3-031-41679-8_3 . None [9] M. Lysak, A. Nassar, N. Livathinos, C. Auer, and P. Staar. Optimized Table Tokenization for Table Structure Recognition. In Document Analysis and Recognition - ICDAR 2023: 17th International Conference, San Jos´ e, CA, USA, August 21-26, 2023, Proceedings, Part II , pages 37-50, Berlin, Heidelberg, Aug. 2023. Springer-Verlag. ISBN 978-3-031-41678-1. doi: 10. 1007/978-3-031-41679-8 3. URL https://doi.org/10.1007/978-3-031-41679-8_3 . {'$ref': '#/groups/4'}\n",
            "90 [10] L. Mishra, S. Dhibi, Y. Kim, C. Berrospi Ramis, S. Gupta, M. Dolfi, and P. Staar. Statements: Universal information extraction from tables with large language models for ESG KPIs. In D. Stammbach, J. Ni, T. Schimanski, K. Dutia, A. Singh, J. Bingler, C. Christiaen, N. Kushwaha, V. Muccione, S. A. Vaghefi, and M. Leippold, editors, Proceedings of the 1st Workshop on Natural Language Processing Meets Climate Change (ClimateNLP 2024) , pages 193-214, Bangkok, Thailand, Aug. 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024.climatenlp-1.15 . None [10] L. Mishra, S. Dhibi, Y. Kim, C. Berrospi Ramis, S. Gupta, M. Dolfi, and P. Staar. Statements: Universal information extraction from tables with large language models for ESG KPIs. In D. Stammbach, J. Ni, T. Schimanski, K. Dutia, A. Singh, J. Bingler, C. Christiaen, N. Kushwaha, V. Muccione, S. A. Vaghefi, and M. Leippold, editors, Proceedings of the 1st Workshop on Natural Language Processing Meets Climate Change (ClimateNLP 2024) , pages 193-214, Bangkok, Thailand, Aug. 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024.climatenlp-1.15 . {'$ref': '#/groups/4'}\n",
            "91 [11] L. Morin, V. Weber, G. I. Meijer, F. Yu, and P. W. J. Staar. Patcid: an open-access dataset of chemical structures in patent documents. Nature Communications , 15(1):6532, August 2024. ISSN 2041-1723. doi: 10.1038/s41467-024-50779-y. URL https://doi.org/10.1038/ s41467-024-50779-y . None [11] L. Morin, V. Weber, G. I. Meijer, F. Yu, and P. W. J. Staar. Patcid: an open-access dataset of chemical structures in patent documents. Nature Communications , 15(1):6532, August 2024. ISSN 2041-1723. doi: 10.1038/s41467-024-50779-y. URL https://doi.org/10.1038/ s41467-024-50779-y . {'$ref': '#/groups/4'}\n",
            "92 [12] A. Nassar, N. Livathinos, M. Lysak, and P. Staar. Tableformer: Table structure understanding with transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4614-4623, 2022. None [12] A. Nassar, N. Livathinos, M. Lysak, and P. Staar. Tableformer: Table structure understanding with transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4614-4623, 2022. {'$ref': '#/groups/4'}\n",
            "93 [13] B. Pfitzmann, C. Auer, M. Dolfi, A. S. Nassar, and P. Staar. Doclaynet: a large humanannotated dataset for document-layout segmentation. pages 3743-3751, 2022. None [13] B. Pfitzmann, C. Auer, M. Dolfi, A. S. Nassar, and P. Staar. Doclaynet: a large humanannotated dataset for document-layout segmentation. pages 3743-3751, 2022. {'$ref': '#/groups/4'}\n",
            "94 [14] pypdf Maintainers. pypdf: A Pure-Python PDF Library, 2024. URL https://github.com/ py-pdf/pypdf . None [14] pypdf Maintainers. pypdf: A Pure-Python PDF Library, 2024. URL https://github.com/ py-pdf/pypdf . {'$ref': '#/groups/4'}\n",
            "95 [15] P. Team. PyPDFium2: Python bindings for PDFium, 2024. URL https://github.com/ pypdfium2-team/pypdfium2 . None [15] P. Team. PyPDFium2: Python bindings for PDFium, 2024. URL https://github.com/ pypdfium2-team/pypdfium2 . {'$ref': '#/groups/4'}\n",
            "96 [16] Y. Zhao, W. Lv, S. Xu, J. Wei, G. Wang, Q. Dang, Y. Liu, and J. Chen. Detrs beat yolos on real-time object detection, 2023. None [16] Y. Zhao, W. Lv, S. Xu, J. Wei, G. Wang, Q. Dang, Y. Liu, and J. Chen. Detrs beat yolos on real-time object detection, 2023. {'$ref': '#/groups/4'}\n",
            "97 6 None 6 {'$ref': '#/body'}\n",
            "98 arXiv:2206.01062v1  [cs.CV]  2 Jun 2022 None arXiv:2206.01062v1  [cs.CV]  2 Jun 2022 {'$ref': '#/body'}\n",
            "99 Appendix 1 Appendix {'$ref': '#/body'}\n",
            "100 In this section, we illustrate a few examples of Docling's output in Markdown and JSON. None In this section, we illustrate a few examples of Docling's output in Markdown and JSON. {'$ref': '#/body'}\n",
            "101 DocLayNet: A Large Human-Annotated Dataset for Document-Layout Analysis 1 DocLayNet: A Large Human-Annotated Dataset for Document-Layout Analysis {'$ref': '#/body'}\n",
            "102 DocLayNet: A Large Human-Annotated Dataset for Document-Layout Analysis 1 DocLayNet: A Large Human-Annotated Dataset for Document-Layout Analysis {'$ref': '#/body'}\n",
            "103 Birgit Pfitzmann IBM Research Rueschlikon, Switzerland bpf@zurich.ibm.com None Birgit Pfitzmann IBM Research Rueschlikon, Switzerland bpf@zurich.ibm.com {'$ref': '#/body'}\n",
            "104 Christoph Auer IBM Research Rueschlikon, Switzerland cau@zurich.ibm.com None Christoph Auer IBM Research Rueschlikon, Switzerland cau@zurich.ibm.com {'$ref': '#/body'}\n",
            "105 Michele Dolfi IBM Research Rueschlikon, Switzerland dol@zurich.ibm.com None Michele Dolfi IBM Research Rueschlikon, Switzerland dol@zurich.ibm.com {'$ref': '#/body'}\n",
            "106 Ahmed S. Nassar IBM Research Rueschlikon, Switzerland ahn@zurich.ibm.com None Ahmed S. Nassar IBM Research Rueschlikon, Switzerland ahn@zurich.ibm.com {'$ref': '#/body'}\n",
            "107 Peter Staar IBM Research Rueschlikon, Switzerland taa@zurich.ibm.com None Peter Staar IBM Research Rueschlikon, Switzerland taa@zurich.ibm.com {'$ref': '#/body'}\n",
            "108 ABSTRACT 1 ABSTRACT {'$ref': '#/body'}\n",
            "109 Accurate document layout analysis is a key requirement for highquality PDF document conversion. With the recent availability of public, large ground-truth datasets such as PubLayNet and DocBank, deep-learning models have proven to be very effective at layout detection and segmentation. While these datasets are of adequate size to train such models, they severely lack in layout variability since they are sourced from scientific article repositories such as PubMed and arXiv only. Consequently, the accuracy of the layout segmentation drops significantly when these models are applied on more challenging and diverse layouts. In this paper, we present DocLayNet , a new, publicly available, document-layout annotation dataset in COCO format. It contains 80863 manually annotated pages from diverse data sources to represent a wide variability in layouts. For each PDF page, the layout annotations provide labelled bounding-boxes with a choice of 11 distinct classes. DocLayNet also provides a subset of double- and triple-annotated pages to determine the inter-annotator agreement. In multiple experiments, we provide baseline accuracy scores (in mAP) for a set of popular object detection models. We also demonstrate that these models fall approximately 10% behind the inter-annotator agreement. Furthermore, we provide evidence that DocLayNet is of sufficient size. Lastly, we compare models trained on PubLayNet, DocBank and DocLayNet, showing that layout predictions of the DocLayNettrained models are more robust and thus the preferred choice for general-purpose document-layout analysis. None Accurate document layout analysis is a key requirement for highquality PDF document conversion. With the recent availability of public, large ground-truth datasets such as PubLayNet and DocBank, deep-learning models have proven to be very effective at layout detection and segmentation. While these datasets are of adequate size to train such models, they severely lack in layout variability since they are sourced from scientific article repositories such as PubMed and arXiv only. Consequently, the accuracy of the layout segmentation drops significantly when these models are applied on more challenging and diverse layouts. In this paper, we present DocLayNet , a new, publicly available, document-layout annotation dataset in COCO format. It contains 80863 manually annotated pages from diverse data sources to represent a wide variability in layouts. For each PDF page, the layout annotations provide labelled bounding-boxes with a choice of 11 distinct classes. DocLayNet also provides a subset of double- and triple-annotated pages to determine the inter-annotator agreement. In multiple experiments, we provide baseline accuracy scores (in mAP) for a set of popular object detection models. We also demonstrate that these models fall approximately 10% behind the inter-annotator agreement. Furthermore, we provide evidence that DocLayNet is of sufficient size. Lastly, we compare models trained on PubLayNet, DocBank and DocLayNet, showing that layout predictions of the DocLayNettrained models are more robust and thus the preferred choice for general-purpose document-layout analysis. {'$ref': '#/body'}\n",
            "110 CCS CONCEPTS 1 CCS CONCEPTS {'$ref': '#/body'}\n",
            "111 · Informationsystems → Documentstructure ; · Appliedcomputing → Document analysis ; · Computing methodologies → Machine learning Computer vision ; ; Object detection ; None · Informationsystems → Documentstructure ; · Appliedcomputing → Document analysis ; · Computing methodologies → Machine learning Computer vision ; ; Object detection ; {'$ref': '#/body'}\n",
            "112 Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). KDD '22, August 14-18, 2022, Washington, DC, USA © 2022 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-9385-0/22/08. https://doi.org/10.1145/3534678.3539043 None Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). KDD '22, August 14-18, 2022, Washington, DC, USA © 2022 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-9385-0/22/08. https://doi.org/10.1145/3534678.3539043 {'$ref': '#/body'}\n",
            "113 Birgit Pfitzmann IBM Research Rueschlikon, Switzerland bpf@zurich.ibm.com None Birgit Pfitzmann IBM Research Rueschlikon, Switzerland bpf@zurich.ibm.com {'$ref': '#/groups/5'}\n",
            "114 Christoph Auer IBM Research Rueschlikon, Switzerland cau@zurich.ibm.com None Christoph Auer IBM Research Rueschlikon, Switzerland cau@zurich.ibm.com {'$ref': '#/groups/5'}\n",
            "115 Michele Dolfi IBM Research Rueschlikon, Switzerland dol@zurich.ibm.com None Michele Dolfi IBM Research Rueschlikon, Switzerland dol@zurich.ibm.com {'$ref': '#/groups/5'}\n",
            "116 Ahmed S. Nassar IBM Research Rueschlikon, Switzerland ahn@zurich.ibm.com None Ahmed S. Nassar IBM Research Rueschlikon, Switzerland ahn@zurich.ibm.com {'$ref': '#/groups/5'}\n",
            "117 Peter Staar IBM Research Rueschlikon, Switzerland taa@zurich.ibm.com None Peter Staar IBM Research Rueschlikon, Switzerland taa@zurich.ibm.com {'$ref': '#/groups/5'}\n",
            "118 ABSTRACT 1 ABSTRACT {'$ref': '#/body'}\n",
            "119 Accurate document layout analysis is a key requirement for highquality PDF document conversion. With the recent availability of public, large groundtruth datasets such as PubLayNet and DocBank, deep-learning models have proven to be very effective at layout detection and segmentation. While these datasets are of adequate size to train such models, they severely lack in layout variability since they are sourced from scientific article repositories such as PubMed and arXiv only. Consequently, the accuracy of the layout segmentation drops significantly when these models are applied on more challenging and diverse layouts. In this paper, we present DocLayNet , a new, publicly available, document-layout annotation dataset in COCO format. It contains 80863 manually annotated pages from diverse data sources to represent a wide variability in layouts. For each PDF page, the layout annotations provide labelled bounding-boxes with a choice of 11 distinct classes. DocLayNet also provides a subset of double- and triple-annotated pages to determine the inter-annotator agreement. In multiple experiments, we provide baseline accuracy scores (in mAP) for a set of popular object detection models. We also demonstrate that these models fall approximately 10% behind the inter-annotator agreement. Furthermore, we provide evidence that DocLayNet is of sufficient size. Lastly, we compare models trained on PubLayNet, DocBank and DocLayNet, showing that layout predictions of the DocLayNettrained models are more robust and thus the preferred choice for general-purpose document-layout analysis. None Accurate document layout analysis is a key requirement for highquality PDF document conversion. With the recent availability of public, large groundtruth datasets such as PubLayNet and DocBank, deep-learning models have proven to be very effective at layout detection and segmentation. While these datasets are of adequate size to train such models, they severely lack in layout variability since they are sourced from scientific article repositories such as PubMed and arXiv only. Consequently, the accuracy of the layout segmentation drops significantly when these models are applied on more challenging and diverse layouts. In this paper, we present DocLayNet , a new, publicly available, document-layout annotation dataset in COCO format. It contains 80863 manually annotated pages from diverse data sources to represent a wide variability in layouts. For each PDF page, the layout annotations provide labelled bounding-boxes with a choice of 11 distinct classes. DocLayNet also provides a subset of double- and triple-annotated pages to determine the inter-annotator agreement. In multiple experiments, we provide baseline accuracy scores (in mAP) for a set of popular object detection models. We also demonstrate that these models fall approximately 10% behind the inter-annotator agreement. Furthermore, we provide evidence that DocLayNet is of sufficient size. Lastly, we compare models trained on PubLayNet, DocBank and DocLayNet, showing that layout predictions of the DocLayNettrained models are more robust and thus the preferred choice for general-purpose document-layout analysis. {'$ref': '#/body'}\n",
            "120 CCS CONCEPTS 1 CCS CONCEPTS {'$ref': '#/body'}\n",
            "121 Æ Information systems → Document structure ; Æ Applied computing → Document analysis ; Æ Computing methodologies → Machine learning ; Computer vision ; Object detection ; None Æ Information systems → Document structure ; Æ Applied computing → Document analysis ; Æ Computing methodologies → Machine learning ; Computer vision ; Object detection ; {'$ref': '#/body'}\n",
            "122 Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). None Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). {'$ref': '#/body'}\n",
            "123 KDD '22, August 14-18, 2022, Washington, DC, USA ' 2022 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-9385-0/22/08. https://doi.org/10.1145/3534678.3539043 None KDD '22, August 14-18, 2022, Washington, DC, USA ' 2022 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-9385-0/22/08. https://doi.org/10.1145/3534678.3539043 {'$ref': '#/body'}\n",
            "124 Figure 1: Four examples of complex page layouts across different document categories None Figure 1: Four examples of complex page layouts across different document categories {'$ref': '#/body'}\n",
            "125 KEYWORDS 1 KEYWORDS {'$ref': '#/body'}\n",
            "126 PDF document conversion, layout segmentation, object-detection, data set, Machine Learning None PDF document conversion, layout segmentation, object-detection, data set, Machine Learning {'$ref': '#/body'}\n",
            "127 ACM Reference Format: 1 ACM Reference Format: {'$ref': '#/body'}\n",
            "128 Birgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S. Nassar, and Peter Staar. 2022. DocLayNet: A Large Human-Annotated Dataset for DocumentLayout Analysis. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD '22), August 14-18, 2022, Washington, DC, USA. ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/ 3534678.3539043 None Birgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S. Nassar, and Peter Staar. 2022. DocLayNet: A Large Human-Annotated Dataset for DocumentLayout Analysis. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD '22), August 14-18, 2022, Washington, DC, USA. ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/ 3534678.3539043 {'$ref': '#/body'}\n",
            "129 13 None 13 {'$ref': '#/pictures/2'}\n",
            "130 USING THE VERTICAL TUBE - None USING THE VERTICAL TUBE - {'$ref': '#/pictures/2'}\n",
            "131 MODELS AY1 230/1 234 None MODELS AY1 230/1 234 {'$ref': '#/pictures/2'}\n",
            "132 1. The vertical tube can be used for None 1. The vertical tube can be used for {'$ref': '#/pictures/2'}\n",
            "133 ional viewing or to phot None ional viewing or to phot {'$ref': '#/pictures/2'}\n",
            "134 instruct None instruct {'$ref': '#/pictures/2'}\n",
            "135 graph None graph {'$ref': '#/pictures/2'}\n",
            "136 the image with a dig tal camera or a None the image with a dig tal camera or a {'$ref': '#/pictures/2'}\n",
            "137 micro TV unit None micro TV unit {'$ref': '#/pictures/2'}\n",
            "138 2. Lo None 2. Lo {'$ref': '#/pictures/2'}\n",
            "139 sen the ret None sen the ret {'$ref': '#/pictures/2'}\n",
            "140 ntion screw, then rota e None ntion screw, then rota e {'$ref': '#/pictures/2'}\n",
            "141 the adjustment ring to change the None the adjustment ring to change the {'$ref': '#/pictures/2'}\n",
            "142 re that both the images in None re that both the images in {'$ref': '#/pictures/2'}\n",
            "143 length of the vertical tube. None length of the vertical tube. {'$ref': '#/pictures/2'}\n",
            "144 3. Make su None 3. Make su {'$ref': '#/pictures/2'}\n",
            "145 OPERATION (cont.) None OPERATION (cont.) {'$ref': '#/pictures/2'}\n",
            "146 SELECTING OBJECTIVE None SELECTING OBJECTIVE {'$ref': '#/pictures/2'}\n",
            "147 MAGNIFICATION None MAGNIFICATION {'$ref': '#/pictures/2'}\n",
            "148 1. There are two objectives. The lower None 1. There are two objectives. The lower {'$ref': '#/pictures/2'}\n",
            "149 magnif cation objective has a greater None magnif cation objective has a greater {'$ref': '#/pictures/2'}\n",
            "150 depth of  ield and view. None depth of  ield and view. {'$ref': '#/pictures/2'}\n",
            "151 2. In order to observe the specimen None 2. In order to observe the specimen {'$ref': '#/pictures/2'}\n",
            "152 easily use the lower magnif cation None easily use the lower magnif cation {'$ref': '#/pictures/2'}\n",
            "153 objective first. Then, by rotating the None objective first. Then, by rotating the {'$ref': '#/pictures/2'}\n",
            "154 case, the magnif cation can be None case, the magnif cation can be {'$ref': '#/pictures/2'}\n",
            "155 changed. None changed. {'$ref': '#/pictures/2'}\n",
            "156 CHANGING THE INTERPUPIL ARY None CHANGING THE INTERPUPIL ARY {'$ref': '#/pictures/2'}\n",
            "157 DISTANCE None DISTANCE {'$ref': '#/pictures/2'}\n",
            "158 1. The distance betwe None 1. The distance betwe {'$ref': '#/pictures/2'}\n",
            "159 n the observer's None n the observer's {'$ref': '#/pictures/2'}\n",
            "160 pupils is the interpupil ary distance. None pupils is the interpupil ary distance. {'$ref': '#/pictures/2'}\n",
            "161 2. To adjust  he interpupil ary distance None 2. To adjust  he interpupil ary distance {'$ref': '#/pictures/2'}\n",
            "162 rotate the prism caps until both eyes None rotate the prism caps until both eyes {'$ref': '#/pictures/2'}\n",
            "163 coincide with the image in the None coincide with the image in the {'$ref': '#/pictures/2'}\n",
            "164 eyepiece. None eyepiece. {'$ref': '#/pictures/2'}\n",
            "165 FOCUSING None FOCUSING {'$ref': '#/pictures/2'}\n",
            "166 1. Remove the lens protective cover. None 1. Remove the lens protective cover. {'$ref': '#/pictures/2'}\n",
            "167 2. Place the specimen on the working None 2. Place the specimen on the working {'$ref': '#/pictures/2'}\n",
            "168 stage. None stage. {'$ref': '#/pictures/2'}\n",
            "169 3. Focus the specimen with the left eye None 3. Focus the specimen with the left eye {'$ref': '#/pictures/2'}\n",
            "170 first while turni None first while turni {'$ref': '#/pictures/2'}\n",
            "171 g the focus knob until None g the focus knob until {'$ref': '#/pictures/2'}\n",
            "172 the image ap None the image ap {'$ref': '#/pictures/2'}\n",
            "173 ears clear and sharp. None ears clear and sharp. {'$ref': '#/pictures/2'}\n",
            "174 4. Rotate the right eyepiece ring until the None 4. Rotate the right eyepiece ring until the {'$ref': '#/pictures/2'}\n",
            "175 images in each eyepiece coincide and None images in each eyepiece coincide and {'$ref': '#/pictures/2'}\n",
            "176 are sharp and clear. None are sharp and clear. {'$ref': '#/pictures/2'}\n",
            "177 CHANGING THE BULB None CHANGING THE BULB {'$ref': '#/pictures/2'}\n",
            "178 1. Discon None 1. Discon {'$ref': '#/pictures/2'}\n",
            "179 ect  he power cord. None ect  he power cord. {'$ref': '#/pictures/2'}\n",
            "180 2. When the bulb is co None 2. When the bulb is co {'$ref': '#/pictures/2'}\n",
            "181 l, remove the None l, remove the {'$ref': '#/pictures/2'}\n",
            "182 oblique il uminator cap and remove None oblique il uminator cap and remove {'$ref': '#/pictures/2'}\n",
            "183 the halogen bulb with cap. None the halogen bulb with cap. {'$ref': '#/pictures/2'}\n",
            "184 3. Replace with a new halogen bulb. None 3. Replace with a new halogen bulb. {'$ref': '#/pictures/2'}\n",
            "185 4. Open the window in the base plate and None 4. Open the window in the base plate and {'$ref': '#/pictures/2'}\n",
            "186 replace the halogen lamp or None replace the halogen lamp or {'$ref': '#/pictures/2'}\n",
            "187 fluorescent lamp of transmit ed None fluorescent lamp of transmit ed {'$ref': '#/pictures/2'}\n",
            "188 il uminator. None il uminator. {'$ref': '#/pictures/2'}\n",
            "189 FOCUSING None FOCUSING {'$ref': '#/pictures/2'}\n",
            "190 1. Turn the focusing knob away or toward None 1. Turn the focusing knob away or toward {'$ref': '#/pictures/2'}\n",
            "191 you until a clear image is viewed. None you until a clear image is viewed. {'$ref': '#/pictures/2'}\n",
            "192 2. If the image is unclear, adjust  he None 2. If the image is unclear, adjust  he {'$ref': '#/pictures/2'}\n",
            "193 height of the el None height of the el {'$ref': '#/pictures/2'}\n",
            "194 vator up or down, None vator up or down, {'$ref': '#/pictures/2'}\n",
            "195 then turn the focusing knob again. None then turn the focusing knob again. {'$ref': '#/pictures/2'}\n",
            "196 ZO None ZO {'$ref': '#/pictures/2'}\n",
            "197 M MAGNIFICATION None M MAGNIFICATION {'$ref': '#/pictures/2'}\n",
            "198 1. Turn the zo None 1. Turn the zo {'$ref': '#/pictures/2'}\n",
            "199 m magnif cation knob to None m magnif cation knob to {'$ref': '#/pictures/2'}\n",
            "200 the desired magnif cation and field of None the desired magnif cation and field of {'$ref': '#/pictures/2'}\n",
            "201 view. None view. {'$ref': '#/pictures/2'}\n",
            "202 2. In most situations, it is recommended None 2. In most situations, it is recommended {'$ref': '#/pictures/2'}\n",
            "203 that you focus at  he lowest None that you focus at  he lowest {'$ref': '#/pictures/2'}\n",
            "204 magnif cation, then move to a higher None magnif cation, then move to a higher {'$ref': '#/pictures/2'}\n",
            "205 magnif cation and re-focus as None magnif cation and re-focus as {'$ref': '#/pictures/2'}\n",
            "206 neces ary. None neces ary. {'$ref': '#/pictures/2'}\n",
            "207 3. If the image is not clear to both eyes None 3. If the image is not clear to both eyes {'$ref': '#/pictures/2'}\n",
            "208 at  he same time, the diopter  ing may None at  he same time, the diopter  ing may {'$ref': '#/pictures/2'}\n",
            "209 ne None ne {'$ref': '#/pictures/2'}\n",
            "210 d adjustment. None d adjustment. {'$ref': '#/pictures/2'}\n",
            "211 DIOPTER RING ADJUSTMENT None DIOPTER RING ADJUSTMENT {'$ref': '#/pictures/2'}\n",
            "212 1. To adjust  he eyepiece for viewing with None 1. To adjust  he eyepiece for viewing with {'$ref': '#/pictures/2'}\n",
            "213 or without eyeglas None or without eyeglas {'$ref': '#/pictures/2'}\n",
            "214 es and for None es and for {'$ref': '#/pictures/2'}\n",
            "215 dif erences in acuity betwe None dif erences in acuity betwe {'$ref': '#/pictures/2'}\n",
            "216 n the right None n the right {'$ref': '#/pictures/2'}\n",
            "217 and left eyes, fol ow the fol owing None and left eyes, fol ow the fol owing {'$ref': '#/pictures/2'}\n",
            "218 steps: None steps: {'$ref': '#/pictures/2'}\n",
            "219 a. Observe an image through the left None a. Observe an image through the left {'$ref': '#/pictures/2'}\n",
            "220 eyepiece and bring a specif c point None eyepiece and bring a specif c point {'$ref': '#/pictures/2'}\n",
            "221 into focus using the focus knob. None into focus using the focus knob. {'$ref': '#/pictures/2'}\n",
            "222 b. By turni None b. By turni {'$ref': '#/pictures/2'}\n",
            "223 g the diopter  ing None g the diopter  ing {'$ref': '#/pictures/2'}\n",
            "224 adjustment for the left eyepiece, None adjustment for the left eyepiece, {'$ref': '#/pictures/2'}\n",
            "225 bring the same point into sharp None bring the same point into sharp {'$ref': '#/pictures/2'}\n",
            "226 focus. None focus. {'$ref': '#/pictures/2'}\n",
            "227 c.Then bring the same point into None c.Then bring the same point into {'$ref': '#/pictures/2'}\n",
            "228 focus through the right eyepiece None focus through the right eyepiece {'$ref': '#/pictures/2'}\n",
            "229 by turni None by turni {'$ref': '#/pictures/2'}\n",
            "230 g the right diopter  ing. None g the right diopter  ing. {'$ref': '#/pictures/2'}\n",
            "231 d.With more than one viewer, each None d.With more than one viewer, each {'$ref': '#/pictures/2'}\n",
            "232 viewer should note their own None viewer should note their own {'$ref': '#/pictures/2'}\n",
            "233 diopter  ing posit on for the left None diopter  ing posit on for the left {'$ref': '#/pictures/2'}\n",
            "234 and right eyepieces, then before None and right eyepieces, then before {'$ref': '#/pictures/2'}\n",
            "235 viewing set  he diopter  ing None viewing set  he diopter  ing {'$ref': '#/pictures/2'}\n",
            "236 adjustments to that set ing. None adjustments to that set ing. {'$ref': '#/pictures/2'}\n",
            "237 CHANGING THE BULB None CHANGING THE BULB {'$ref': '#/pictures/2'}\n",
            "238 1. Discon None 1. Discon {'$ref': '#/pictures/2'}\n",
            "239 ect  he power cord from the None ect  he power cord from the {'$ref': '#/pictures/2'}\n",
            "240 el None el {'$ref': '#/pictures/2'}\n",
            "241 ctrical outlet. None ctrical outlet. {'$ref': '#/pictures/2'}\n",
            "242 2. When the bulb is co None 2. When the bulb is co {'$ref': '#/pictures/2'}\n",
            "243 l, remove the None l, remove the {'$ref': '#/pictures/2'}\n",
            "244 oblique il uminator cap and remove None oblique il uminator cap and remove {'$ref': '#/pictures/2'}\n",
            "245 the halogen bulb with cap. None the halogen bulb with cap. {'$ref': '#/pictures/2'}\n",
            "246 3. Replace with a new halogen bulb. None 3. Replace with a new halogen bulb. {'$ref': '#/pictures/2'}\n",
            "247 4. Open the window in the base plate None 4. Open the window in the base plate {'$ref': '#/pictures/2'}\n",
            "248 and replace the halogen lamp or None and replace the halogen lamp or {'$ref': '#/pictures/2'}\n",
            "249 fluorescent lamp of transmit ed None fluorescent lamp of transmit ed {'$ref': '#/pictures/2'}\n",
            "250 il uminator. None il uminator. {'$ref': '#/pictures/2'}\n",
            "251 Model AY1 None Model AY1 {'$ref': '#/pictures/2'}\n",
            "252 230 None 230 {'$ref': '#/pictures/2'}\n",
            "253 Model AY1 None Model AY1 {'$ref': '#/pictures/2'}\n",
            "254 234 None 234 {'$ref': '#/pictures/2'}\n",
            "255 14 None 14 {'$ref': '#/pictures/2'}\n",
            "256 Objectives None Objectives {'$ref': '#/pictures/2'}\n",
            "257 Revolving Tur et None Revolving Tur et {'$ref': '#/pictures/2'}\n",
            "258 Coarse None Coarse {'$ref': '#/pictures/2'}\n",
            "259 Adju tment None Adju tment {'$ref': '#/pictures/2'}\n",
            "260 Knob None Knob {'$ref': '#/pictures/2'}\n",
            "261 MODEL AY11236 None MODEL AY11236 {'$ref': '#/pictures/2'}\n",
            "262 MICROSCOPE USAGE None MICROSCOPE USAGE {'$ref': '#/pictures/2'}\n",
            "263 BARSKA Model AY1 None BARSKA Model AY1 {'$ref': '#/pictures/2'}\n",
            "264 236 is a powerful fixed power compound None 236 is a powerful fixed power compound {'$ref': '#/pictures/2'}\n",
            "265 microscope designed for biological studies such as specimen None microscope designed for biological studies such as specimen {'$ref': '#/pictures/2'}\n",
            "266 examination. It can also be used for examining bacteria and None examination. It can also be used for examining bacteria and {'$ref': '#/pictures/2'}\n",
            "267 for general clinical and medical studies and other scientif c uses. None for general clinical and medical studies and other scientif c uses. {'$ref': '#/pictures/2'}\n",
            "268 CONSTRUCTION None CONSTRUCTION {'$ref': '#/pictures/2'}\n",
            "269 BARSKA Model AY1 None BARSKA Model AY1 {'$ref': '#/pictures/2'}\n",
            "270 236 is a fixed power compound microscope. None 236 is a fixed power compound microscope. {'$ref': '#/pictures/2'}\n",
            "271 It is constructed with two optical paths at  he same angle. It is None It is constructed with two optical paths at  he same angle. It is {'$ref': '#/pictures/2'}\n",
            "272 equip None equip {'$ref': '#/pictures/2'}\n",
            "273 ed with transmit ed il umination. By using this instrument, None ed with transmit ed il umination. By using this instrument, {'$ref': '#/pictures/2'}\n",
            "274 the user can observe specimens at magnif cation from 40x to None the user can observe specimens at magnif cation from 40x to {'$ref': '#/pictures/2'}\n",
            "275 10 None 10 {'$ref': '#/pictures/2'}\n",
            "276 0x by selecting the desired objective lens. Coarse and fine None 0x by selecting the desired objective lens. Coarse and fine {'$ref': '#/pictures/2'}\n",
            "277 focus adjustments provide ac uracy and image detail. The rotating None focus adjustments provide ac uracy and image detail. The rotating {'$ref': '#/pictures/2'}\n",
            "278 head al ows the user to posit on the eyepieces for maximum None head al ows the user to posit on the eyepieces for maximum {'$ref': '#/pictures/2'}\n",
            "279 viewing comfort and easy ac es  to al  adjustment knobs. None viewing comfort and easy ac es  to al  adjustment knobs. {'$ref': '#/pictures/2'}\n",
            "280 Model AY1 None Model AY1 {'$ref': '#/pictures/2'}\n",
            "281 236 None 236 {'$ref': '#/pictures/2'}\n",
            "282 Fine None Fine {'$ref': '#/pictures/2'}\n",
            "283 Adjustment None Adjustment {'$ref': '#/pictures/2'}\n",
            "284 Knob None Knob {'$ref': '#/pictures/2'}\n",
            "285 Stage None Stage {'$ref': '#/pictures/2'}\n",
            "286 Condenser None Condenser {'$ref': '#/pictures/2'}\n",
            "287 F None F {'$ref': '#/pictures/2'}\n",
            "288 cusi None cusi {'$ref': '#/pictures/2'}\n",
            "289 g None g {'$ref': '#/pictures/2'}\n",
            "290 Knob None Knob {'$ref': '#/pictures/2'}\n",
            "291 Eyepiec None Eyepiec {'$ref': '#/pictures/2'}\n",
            "292 Stand None Stand {'$ref': '#/pictures/2'}\n",
            "293 Lamp None Lamp {'$ref': '#/pictures/2'}\n",
            "294 On/Of None On/Of {'$ref': '#/pictures/2'}\n",
            "295 Switch None Switch {'$ref': '#/pictures/2'}\n",
            "296 Lamp None Lamp {'$ref': '#/pictures/2'}\n",
            "297 Power None Power {'$ref': '#/pictures/2'}\n",
            "298 Cord None Cord {'$ref': '#/pictures/2'}\n",
            "299 Rota ing Head None Rota ing Head {'$ref': '#/pictures/2'}\n",
            "300 Stage Clip None Stage Clip {'$ref': '#/pictures/2'}\n",
            "301 Adjustment None Adjustment {'$ref': '#/pictures/2'}\n",
            "302 Interpupil ary Slide Adjustment None Interpupil ary Slide Adjustment {'$ref': '#/pictures/2'}\n",
            "303 29 None 29 {'$ref': '#/pictures/3'}\n",
            "304 signs, signals and road markings None signs, signals and road markings {'$ref': '#/pictures/3'}\n",
            "305 3 None 3 {'$ref': '#/pictures/3'}\n",
            "306 In None In {'$ref': '#/pictures/3'}\n",
            "307 chapter 2, you and your vehicle None chapter 2, you and your vehicle {'$ref': '#/pictures/3'}\n",
            "308 , you learned about None , you learned about {'$ref': '#/pictures/3'}\n",
            "309 some of the controls in your vehicle. This chapter is a handy None some of the controls in your vehicle. This chapter is a handy {'$ref': '#/pictures/3'}\n",
            "310 reference section that gives examples of the most common None reference section that gives examples of the most common {'$ref': '#/pictures/3'}\n",
            "311 signs, signals and road markings that ke None signs, signals and road markings that ke {'$ref': '#/pictures/3'}\n",
            "312 p traffic organized None p traffic organized {'$ref': '#/pictures/3'}\n",
            "313 and flowing smo None and flowing smo {'$ref': '#/pictures/3'}\n",
            "314 thly. None thly. {'$ref': '#/pictures/3'}\n",
            "315 Signs None Signs {'$ref': '#/pictures/3'}\n",
            "316 There are thre None There are thre {'$ref': '#/pictures/3'}\n",
            "317 ways to read signs: by their shape, colour and None ways to read signs: by their shape, colour and {'$ref': '#/pictures/3'}\n",
            "318 the mes ages printed on them. Understanding these thre None the mes ages printed on them. Understanding these thre {'$ref': '#/pictures/3'}\n",
            "319 ways None ways {'$ref': '#/pictures/3'}\n",
            "320 of clas None of clas {'$ref': '#/pictures/3'}\n",
            "321 ifying signs wil  help you figure out  he meani None ifying signs wil  help you figure out  he meani {'$ref': '#/pictures/3'}\n",
            "322 g of signs None g of signs {'$ref': '#/pictures/3'}\n",
            "323 that are new to you. None that are new to you. {'$ref': '#/pictures/3'}\n",
            "324 Stop None Stop {'$ref': '#/pictures/3'}\n",
            "325 Yield the right-of-way None Yield the right-of-way {'$ref': '#/pictures/3'}\n",
            "326 Shows driv ng None Shows driv ng {'$ref': '#/pictures/3'}\n",
            "327 regulations None regulations {'$ref': '#/pictures/3'}\n",
            "328 Explains lane use None Explains lane use {'$ref': '#/pictures/3'}\n",
            "329 Scho None Scho {'$ref': '#/pictures/3'}\n",
            "330 l zone signs None l zone signs {'$ref': '#/pictures/3'}\n",
            "331 are fluorescent None are fluorescent {'$ref': '#/pictures/3'}\n",
            "332 yel ow-gre None yel ow-gre {'$ref': '#/pictures/3'}\n",
            "333 n None n {'$ref': '#/pictures/3'}\n",
            "334 Tel s about motorist None Tel s about motorist {'$ref': '#/pictures/3'}\n",
            "335 services None services {'$ref': '#/pictures/3'}\n",
            "336 Shows a permit ed None Shows a permit ed {'$ref': '#/pictures/3'}\n",
            "337 action None action {'$ref': '#/pictures/3'}\n",
            "338 Shows an action that None Shows an action that {'$ref': '#/pictures/3'}\n",
            "339 is not permit ed None is not permit ed {'$ref': '#/pictures/3'}\n",
            "340 Warns of hazards None Warns of hazards {'$ref': '#/pictures/3'}\n",
            "341 ahead None ahead {'$ref': '#/pictures/3'}\n",
            "342 Warns of None Warns of {'$ref': '#/pictures/3'}\n",
            "343 construction zones None construction zones {'$ref': '#/pictures/3'}\n",
            "344 Railway cros ing None Railway cros ing {'$ref': '#/pictures/3'}\n",
            "345 Shows distance and None Shows distance and {'$ref': '#/pictures/3'}\n",
            "346 direction None direction {'$ref': '#/pictures/3'}\n",
            "347 •  Signs None •  Signs {'$ref': '#/pictures/3'}\n",
            "348 - None - {'$ref': '#/pictures/3'}\n",
            "349 regulatory signs None regulatory signs {'$ref': '#/pictures/3'}\n",
            "350 - None - {'$ref': '#/pictures/3'}\n",
            "351 scho None scho {'$ref': '#/pictures/3'}\n",
            "352 l, None l, {'$ref': '#/pictures/3'}\n",
            "353 playground and None playground and {'$ref': '#/pictures/3'}\n",
            "354 cros walk signs None cros walk signs {'$ref': '#/pictures/3'}\n",
            "355 - None - {'$ref': '#/pictures/3'}\n",
            "356 lane use signs None lane use signs {'$ref': '#/pictures/3'}\n",
            "357 - None - {'$ref': '#/pictures/3'}\n",
            "358 turn control signs None turn control signs {'$ref': '#/pictures/3'}\n",
            "359 - None - {'$ref': '#/pictures/3'}\n",
            "360 parking signs None parking signs {'$ref': '#/pictures/3'}\n",
            "361 - None - {'$ref': '#/pictures/3'}\n",
            "362 reserved lane None reserved lane {'$ref': '#/pictures/3'}\n",
            "363 signs None signs {'$ref': '#/pictures/3'}\n",
            "364 - None - {'$ref': '#/pictures/3'}\n",
            "365 warni None warni {'$ref': '#/pictures/3'}\n",
            "366 g signs None g signs {'$ref': '#/pictures/3'}\n",
            "367 - None - {'$ref': '#/pictures/3'}\n",
            "368 object markers None object markers {'$ref': '#/pictures/3'}\n",
            "369 - None - {'$ref': '#/pictures/3'}\n",
            "370 construction None construction {'$ref': '#/pictures/3'}\n",
            "371 signs None signs {'$ref': '#/pictures/3'}\n",
            "372 - None - {'$ref': '#/pictures/3'}\n",
            "373 information and None information and {'$ref': '#/pictures/3'}\n",
            "374 destination signs None destination signs {'$ref': '#/pictures/3'}\n",
            "375 - None - {'$ref': '#/pictures/3'}\n",
            "376 railway signs None railway signs {'$ref': '#/pictures/3'}\n",
            "377 •  Signals None •  Signals {'$ref': '#/pictures/3'}\n",
            "378 - None - {'$ref': '#/pictures/3'}\n",
            "379 lane control None lane control {'$ref': '#/pictures/3'}\n",
            "380 signals None signals {'$ref': '#/pictures/3'}\n",
            "381 - None - {'$ref': '#/pictures/3'}\n",
            "382 traffic lights None traffic lights {'$ref': '#/pictures/3'}\n",
            "383 •  Road markings None •  Road markings {'$ref': '#/pictures/3'}\n",
            "384 - None - {'$ref': '#/pictures/3'}\n",
            "385 yel ow lines None yel ow lines {'$ref': '#/pictures/3'}\n",
            "386 - None - {'$ref': '#/pictures/3'}\n",
            "387 white lines None white lines {'$ref': '#/pictures/3'}\n",
            "388 - None - {'$ref': '#/pictures/3'}\n",
            "389 reserved lane None reserved lane {'$ref': '#/pictures/3'}\n",
            "390 markings None markings {'$ref': '#/pictures/3'}\n",
            "391 - None - {'$ref': '#/pictures/3'}\n",
            "392 other markings None other markings {'$ref': '#/pictures/3'}\n",
            "393 in this chapter None in this chapter {'$ref': '#/pictures/3'}\n",
            "394 AGL Energy Limited  ABN 74 1 None AGL Energy Limited  ABN 74 1 {'$ref': '#/body'}\n",
            "395 5 061 375 None 5 061 375 {'$ref': '#/body'}\n",
            "396 AGL 2013 Financial Calendar None AGL 2013 Financial Calendar {'$ref': '#/pictures/4'}\n",
            "397 2  August 2012 None 2  August 2012 {'$ref': '#/pictures/4'}\n",
            "398 2012 ful  year  esult and final div dend an None 2012 ful  year  esult and final div dend an {'$ref': '#/pictures/4'}\n",
            "399 ounced None ounced {'$ref': '#/pictures/4'}\n",
            "400 30 August 2012 None 30 August 2012 {'$ref': '#/pictures/4'}\n",
            "401 Ex-div dend trading com None Ex-div dend trading com {'$ref': '#/pictures/4'}\n",
            "402 ences None ences {'$ref': '#/pictures/4'}\n",
            "403 5 September 2012 None 5 September 2012 {'$ref': '#/pictures/4'}\n",
            "404 Record None Record {'$ref': '#/pictures/4'}\n",
            "405 ate for 2012 final div dend None ate for 2012 final div dend {'$ref': '#/pictures/4'}\n",
            "406 27 September 2012 None 27 September 2012 {'$ref': '#/pictures/4'}\n",
            "407 Final div dend payable None Final div dend payable {'$ref': '#/pictures/4'}\n",
            "408 23 October 2012 None 23 October 2012 {'$ref': '#/pictures/4'}\n",
            "409 An ual General Me None An ual General Me {'$ref': '#/pictures/4'}\n",
            "410 ting None ting {'$ref': '#/pictures/4'}\n",
            "411 27 February 2013 1 None 27 February 2013 1 {'$ref': '#/pictures/4'}\n",
            "412 2013 interim result and interim div dend an None 2013 interim result and interim div dend an {'$ref': '#/pictures/4'}\n",
            "413 ounced None ounced {'$ref': '#/pictures/4'}\n",
            "414 28 August 2013 1 None 28 August 2013 1 {'$ref': '#/pictures/4'}\n",
            "415 2013 ful  year  esults and final div dend an None 2013 ful  year  esults and final div dend an {'$ref': '#/pictures/4'}\n",
            "416 ounced None ounced {'$ref': '#/pictures/4'}\n",
            "417 1 Indicative dates only, subject  o change/Board confirmation None 1 Indicative dates only, subject  o change/Board confirmation {'$ref': '#/pictures/4'}\n",
            "418 AGL's An None AGL's An {'$ref': '#/pictures/4'}\n",
            "419 ual General Me None ual General Me {'$ref': '#/pictures/4'}\n",
            "420 ting wil  be held at None ting wil  be held at {'$ref': '#/pictures/4'}\n",
            "421 he City Recital Hal , Angel Place, Sydney None he City Recital Hal , Angel Place, Sydney {'$ref': '#/pictures/4'}\n",
            "422 commencing at 10.30am on Tuesday 23 October 2012. None commencing at 10.30am on Tuesday 23 October 2012. {'$ref': '#/pictures/4'}\n",
            "423 Yesterday None Yesterday {'$ref': '#/pictures/4'}\n",
            "424 Established in Sydney in 1837, and then None Established in Sydney in 1837, and then {'$ref': '#/pictures/4'}\n",
            "425 known as The Australian Gas Light Company, None known as The Australian Gas Light Company, {'$ref': '#/pictures/4'}\n",
            "426 the AGL busines  has an established history None the AGL busines  has an established history {'$ref': '#/pictures/4'}\n",
            "427 and reputation for serving the gas and None and reputation for serving the gas and {'$ref': '#/pictures/4'}\n",
            "428 el None el {'$ref': '#/pictures/4'}\n",
            "429 ctric ty ne None ctric ty ne {'$ref': '#/pictures/4'}\n",
            "430 ds of Australian households. None ds of Australian households. {'$ref': '#/pictures/4'}\n",
            "431 In 1841, when AGL sup None In 1841, when AGL sup {'$ref': '#/pictures/4'}\n",
            "432 lied the gas to light None lied the gas to light {'$ref': '#/pictures/4'}\n",
            "433 the first public stre None the first public stre {'$ref': '#/pictures/4'}\n",
            "434 t lamp, it was reported None t lamp, it was reported {'$ref': '#/pictures/4'}\n",
            "435 in the Sydney Gazet e as a 'wonderful None in the Sydney Gazet e as a 'wonderful {'$ref': '#/pictures/4'}\n",
            "436 achievement of scientific knowledge, as isted None achievement of scientific knowledge, as isted {'$ref': '#/pictures/4'}\n",
            "437 by mechanical ingenuity.' Within two years, None by mechanical ingenuity.' Within two years, {'$ref': '#/pictures/4'}\n",
            "438 165 gas lamps were lighting the City of Sydney. None 165 gas lamps were lighting the City of Sydney. {'$ref': '#/pictures/4'}\n",
            "439 Looking back on None Looking back on {'$ref': '#/pictures/4'}\n",
            "440 175years of None 175years of {'$ref': '#/pictures/4'}\n",
            "441 lookingforward. None lookingforward. {'$ref': '#/pictures/4'}\n",
            "442 Circling Minimums None Circling Minimums {'$ref': '#/pictures/5'}\n",
            "443 7KHUHglyph<c=3,font=/AAAAAU+ArialMT>ZDVglyph<c=3,font=/AAAAAU+ArialMT>Dglyph<c=3,font=/AAAAAU+ArialMT>FKDQJHglyph<c=3,font=/AAAAAU+ArialMT>WRglyph<c=3,font=/AAAAAU+ArialMT>WKHglyph<c=3,font=/AAAAAU+ArialMT>7(536glyph<c=3,font=/AAAAAU+ArialMT>FULWHULDglyph<c=3,font=/AAAAAU+ArialMT>LQglyph<c=3,font=/AAAAAU+ArialMT>glyph<c=21,font=/AAAAAU+ArialMT>glyph<c=19,font=/AAAAAU+ArialMT>glyph<c=20,font=/AAAAAU+ArialMT>glyph<c=21,font=/AAAAAU+ArialMT>glyph<c=3,font=/AAAAAU+ArialMT>WKDWglyph<c=3,font=/AAAAAU+ArialMT>DႇHFWVglyph<c=3,font=/AAAAAU+ArialMT>FLUFOLQJglyph<c=3,font=/AAAAAU+ArialMT>DUHDglyph<c=3,font=/AAAAAU+ArialMT>GLPHQVLRQglyph<c=3,font=/AAAAAU+ArialMT>E\\glyph<c=3,font=/AAAAAU+ArialMT>H[SDQGLQJglyph<c=3,font=/AAAAAU+ArialMT>WKHglyph<c=3,font=/AAAAAU+ArialMT>DUHDVglyph<c=3,font=/AAAAAU+ArialMT>WRglyph<c=3,font=/AAAAAU+ArialMT>SURYLGHglyph<c=3,font=/AAAAAU+ArialMT> None 7KHUHglyph<c=3,font=/AAAAAU+ArialMT>ZDVglyph<c=3,font=/AAAAAU+ArialMT>Dglyph<c=3,font=/AAAAAU+ArialMT>FKDQJHglyph<c=3,font=/AAAAAU+ArialMT>WRglyph<c=3,font=/AAAAAU+ArialMT>WKHglyph<c=3,font=/AAAAAU+ArialMT>7(536glyph<c=3,font=/AAAAAU+ArialMT>FULWHULDglyph<c=3,font=/AAAAAU+ArialMT>LQglyph<c=3,font=/AAAAAU+ArialMT>glyph<c=21,font=/AAAAAU+ArialMT>glyph<c=19,font=/AAAAAU+ArialMT>glyph<c=20,font=/AAAAAU+ArialMT>glyph<c=21,font=/AAAAAU+ArialMT>glyph<c=3,font=/AAAAAU+ArialMT>WKDWglyph<c=3,font=/AAAAAU+ArialMT>DႇHFWVglyph<c=3,font=/AAAAAU+ArialMT>FLUFOLQJglyph<c=3,font=/AAAAAU+ArialMT>DUHDglyph<c=3,font=/AAAAAU+ArialMT>GLPHQVLRQglyph<c=3,font=/AAAAAU+ArialMT>E\\glyph<c=3,font=/AAAAAU+ArialMT>H[SDQGLQJglyph<c=3,font=/AAAAAU+ArialMT>WKHglyph<c=3,font=/AAAAAU+ArialMT>DUHDVglyph<c=3,font=/AAAAAU+ArialMT>WRglyph<c=3,font=/AAAAAU+ArialMT>SURYLGHglyph<c=3,font=/AAAAAU+ArialMT> {'$ref': '#/pictures/5'}\n",
            "444 improved obstacle protection. To indicate that  he new criteria had be None improved obstacle protection. To indicate that  he new criteria had be {'$ref': '#/pictures/5'}\n",
            "445 n ap None n ap {'$ref': '#/pictures/5'}\n",
            "446 lied to a given procedure, a None lied to a given procedure, a {'$ref': '#/pictures/5'}\n",
            "447 is placed on None is placed on {'$ref': '#/pictures/5'}\n",
            "448 the circling line of minimums. The new circling tables and explanatory information is located in the Legend of the TP None the circling line of minimums. The new circling tables and explanatory information is located in the Legend of the TP {'$ref': '#/pictures/5'}\n",
            "449 . None . {'$ref': '#/pictures/5'}\n",
            "450 7KHglyph<c=3,font=/AAAAAU+ArialMT>DS None 7KHglyph<c=3,font=/AAAAAU+ArialMT>DS {'$ref': '#/pictures/5'}\n",
            "451 URDFKHVglyph<c=3,font=/AAAAAU+ArialMT>XVLQJglyph<c=3,font=/AAAAAU+ArialMT>VWDQGDUGglyph<c=3,font=/AAAAAU+ArialMT>FLUFOLQJglyph<c=3,font=/AAAAAU+ArialMT>DS None URDFKHVglyph<c=3,font=/AAAAAU+ArialMT>XVLQJglyph<c=3,font=/AAAAAU+ArialMT>VWDQGDUGglyph<c=3,font=/AAAAAU+ArialMT>FLUFOLQJglyph<c=3,font=/AAAAAU+ArialMT>DS {'$ref': '#/pictures/5'}\n",
            "452 URDFKglyph<c=3,font=/AAAAAU+ArialMT>DUHDVglyph<c=3,font=/AAAAAU+ArialMT>FDQglyph<c=3,font=/AAAAAU+ArialMT>EHglyph<c=3,font=/AAAAAU+ArialMT>LGHQWL¿HGglyph<c=3,font=/AAAAAU+ArialMT>E\\glyph<c=3,font=/AAAAAU+ArialMT>WKHglyph<c=3,font=/AAAAAU+ArialMT>DEVHQFHglyph<c=3,font=/AAAAAU+ArialMT>RIglyph<c=3,font=/AAAAAU+ArialMT>WKHglyph<c=3,font=/AAAAAU+ArialMT> None URDFKglyph<c=3,font=/AAAAAU+ArialMT>DUHDVglyph<c=3,font=/AAAAAU+ArialMT>FDQglyph<c=3,font=/AAAAAU+ArialMT>EHglyph<c=3,font=/AAAAAU+ArialMT>LGHQWL¿HGglyph<c=3,font=/AAAAAU+ArialMT>E\\glyph<c=3,font=/AAAAAU+ArialMT>WKHglyph<c=3,font=/AAAAAU+ArialMT>DEVHQFHglyph<c=3,font=/AAAAAU+ArialMT>RIglyph<c=3,font=/AAAAAU+ArialMT>WKHglyph<c=3,font=/AAAAAU+ArialMT> {'$ref': '#/pictures/5'}\n",
            "453 on the circling line of None on the circling line of {'$ref': '#/pictures/5'}\n",
            "454 minima. None minima. {'$ref': '#/pictures/5'}\n",
            "455 $S O\\glyph<c=3,font=/AAAAAX+Arial-ItalicMT>6WDQGDUGglyph<c=3,font=/AAAAAX+Arial-ItalicMT>&LUFOLQJglyph<c=3,font=/AAAAAX+Arial-ItalicMT> $S None $S O\\glyph<c=3,font=/AAAAAX+Arial-ItalicMT>6WDQGDUGglyph<c=3,font=/AAAAAX+Arial-ItalicMT>&LUFOLQJglyph<c=3,font=/AAAAAX+Arial-ItalicMT> $S {'$ref': '#/pictures/5'}\n",
            "456 URDFKglyph<c=3,font=/AAAAAX+Arial-ItalicMT>0DQHXYHULQJglyph<c=3,font=/AAAAAX+Arial-ItalicMT>5DGLXVglyph<c=3,font=/AAAAAX+Arial-ItalicMT>7 DEOH None URDFKglyph<c=3,font=/AAAAAX+Arial-ItalicMT>0DQHXYHULQJglyph<c=3,font=/AAAAAX+Arial-ItalicMT>5DGLXVglyph<c=3,font=/AAAAAX+Arial-ItalicMT>7 DEOH {'$ref': '#/pictures/5'}\n",
            "457 $S O\\glyph<c=3,font=/AAAAAX+Arial-ItalicMT>([SDQGHGglyph<c=3,font=/AAAAAX+Arial-ItalicMT>&LUFOLQJglyph<c=3,font=/AAAAAX+Arial-ItalicMT> $S None $S O\\glyph<c=3,font=/AAAAAX+Arial-ItalicMT>([SDQGHGglyph<c=3,font=/AAAAAX+Arial-ItalicMT>&LUFOLQJglyph<c=3,font=/AAAAAX+Arial-ItalicMT> $S {'$ref': '#/pictures/5'}\n",
            "458 URDFKglyph<c=3,font=/AAAAAX+Arial-ItalicMT>0DQHXYHULQJglyph<c=3,font=/AAAAAX+Arial-ItalicMT> $LUVSDFHglyph<c=3,font=/AAAAAX+Arial-ItalicMT>5DGLXVglyph<c=3,font=/AAAAAX+Arial-ItalicMT> None URDFKglyph<c=3,font=/AAAAAX+Arial-ItalicMT>0DQHXYHULQJglyph<c=3,font=/AAAAAX+Arial-ItalicMT> $LUVSDFHglyph<c=3,font=/AAAAAX+Arial-ItalicMT>5DGLXVglyph<c=3,font=/AAAAAX+Arial-ItalicMT> {'$ref': '#/pictures/5'}\n",
            "459 Table None Table {'$ref': '#/pictures/5'}\n",
            "460 AIRPORT SKETCH None AIRPORT SKETCH {'$ref': '#/pictures/5'}\n",
            "461 The airport sketch is a depiction of the airport with emphasis on runway pat ern and related None The airport sketch is a depiction of the airport with emphasis on runway pat ern and related {'$ref': '#/pictures/5'}\n",
            "462 information, posit oned in either the lower left or lower  ight corner of the chart  o aid pi- None information, posit oned in either the lower left or lower  ight corner of the chart  o aid pi- {'$ref': '#/pictures/5'}\n",
            "463 lot recognit on of the airport from the air and to provide some information to aid on ground None lot recognit on of the airport from the air and to provide some information to aid on ground {'$ref': '#/pictures/5'}\n",
            "464 navigation of the airport. The runways are drawn to scale and oriented to true north. Runway None navigation of the airport. The runways are drawn to scale and oriented to true north. Runway {'$ref': '#/pictures/5'}\n",
            "465 dimensions (length and width) are shown for al  active runways. None dimensions (length and width) are shown for al  active runways. {'$ref': '#/pictures/5'}\n",
            "466 Runway(s) are depicted based on what  ype and construction of the runway. None Runway(s) are depicted based on what  ype and construction of the runway. {'$ref': '#/pictures/5'}\n",
            "467 Hard Surface None Hard Surface {'$ref': '#/pictures/5'}\n",
            "468 Other Than None Other Than {'$ref': '#/pictures/5'}\n",
            "469 Hard Surface None Hard Surface {'$ref': '#/pictures/5'}\n",
            "470 Metal Surface None Metal Surface {'$ref': '#/pictures/5'}\n",
            "471 Closed Runway None Closed Runway {'$ref': '#/pictures/5'}\n",
            "472 Under Construction None Under Construction {'$ref': '#/pictures/5'}\n",
            "473 Stopways, None Stopways, {'$ref': '#/pictures/5'}\n",
            "474 Taxiways, Park- None Taxiways, Park- {'$ref': '#/pictures/5'}\n",
            "475 ing Areas None ing Areas {'$ref': '#/pictures/5'}\n",
            "476 Displaced None Displaced {'$ref': '#/pictures/5'}\n",
            "477 Threshold None Threshold {'$ref': '#/pictures/5'}\n",
            "478 Closed None Closed {'$ref': '#/pictures/5'}\n",
            "479 Pavement None Pavement {'$ref': '#/pictures/5'}\n",
            "480 Water Runway None Water Runway {'$ref': '#/pictures/5'}\n",
            "481 Taxiways and aprons are shaded grey. Other  unway features that may be shown are runway numbers, runway dimen- None Taxiways and aprons are shaded grey. Other  unway features that may be shown are runway numbers, runway dimen- {'$ref': '#/pictures/5'}\n",
            "482 sions, runway slope, ar esting gear, and displaced threshold. None sions, runway slope, ar esting gear, and displaced threshold. {'$ref': '#/pictures/5'}\n",
            "483 2WKHUglyph<c=3,font=/AAAAAU+ArialMT>LQIRUPDWLRQglyph<c=3,font=/AAAAAU+ArialMT>FRQFHUQLQJglyph<c=3,font=/AAAAAU+ArialMT>OLJKWLQJglyph<c=15,font=/AAAAAU+ArialMT>glyph<c=3,font=/AAAAAU+ArialMT>¿QDOglyph<c=3,font=/AAAAAU+ArialMT>DS None 2WKHUglyph<c=3,font=/AAAAAU+ArialMT>LQIRUPDWLRQglyph<c=3,font=/AAAAAU+ArialMT>FRQFHUQLQJglyph<c=3,font=/AAAAAU+ArialMT>OLJKWLQJglyph<c=15,font=/AAAAAU+ArialMT>glyph<c=3,font=/AAAAAU+ArialMT>¿QDOglyph<c=3,font=/AAAAAU+ArialMT>DS {'$ref': '#/pictures/5'}\n",
            "484 URDFKglyph<c=3,font=/AAAAAU+ArialMT>EHDULQJVglyph<c=15,font=/AAAAAU+ArialMT>glyph<c=3,font=/AAAAAU+ArialMT>DLUSRUWglyph<c=3,font=/AAAAAU+ArialMT>EHDFRQglyph<c=15,font=/AAAAAU+ArialMT>glyph<c=3,font=/AAAAAU+ArialMT>REVWDFOHVglyph<c=15,font=/AAAAAU+ArialMT>glyph<c=3,font=/AAAAAU+ArialMT>FRQWUROglyph<c=3,font=/AAAAAU+ArialMT>WRZHU glyph<c=15,font=/AAAAAU+ArialMT>glyph<c=3,font=/AAAAAU+ArialMT>1$ None URDFKglyph<c=3,font=/AAAAAU+ArialMT>EHDULQJVglyph<c=15,font=/AAAAAU+ArialMT>glyph<c=3,font=/AAAAAU+ArialMT>DLUSRUWglyph<c=3,font=/AAAAAU+ArialMT>EHDFRQglyph<c=15,font=/AAAAAU+ArialMT>glyph<c=3,font=/AAAAAU+ArialMT>REVWDFOHVglyph<c=15,font=/AAAAAU+ArialMT>glyph<c=3,font=/AAAAAU+ArialMT>FRQWUROglyph<c=3,font=/AAAAAU+ArialMT>WRZHU glyph<c=15,font=/AAAAAU+ArialMT>glyph<c=3,font=/AAAAAU+ArialMT>1$ {'$ref': '#/pictures/5'}\n",
            "485 9$,'Vglyph<c=15,font=/AAAAAU+ArialMT>glyph<c=3,font=/AAAAAU+ArialMT>KHOL None 9$,'Vglyph<c=15,font=/AAAAAU+ArialMT>glyph<c=3,font=/AAAAAU+ArialMT>KHOL {'$ref': '#/pictures/5'}\n",
            "486 - None - {'$ref': '#/pictures/5'}\n",
            "487 pads may also be shown. None pads may also be shown. {'$ref': '#/pictures/5'}\n",
            "488 $LUSRUWglyph<c=3,font=/AAAABA+Arial-BoldMT>(OHYDWLRQglyph<c=3,font=/AAAABA+Arial-BoldMT>DQGglyph<c=3,font=/AAAABA+Arial-BoldMT>7 RXFKGRZQglyph<c=3,font=/AAAABA+Arial-BoldMT>=RQHglyph<c=3,font=/AAAABA+Arial-BoldMT>(OHYDWLRQ None $LUSRUWglyph<c=3,font=/AAAABA+Arial-BoldMT>(OHYDWLRQglyph<c=3,font=/AAAABA+Arial-BoldMT>DQGglyph<c=3,font=/AAAABA+Arial-BoldMT>7 RXFKGRZQglyph<c=3,font=/AAAABA+Arial-BoldMT>=RQHglyph<c=3,font=/AAAABA+Arial-BoldMT>(OHYDWLRQ {'$ref': '#/pictures/5'}\n",
            "489 The airport elevation is shown enclosed within a box in the up None The airport elevation is shown enclosed within a box in the up {'$ref': '#/pictures/5'}\n",
            "490 er left corner of the sketch box and the touchdown zone None er left corner of the sketch box and the touchdown zone {'$ref': '#/pictures/5'}\n",
            "491 elevation (TDZE) is shown in the up None elevation (TDZE) is shown in the up {'$ref': '#/pictures/5'}\n",
            "492 er  ight corner of the sketch box. The airport elevation is the highest point of an None er  ight corner of the sketch box. The airport elevation is the highest point of an {'$ref': '#/pictures/5'}\n",
            "493 DLUSRUW¶Vglyph<c=3,font=/AAAAAU+ArialMT>XVDEOHglyph<c=3,font=/AAAAAU+ArialMT>UXQZD\\Vglyph<c=3,font=/AAAAAU+ArialMT>PHDVXUHGglyph<c=3,font=/AAAAAU+ArialMT>LQglyph<c=3,font=/AAAAAU+ArialMT>IH None DLUSRUW¶Vglyph<c=3,font=/AAAAAU+ArialMT>XVDEOHglyph<c=3,font=/AAAAAU+ArialMT>UXQZD\\Vglyph<c=3,font=/AAAAAU+ArialMT>PHDVXUHGglyph<c=3,font=/AAAAAU+ArialMT>LQglyph<c=3,font=/AAAAAU+ArialMT>IH {'$ref': '#/pictures/5'}\n",
            "494 Wglyph<c=3,font=/AAAAAU+ArialMT>IURPglyph<c=3,font=/AAAAAU+ArialMT>PHDQglyph<c=3,font=/AAAAAU+ArialMT>VHDglyph<c=3,font=/AAAAAU+ArialMT>OHYHOglyph<c=17,font=/AAAAAU+ArialMT>glyph<c=3,font=/AAAAAU+ArialMT>7KHglyph<c=3,font=/AAAAAU+ArialMT> 7'=(glyph<c=3,font=/AAAAAU+ArialMT>LVglyph<c=3,font=/AAAAAU+ArialMT>WKHglyph<c=3,font=/AAAAAU+ArialMT>KLJKHVWglyph<c=3,font=/AAAAAU+ArialMT>HOHYDWLRQglyph<c=3,font=/AAAAAU+ArialMT>LQglyph<c=3,font=/AAAAAU+ArialMT>WKHglyph<c=3,font=/AAAAAU+ArialMT>¿UVWglyph<c=3,font=/AAAAAU+ArialMT>glyph<c=22,font=/AAAAAU+ArialMT>glyph<c=15,font=/AAAAAU+ArialMT>glyph<c=19,font=/AAAAAU+ArialMT> None Wglyph<c=3,font=/AAAAAU+ArialMT>IURPglyph<c=3,font=/AAAAAU+ArialMT>PHDQglyph<c=3,font=/AAAAAU+ArialMT>VHDglyph<c=3,font=/AAAAAU+ArialMT>OHYHOglyph<c=17,font=/AAAAAU+ArialMT>glyph<c=3,font=/AAAAAU+ArialMT>7KHglyph<c=3,font=/AAAAAU+ArialMT> 7'=(glyph<c=3,font=/AAAAAU+ArialMT>LVglyph<c=3,font=/AAAAAU+ArialMT>WKHglyph<c=3,font=/AAAAAU+ArialMT>KLJKHVWglyph<c=3,font=/AAAAAU+ArialMT>HOHYDWLRQglyph<c=3,font=/AAAAAU+ArialMT>LQglyph<c=3,font=/AAAAAU+ArialMT>WKHglyph<c=3,font=/AAAAAU+ArialMT>¿UVWglyph<c=3,font=/AAAAAU+ArialMT>glyph<c=22,font=/AAAAAU+ArialMT>glyph<c=15,font=/AAAAAU+ArialMT>glyph<c=19,font=/AAAAAU+ArialMT> {'$ref': '#/pictures/5'}\n",
            "495 glyph<c=19,font=/AAAAAU+ArialMT>glyph<c=3,font=/AAAAAU+ArialMT>IH None glyph<c=19,font=/AAAAAU+ArialMT>glyph<c=3,font=/AAAAAU+ArialMT>IH {'$ref': '#/pictures/5'}\n",
            "496 Wglyph<c=3,font=/AAAAAU+ArialMT>RIglyph<c=3,font=/AAAAAU+ArialMT> None Wglyph<c=3,font=/AAAAAU+ArialMT>RIglyph<c=3,font=/AAAAAU+ArialMT> {'$ref': '#/pictures/5'}\n",
            "497 the landing surface. Circling only ap None the landing surface. Circling only ap {'$ref': '#/pictures/5'}\n",
            "498 roaches wil  not show a TDZE. None roaches wil  not show a TDZE. {'$ref': '#/pictures/5'}\n",
            "499 1 None 1 {'$ref': '#/pictures/5'}\n",
            "500 4 None 4 {'$ref': '#/pictures/5'}\n",
            "501 FA None FA {'$ref': '#/pictures/5'}\n",
            "502 Chart Users' Guide - Terminal Procedures Publication (TP None Chart Users' Guide - Terminal Procedures Publication (TP {'$ref': '#/pictures/5'}\n",
            "503 ) - Terms None ) - Terms {'$ref': '#/pictures/5'}\n",
            "504 Figure 1: Four examples of complex page layouts across different document categories None Figure 1: Four examples of complex page layouts across different document categories {'$ref': '#/body'}\n",
            "505 KEYWORDS 1 KEYWORDS {'$ref': '#/body'}\n",
            "506 PDF document conversion, layout segmentation, object-detection, data set, Machine Learning None PDF document conversion, layout segmentation, object-detection, data set, Machine Learning {'$ref': '#/body'}\n",
            "507 ACMReference Format: 1 ACMReference Format: {'$ref': '#/body'}\n",
            "508 Birgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S. Nassar, and Peter Staar. 2022. DocLayNet: A Large Human-Annotated Dataset for DocumentLayout Analysis. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD '22), August 14-18, 2022, Washington, DC, USA. ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/ 3534678.3539043 None Birgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S. Nassar, and Peter Staar. 2022. DocLayNet: A Large Human-Annotated Dataset for DocumentLayout Analysis. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD '22), August 14-18, 2022, Washington, DC, USA. ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/ 3534678.3539043 {'$ref': '#/body'}\n",
            "509 1 INTRODUCTION None 1 INTRODUCTION {'$ref': '#/body'}\n",
            "510 Despite the substantial improvements achieved with machine-learning (ML) approaches and deep neural networks in recent years, document conversion remains a challenging problem, as demonstrated by the numerous public competitions held on this topic [1-4]. The challenge originates from the huge variability in PDF documents regarding layout, language and formats (scanned, programmatic or a combination of both). Engineering a single ML model that can be applied on all types of documents and provides high-quality layout segmentation remains to this day extremely challenging [5]. To highlight the variability in document layouts, we show a few example documents from the DocLayNet dataset in Figure 1. Figure 2: Title page of the DocLayNet paper (arxiv.org/pdf/2206.01062) - left PDF, right rendered Markdown. If recognized, metadata such as authors are appearing first under the title. Text content inside figures is currently dropped, the caption is retained and linked to the figure in the JSON representation (not shown). None Despite the substantial improvements achieved with machine-learning (ML) approaches and deep neural networks in recent years, document conversion remains a challenging problem, as demonstrated by the numerous public competitions held on this topic [1-4]. The challenge originates from the huge variability in PDF documents regarding layout, language and formats (scanned, programmatic or a combination of both). Engineering a single ML model that can be applied on all types of documents and provides high-quality layout segmentation remains to this day extremely challenging [5]. To highlight the variability in document layouts, we show a few example documents from the DocLayNet dataset in Figure 1. Figure 2: Title page of the DocLayNet paper (arxiv.org/pdf/2206.01062) - left PDF, right rendered Markdown. If recognized, metadata such as authors are appearing first under the title. Text content inside figures is currently dropped, the caption is retained and linked to the figure in the JSON representation (not shown). {'$ref': '#/body'}\n",
            "511 7 None 7 {'$ref': '#/body'}\n",
            "512 KDD '22, August 14-18, 2022, Washington, DC, USA Birgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S. Nassar, and Peter Staar None KDD '22, August 14-18, 2022, Washington, DC, USA Birgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S. Nassar, and Peter Staar {'$ref': '#/body'}\n",
            "513 Table 2: Prediction performance (mAP@0.5-0.95) of object detection networks on DocLayNet test set. The MRCNN (Mask R-CNN) and FRCNN (Faster R-CNN) models with ResNet-50 or ResNet-101 backbone were trained based on the network architectures from the detectron2 model zoo (Mask R-CNN R50, R101-FPN 3x, Faster R-CNN R101-FPN 3x), with default configurations. The YOLO implementation utilized was YOLOv5x6 [13]. All models were initialised using pre-trained weights from the COCO 2017 dataset. None Table 2: Prediction performance (mAP@0.5-0.95) of object detection networks on DocLayNet test set. The MRCNN (Mask R-CNN) and FRCNN (Faster R-CNN) models with ResNet-50 or ResNet-101 backbone were trained based on the network architectures from the detectron2 model zoo (Mask R-CNN R50, R101-FPN 3x, Faster R-CNN R101-FPN 3x), with default configurations. The YOLO implementation utilized was YOLOv5x6 [13]. All models were initialised using pre-trained weights from the COCO 2017 dataset. {'$ref': '#/body'}\n",
            "514 to avoid this at any cost in order to have clear, unbiased baseline numbers for human document-layout annotation. Third, we introduced the feature of snapping boxes around text segments to obtain a pixel-accurate annotation and again reduce time and effort. The CCS annotation tool automatically shrinks every user-drawn box to the minimum bounding-box around the enclosed text-cells for all purely text-based segments, which excludes only Table and Picture . For the latter, we instructed annotation staff to minimise inclusion of surrounding whitespace while including all graphical lines. A downside of snapping boxes to enclosed text cells is that some wrongly parsed PDF pages cannot be annotated correctly and need to be skipped. Fourth, we established a way to flag pages as rejected for cases where no valid annotation according to the label guidelines could be achieved. Example cases for this would be PDF pages that render incorrectly or contain layouts that are impossible to capture with non-overlapping rectangles. Such rejected pages are not contained in the final dataset. With all these measures in place, experienced annotation staff managed to annotate a single page in a typical timeframe of 20s to 60s, depending on its complexity. None to avoid this at any cost in order to have clear, unbiased baseline numbers for human document-layout annotation. Third, we introduced the feature of snapping boxes around text segments to obtain a pixel-accurate annotation and again reduce time and effort. The CCS annotation tool automatically shrinks every user-drawn box to the minimum bounding-box around the enclosed text-cells for all purely text-based segments, which excludes only Table and Picture . For the latter, we instructed annotation staff to minimise inclusion of surrounding whitespace while including all graphical lines. A downside of snapping boxes to enclosed text cells is that some wrongly parsed PDF pages cannot be annotated correctly and need to be skipped. Fourth, we established a way to flag pages as rejected for cases where no valid annotation according to the label guidelines could be achieved. Example cases for this would be PDF pages that render incorrectly or contain layouts that are impossible to capture with non-overlapping rectangles. Such rejected pages are not contained in the final dataset. With all these measures in place, experienced annotation staff managed to annotate a single page in a typical timeframe of 20s to 60s, depending on its complexity. {'$ref': '#/body'}\n",
            "515 5 EXPERIMENTS 1 5 EXPERIMENTS {'$ref': '#/body'}\n",
            "516 The primary goal of DocLayNet is to obtain high-quality ML models capable of accurate document-layout analysis on a wide variety of challenging layouts. As discussed in Section 2, object detection models are currently the easiest to use, due to the standardisation of ground-truth data in COCO format [16] and the availability of general frameworks such as detectron2 [17]. Furthermore, baseline numbers in PubLayNet and DocBank were obtained using standard object detection models such as Mask R-CNN and Faster R-CNN. As such, we will relate to these object detection methods in this None The primary goal of DocLayNet is to obtain high-quality ML models capable of accurate document-layout analysis on a wide variety of challenging layouts. As discussed in Section 2, object detection models are currently the easiest to use, due to the standardisation of ground-truth data in COCO format [16] and the availability of general frameworks such as detectron2 [17]. Furthermore, baseline numbers in PubLayNet and DocBank were obtained using standard object detection models such as Mask R-CNN and Faster R-CNN. As such, we will relate to these object detection methods in this {'$ref': '#/body'}\n",
            "517 Third, Ccs label qu Oolines achienec Exanole None Third, Ccs label qu Oolines achienec Exanole {'$ref': '#/body'}\n",
            "518 EXPERIMENTS 1 EXPERIMENTS {'$ref': '#/body'}\n",
            "519 chalenongayouls ground-vuth dawa such WC None chalenongayouls ground-vuth dawa such WC {'$ref': '#/body'}\n",
            "520 Figure 5: Prediction performance (mAP@0.5-0.95) of a Mask R-CNNnetworkwithResNet50backbonetrainedonincreasing fractions of the DocLayNet dataset. The learning curve flattens around the 80% mark, indicating that increasing the size of the DocLayNet dataset with similar data will not yield significantly better predictions. None Figure 5: Prediction performance (mAP@0.5-0.95) of a Mask R-CNNnetworkwithResNet50backbonetrainedonincreasing fractions of the DocLayNet dataset. The learning curve flattens around the 80% mark, indicating that increasing the size of the DocLayNet dataset with similar data will not yield significantly better predictions. {'$ref': '#/body'}\n",
            "521 paper and leave the detailed evaluation of more recent methods mentioned in Section 2 for future work. None paper and leave the detailed evaluation of more recent methods mentioned in Section 2 for future work. {'$ref': '#/body'}\n",
            "522 In this section, we will present several aspects related to the performance of object detection models on DocLayNet. Similarly as in PubLayNet, we will evaluate the quality of their predictions using mean average precision (mAP) with 10 overlaps that range from 0.5 to 0.95 in steps of 0.05 (mAP@0.5-0.95). These scores are computed by leveraging the evaluation code provided by the COCO API [16]. None In this section, we will present several aspects related to the performance of object detection models on DocLayNet. Similarly as in PubLayNet, we will evaluate the quality of their predictions using mean average precision (mAP) with 10 overlaps that range from 0.5 to 0.95 in steps of 0.05 (mAP@0.5-0.95). These scores are computed by leveraging the evaluation code provided by the COCO API [16]. {'$ref': '#/body'}\n",
            "523 Baselines for Object Detection 1 Baselines for Object Detection {'$ref': '#/body'}\n",
            "524 In Table 2, we present baseline experiments (given in mAP) on Mask R-CNN [12], Faster R-CNN [11], and YOLOv5 [13]. Both training and evaluation were performed on RGB images with dimensions of 1025 × 1025 pixels. For training, we only used one annotation in case of redundantly annotated pages. As one can observe, the variation in mAP between the models is rather low, but overall between 6 and 10% lower than the mAP computed from the pairwise human annotations on triple-annotated pages. This gives a good indication that the DocLayNet dataset poses a worthwhile challenge for the research community to close the gap between human recognition and ML approaches. It is interesting to see that Mask R-CNN and Faster R-CNN produce very comparable mAP scores, indicating that pixel-based image segmentation derived from bounding-boxes does not help to obtain better predictions. On the other hand, the more recent Yolov5x model does very well and even out-performs humans on selected labels such as Text , Table and Picture . This is not entirely surprising, as Text , Table and Picture are abundant and the most visually distinctive in a document. None In Table 2, we present baseline experiments (given in mAP) on Mask R-CNN [12], Faster R-CNN [11], and YOLOv5 [13]. Both training and evaluation were performed on RGB images with dimensions of 1025 × 1025 pixels. For training, we only used one annotation in case of redundantly annotated pages. As one can observe, the variation in mAP between the models is rather low, but overall between 6 and 10% lower than the mAP computed from the pairwise human annotations on triple-annotated pages. This gives a good indication that the DocLayNet dataset poses a worthwhile challenge for the research community to close the gap between human recognition and ML approaches. It is interesting to see that Mask R-CNN and Faster R-CNN produce very comparable mAP scores, indicating that pixel-based image segmentation derived from bounding-boxes does not help to obtain better predictions. On the other hand, the more recent Yolov5x model does very well and even out-performs humans on selected labels such as Text , Table and Picture . This is not entirely surprising, as Text , Table and Picture are abundant and the most visually distinctive in a document. {'$ref': '#/body'}\n",
            "525 Baselines for Object Detection 1 Baselines for Object Detection {'$ref': '#/body'}\n",
            "526 mak enbrel dacuont None mak enbrel dacuont {'$ref': '#/body'}\n",
            "527 Figure 3: Page 6 of the DocLayNet paper. If recognized, metadata such as authors are appearing first under the title. Elements recognized as page headers or footers are suppressed in Markdown to deliver uninterrupted content in reading order. Tables are inserted in reading order. The paragraph in '5. Experiments' wrapping over the column end is broken up in two and interrupted by the table. None Figure 3: Page 6 of the DocLayNet paper. If recognized, metadata such as authors are appearing first under the title. Elements recognized as page headers or footers are suppressed in Markdown to deliver uninterrupted content in reading order. Tables are inserted in reading order. The paragraph in '5. Experiments' wrapping over the column end is broken up in two and interrupted by the table. {'$ref': '#/body'}\n",
            "528 8 None 8 {'$ref': '#/body'}\n",
            "529 KDD '22, August 14-18, 2022, Washington, DC, USA None KDD '22, August 14-18, 2022, Washington, DC, USA {'$ref': '#/body'}\n",
            "530 Birgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S. Nassar, and Peter Staar None Birgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S. Nassar, and Peter Staar {'$ref': '#/body'}\n",
            "531 Table 1: DocLayNet dataset overview. Along with the frequency of each class label, we present the relative occurrence (as % None Table 1: DocLayNet dataset overview. Along with the frequency of each class label, we present the relative occurrence (as % {'$ref': '#/body'}\n",
            "532 between pairwise annotations from the triple-annotated pages, from which we obtain accuracy ranges. None between pairwise annotations from the triple-annotated pages, from which we obtain accuracy ranges. {'$ref': '#/body'}\n",
            "533 of row 'Total') in the train, test and validation sets. The inter-annotator agreement is computed as the mAP@0.5-0.95 metric None of row 'Total') in the train, test and validation sets. The inter-annotator agreement is computed as the mAP@0.5-0.95 metric {'$ref': '#/body'}\n",
            "534 A None A {'$ref': '#/pictures/8'}\n",
            "535 % of Total None % of Total {'$ref': '#/pictures/9'}\n",
            "536 triple inter-annotator None triple inter-annotator {'$ref': '#/pictures/9'}\n",
            "537 mAP @ 0.5-0.95 (%) None mAP @ 0.5-0.95 (%) {'$ref': '#/pictures/9'}\n",
            "538 class label None class label {'$ref': '#/pictures/9'}\n",
            "539 Count None Count {'$ref': '#/pictures/9'}\n",
            "540 Train None Train {'$ref': '#/pictures/9'}\n",
            "541 Test None Test {'$ref': '#/pictures/9'}\n",
            "542 Val None Val {'$ref': '#/pictures/9'}\n",
            "543 All None All {'$ref': '#/pictures/9'}\n",
            "544 Fin None Fin {'$ref': '#/pictures/9'}\n",
            "545 Man None Man {'$ref': '#/pictures/9'}\n",
            "546 Sci None Sci {'$ref': '#/pictures/9'}\n",
            "547 Law None Law {'$ref': '#/pictures/9'}\n",
            "548 Pat None Pat {'$ref': '#/pictures/9'}\n",
            "549 Ten None Ten {'$ref': '#/pictures/9'}\n",
            "550 Caption None Caption {'$ref': '#/pictures/9'}\n",
            "551 22524 None 22524 {'$ref': '#/pictures/9'}\n",
            "552 2.04 None 2.04 {'$ref': '#/pictures/9'}\n",
            "553 1.77 None 1.77 {'$ref': '#/pictures/9'}\n",
            "554 2.32 None 2.32 {'$ref': '#/pictures/9'}\n",
            "555 84-89 None 84-89 {'$ref': '#/pictures/9'}\n",
            "556 40-61 None 40-61 {'$ref': '#/pictures/9'}\n",
            "557 86-92 None 86-92 {'$ref': '#/pictures/9'}\n",
            "558 94-99 None 94-99 {'$ref': '#/pictures/9'}\n",
            "559 95-99 None 95-99 {'$ref': '#/pictures/9'}\n",
            "560 69-78 None 69-78 {'$ref': '#/pictures/9'}\n",
            "561 n/a None n/a {'$ref': '#/pictures/9'}\n",
            "562 Footnote None Footnote {'$ref': '#/pictures/9'}\n",
            "563 6318 None 6318 {'$ref': '#/pictures/9'}\n",
            "564 0.60 None 0.60 {'$ref': '#/pictures/9'}\n",
            "565 0.31 None 0.31 {'$ref': '#/pictures/9'}\n",
            "566 0.58 None 0.58 {'$ref': '#/pictures/9'}\n",
            "567 83-91 None 83-91 {'$ref': '#/pictures/9'}\n",
            "568 n/a None n/a {'$ref': '#/pictures/9'}\n",
            "569 100 None 100 {'$ref': '#/pictures/9'}\n",
            "570 62-88 None 62-88 {'$ref': '#/pictures/9'}\n",
            "571 85-94 None 85-94 {'$ref': '#/pictures/9'}\n",
            "572 n/a None n/a {'$ref': '#/pictures/9'}\n",
            "573 82-97 None 82-97 {'$ref': '#/pictures/9'}\n",
            "574 Formula None Formula {'$ref': '#/pictures/9'}\n",
            "575 25027 None 25027 {'$ref': '#/pictures/9'}\n",
            "576 2.25 None 2.25 {'$ref': '#/pictures/9'}\n",
            "577 1.90 None 1.90 {'$ref': '#/pictures/9'}\n",
            "578 2.96 None 2.96 {'$ref': '#/pictures/9'}\n",
            "579 83-85 None 83-85 {'$ref': '#/pictures/9'}\n",
            "580 n/a None n/a {'$ref': '#/pictures/9'}\n",
            "581 n/a None n/a {'$ref': '#/pictures/9'}\n",
            "582 84-87 None 84-87 {'$ref': '#/pictures/9'}\n",
            "583 86-96 None 86-96 {'$ref': '#/pictures/9'}\n",
            "584 n/a None n/a {'$ref': '#/pictures/9'}\n",
            "585 n/a None n/a {'$ref': '#/pictures/9'}\n",
            "586 List-item None List-item {'$ref': '#/pictures/9'}\n",
            "587 185660 None 185660 {'$ref': '#/pictures/9'}\n",
            "588 17.19 None 17.19 {'$ref': '#/pictures/9'}\n",
            "589 13.34 None 13.34 {'$ref': '#/pictures/9'}\n",
            "590 15.82 None 15.82 {'$ref': '#/pictures/9'}\n",
            "591 87-88 None 87-88 {'$ref': '#/pictures/9'}\n",
            "592 74-83 None 74-83 {'$ref': '#/pictures/9'}\n",
            "593 90-92 None 90-92 {'$ref': '#/pictures/9'}\n",
            "594 97-97 None 97-97 {'$ref': '#/pictures/9'}\n",
            "595 81-85 None 81-85 {'$ref': '#/pictures/9'}\n",
            "596 75-88 None 75-88 {'$ref': '#/pictures/9'}\n",
            "597 93-95 None 93-95 {'$ref': '#/pictures/9'}\n",
            "598 Page-footer None Page-footer {'$ref': '#/pictures/9'}\n",
            "599 70878 None 70878 {'$ref': '#/pictures/9'}\n",
            "600 6.51 None 6.51 {'$ref': '#/pictures/9'}\n",
            "601 5.58 None 5.58 {'$ref': '#/pictures/9'}\n",
            "602 6.00 None 6.00 {'$ref': '#/pictures/9'}\n",
            "603 93-94 None 93-94 {'$ref': '#/pictures/9'}\n",
            "604 88-90 None 88-90 {'$ref': '#/pictures/9'}\n",
            "605 95-96 None 95-96 {'$ref': '#/pictures/9'}\n",
            "606 100 None 100 {'$ref': '#/pictures/9'}\n",
            "607 92-97 None 92-97 {'$ref': '#/pictures/9'}\n",
            "608 100 None 100 {'$ref': '#/pictures/9'}\n",
            "609 96-98 None 96-98 {'$ref': '#/pictures/9'}\n",
            "610 Page-header None Page-header {'$ref': '#/pictures/9'}\n",
            "611 58022 None 58022 {'$ref': '#/pictures/9'}\n",
            "612 5.10 None 5.10 {'$ref': '#/pictures/9'}\n",
            "613 6.70 None 6.70 {'$ref': '#/pictures/9'}\n",
            "614 5.06 None 5.06 {'$ref': '#/pictures/9'}\n",
            "615 85-89 None 85-89 {'$ref': '#/pictures/9'}\n",
            "616 66-76 None 66-76 {'$ref': '#/pictures/9'}\n",
            "617 90-94 None 90-94 {'$ref': '#/pictures/9'}\n",
            "618 98-100 None 98-100 {'$ref': '#/pictures/9'}\n",
            "619 91-92 None 91-92 {'$ref': '#/pictures/9'}\n",
            "620 97-99 None 97-99 {'$ref': '#/pictures/9'}\n",
            "621 81-86 None 81-86 {'$ref': '#/pictures/9'}\n",
            "622 Picture None Picture {'$ref': '#/pictures/9'}\n",
            "623 45976 None 45976 {'$ref': '#/pictures/9'}\n",
            "624 4.21 None 4.21 {'$ref': '#/pictures/9'}\n",
            "625 2.78 None 2.78 {'$ref': '#/pictures/9'}\n",
            "626 5.31 None 5.31 {'$ref': '#/pictures/9'}\n",
            "627 69-71 None 69-71 {'$ref': '#/pictures/9'}\n",
            "628 56-59 None 56-59 {'$ref': '#/pictures/9'}\n",
            "629 82-86 None 82-86 {'$ref': '#/pictures/9'}\n",
            "630 69-82 None 69-82 {'$ref': '#/pictures/9'}\n",
            "631 80-95 None 80-95 {'$ref': '#/pictures/9'}\n",
            "632 66-71 None 66-71 {'$ref': '#/pictures/9'}\n",
            "633 59-76 None 59-76 {'$ref': '#/pictures/9'}\n",
            "634 Section-header None Section-header {'$ref': '#/pictures/9'}\n",
            "635 142884 None 142884 {'$ref': '#/pictures/9'}\n",
            "636 12.60 None 12.60 {'$ref': '#/pictures/9'}\n",
            "637 15.77 None 15.77 {'$ref': '#/pictures/9'}\n",
            "638 12.85 None 12.85 {'$ref': '#/pictures/9'}\n",
            "639 83-84 None 83-84 {'$ref': '#/pictures/9'}\n",
            "640 76-81 None 76-81 {'$ref': '#/pictures/9'}\n",
            "641 90-92 None 90-92 {'$ref': '#/pictures/9'}\n",
            "642 94-95 None 94-95 {'$ref': '#/pictures/9'}\n",
            "643 87-94 None 87-94 {'$ref': '#/pictures/9'}\n",
            "644 69-73 None 69-73 {'$ref': '#/pictures/9'}\n",
            "645 78-86 None 78-86 {'$ref': '#/pictures/9'}\n",
            "646 Table None Table {'$ref': '#/pictures/9'}\n",
            "647 34733 None 34733 {'$ref': '#/pictures/9'}\n",
            "648 3.20 None 3.20 {'$ref': '#/pictures/9'}\n",
            "649 2.27 None 2.27 {'$ref': '#/pictures/9'}\n",
            "650 3.60 None 3.60 {'$ref': '#/pictures/9'}\n",
            "651 77-81 None 77-81 {'$ref': '#/pictures/9'}\n",
            "652 75-80 None 75-80 {'$ref': '#/pictures/9'}\n",
            "653 83-86 None 83-86 {'$ref': '#/pictures/9'}\n",
            "654 98-99 None 98-99 {'$ref': '#/pictures/9'}\n",
            "655 58-80 None 58-80 {'$ref': '#/pictures/9'}\n",
            "656 79-84 None 79-84 {'$ref': '#/pictures/9'}\n",
            "657 70-85 None 70-85 {'$ref': '#/pictures/9'}\n",
            "658 Text None Text {'$ref': '#/pictures/9'}\n",
            "659 510377 None 510377 {'$ref': '#/pictures/9'}\n",
            "660 45.82 None 45.82 {'$ref': '#/pictures/9'}\n",
            "661 49.28 None 49.28 {'$ref': '#/pictures/9'}\n",
            "662 45.00 None 45.00 {'$ref': '#/pictures/9'}\n",
            "663 84-86 None 84-86 {'$ref': '#/pictures/9'}\n",
            "664 81-86 None 81-86 {'$ref': '#/pictures/9'}\n",
            "665 88-93 None 88-93 {'$ref': '#/pictures/9'}\n",
            "666 89-93 None 89-93 {'$ref': '#/pictures/9'}\n",
            "667 87-92 None 87-92 {'$ref': '#/pictures/9'}\n",
            "668 71-79 None 71-79 {'$ref': '#/pictures/9'}\n",
            "669 87-95 None 87-95 {'$ref': '#/pictures/9'}\n",
            "670 Title None Title {'$ref': '#/pictures/9'}\n",
            "671 5071 None 5071 {'$ref': '#/pictures/9'}\n",
            "672 0.47 None 0.47 {'$ref': '#/pictures/9'}\n",
            "673 0.30 None 0.30 {'$ref': '#/pictures/9'}\n",
            "674 0.50 None 0.50 {'$ref': '#/pictures/9'}\n",
            "675 60-72 None 60-72 {'$ref': '#/pictures/9'}\n",
            "676 24-63 None 24-63 {'$ref': '#/pictures/9'}\n",
            "677 50-63 None 50-63 {'$ref': '#/pictures/9'}\n",
            "678 94-100 None 94-100 {'$ref': '#/pictures/9'}\n",
            "679 82-96 None 82-96 {'$ref': '#/pictures/9'}\n",
            "680 68-79 None 68-79 {'$ref': '#/pictures/9'}\n",
            "681 24-56 None 24-56 {'$ref': '#/pictures/9'}\n",
            "682 Total None Total {'$ref': '#/pictures/9'}\n",
            "683 1107470 None 1107470 {'$ref': '#/pictures/9'}\n",
            "684 941123 None 941123 {'$ref': '#/pictures/9'}\n",
            "685 99816 None 99816 {'$ref': '#/pictures/9'}\n",
            "686 66531 None 66531 {'$ref': '#/pictures/9'}\n",
            "687 82-83 None 82-83 {'$ref': '#/pictures/9'}\n",
            "688 71-74 None 71-74 {'$ref': '#/pictures/9'}\n",
            "689 79-81 None 79-81 {'$ref': '#/pictures/9'}\n",
            "690 89-94 None 89-94 {'$ref': '#/pictures/9'}\n",
            "691 86-91 None 86-91 {'$ref': '#/pictures/9'}\n",
            "692 71-76 None 71-76 {'$ref': '#/pictures/9'}\n",
            "693 68-85 None 68-85 {'$ref': '#/pictures/9'}\n",
            "694 B None B {'$ref': '#/pictures/10'}\n",
            "695 include publication repositories such as arXiv None include publication repositories such as arXiv {'$ref': '#/body'}\n",
            "696 Table 1: DocLayNet dataset overview. Along with the frequency of each class label, we present the relative occurrence (as % of row \"Total\") in the train, test and validation sets. The inter-annotator agreement is computed as the mAP@0.5-0.95 metric between pairwise annotations from the triple- None Table 1: DocLayNet dataset overview. Along with the frequency of each class label, we present the relative occurrence (as % of row \"Total\") in the train, test and validation sets. The inter-annotator agreement is computed as the mAP@0.5-0.95 metric between pairwise annotations from the triple- {'$ref': '#/body'}\n",
            "697 annotated pages, from which we obtain accuracy ranges. None annotated pages, from which we obtain accuracy ranges. {'$ref': '#/body'}\n",
            "698 % of None % of {'$ref': '#/pictures/11'}\n",
            "699 Total None Total {'$ref': '#/pictures/11'}\n",
            "700 % of None % of {'$ref': '#/pictures/11'}\n",
            "701 Total None Total {'$ref': '#/pictures/11'}\n",
            "702 % of None % of {'$ref': '#/pictures/11'}\n",
            "703 Total None Total {'$ref': '#/pictures/11'}\n",
            "704 triple None triple {'$ref': '#/pictures/11'}\n",
            "705 inter- None inter- {'$ref': '#/pictures/11'}\n",
            "706 annotator None annotator {'$ref': '#/pictures/11'}\n",
            "707 mAP @ None mAP @ {'$ref': '#/pictures/11'}\n",
            "708 0.5-0.95 None 0.5-0.95 {'$ref': '#/pictures/11'}\n",
            "709 (%) None (%) {'$ref': '#/pictures/11'}\n",
            "710 triple None triple {'$ref': '#/pictures/11'}\n",
            "711 inter- None inter- {'$ref': '#/pictures/11'}\n",
            "712 annotator None annotator {'$ref': '#/pictures/11'}\n",
            "713 mAP @ None mAP @ {'$ref': '#/pictures/11'}\n",
            "714 0.5-0.95 None 0.5-0.95 {'$ref': '#/pictures/11'}\n",
            "715 (%) None (%) {'$ref': '#/pictures/11'}\n",
            "716 triple None triple {'$ref': '#/pictures/11'}\n",
            "717 inter- None inter- {'$ref': '#/pictures/11'}\n",
            "718 annotator None annotator {'$ref': '#/pictures/11'}\n",
            "719 mAP @ None mAP @ {'$ref': '#/pictures/11'}\n",
            "720 0.5-0.95 None 0.5-0.95 {'$ref': '#/pictures/11'}\n",
            "721 (%) None (%) {'$ref': '#/pictures/11'}\n",
            "722 triple None triple {'$ref': '#/pictures/11'}\n",
            "723 inter- None inter- {'$ref': '#/pictures/11'}\n",
            "724 annotator None annotator {'$ref': '#/pictures/11'}\n",
            "725 mAP @ None mAP @ {'$ref': '#/pictures/11'}\n",
            "726 0.5-0.95 None 0.5-0.95 {'$ref': '#/pictures/11'}\n",
            "727 (%) None (%) {'$ref': '#/pictures/11'}\n",
            "728 triple None triple {'$ref': '#/pictures/11'}\n",
            "729 inter- None inter- {'$ref': '#/pictures/11'}\n",
            "730 annotator None annotator {'$ref': '#/pictures/11'}\n",
            "731 mAP @ None mAP @ {'$ref': '#/pictures/11'}\n",
            "732 0.5-0.95 None 0.5-0.95 {'$ref': '#/pictures/11'}\n",
            "733 (%) None (%) {'$ref': '#/pictures/11'}\n",
            "734 triple None triple {'$ref': '#/pictures/11'}\n",
            "735 inter- None inter- {'$ref': '#/pictures/11'}\n",
            "736 annotator None annotator {'$ref': '#/pictures/11'}\n",
            "737 mAP @ None mAP @ {'$ref': '#/pictures/11'}\n",
            "738 0.5-0.95 None 0.5-0.95 {'$ref': '#/pictures/11'}\n",
            "739 (%) None (%) {'$ref': '#/pictures/11'}\n",
            "740 triple None triple {'$ref': '#/pictures/11'}\n",
            "741 inter- None inter- {'$ref': '#/pictures/11'}\n",
            "742 annotator None annotator {'$ref': '#/pictures/11'}\n",
            "743 mAP @ None mAP @ {'$ref': '#/pictures/11'}\n",
            "744 0.5-0.95 None 0.5-0.95 {'$ref': '#/pictures/11'}\n",
            "745 (%) None (%) {'$ref': '#/pictures/11'}\n",
            "746 class None class {'$ref': '#/pictures/11'}\n",
            "747 label None label {'$ref': '#/pictures/11'}\n",
            "748 Count None Count {'$ref': '#/pictures/11'}\n",
            "749 Train None Train {'$ref': '#/pictures/11'}\n",
            "750 Test None Test {'$ref': '#/pictures/11'}\n",
            "751 Val None Val {'$ref': '#/pictures/11'}\n",
            "752 All None All {'$ref': '#/pictures/11'}\n",
            "753 Fin None Fin {'$ref': '#/pictures/11'}\n",
            "754 Man None Man {'$ref': '#/pictures/11'}\n",
            "755 Sci None Sci {'$ref': '#/pictures/11'}\n",
            "756 Law None Law {'$ref': '#/pictures/11'}\n",
            "757 Pat None Pat {'$ref': '#/pictures/11'}\n",
            "758 Ten None Ten {'$ref': '#/pictures/11'}\n",
            "759 Caption None Caption {'$ref': '#/pictures/11'}\n",
            "760 22524 None 22524 {'$ref': '#/pictures/11'}\n",
            "761 2.04 None 2.04 {'$ref': '#/pictures/11'}\n",
            "762 1.77 None 1.77 {'$ref': '#/pictures/11'}\n",
            "763 2.32 None 2.32 {'$ref': '#/pictures/11'}\n",
            "764 84-89 None 84-89 {'$ref': '#/pictures/11'}\n",
            "765 40-61 None 40-61 {'$ref': '#/pictures/11'}\n",
            "766 86-92 None 86-92 {'$ref': '#/pictures/11'}\n",
            "767 94-99 None 94-99 {'$ref': '#/pictures/11'}\n",
            "768 95-99 None 95-99 {'$ref': '#/pictures/11'}\n",
            "769 69-78 None 69-78 {'$ref': '#/pictures/11'}\n",
            "770 n/a None n/a {'$ref': '#/pictures/11'}\n",
            "771 Footnote None Footnote {'$ref': '#/pictures/11'}\n",
            "772 6318 None 6318 {'$ref': '#/pictures/11'}\n",
            "773 0.60 None 0.60 {'$ref': '#/pictures/11'}\n",
            "774 0.31 None 0.31 {'$ref': '#/pictures/11'}\n",
            "775 0.58 None 0.58 {'$ref': '#/pictures/11'}\n",
            "776 83-91 None 83-91 {'$ref': '#/pictures/11'}\n",
            "777 n/a None n/a {'$ref': '#/pictures/11'}\n",
            "778 100 None 100 {'$ref': '#/pictures/11'}\n",
            "779 62-88 None 62-88 {'$ref': '#/pictures/11'}\n",
            "780 85-94 None 85-94 {'$ref': '#/pictures/11'}\n",
            "781 n/a None n/a {'$ref': '#/pictures/11'}\n",
            "782 82-97 None 82-97 {'$ref': '#/pictures/11'}\n",
            "783 Formula None Formula {'$ref': '#/pictures/11'}\n",
            "784 25027 None 25027 {'$ref': '#/pictures/11'}\n",
            "785 2.25 None 2.25 {'$ref': '#/pictures/11'}\n",
            "786 1.90 None 1.90 {'$ref': '#/pictures/11'}\n",
            "787 2.96 None 2.96 {'$ref': '#/pictures/11'}\n",
            "788 83-85 None 83-85 {'$ref': '#/pictures/11'}\n",
            "789 n/a None n/a {'$ref': '#/pictures/11'}\n",
            "790 n/a None n/a {'$ref': '#/pictures/11'}\n",
            "791 84-87 None 84-87 {'$ref': '#/pictures/11'}\n",
            "792 86-96 None 86-96 {'$ref': '#/pictures/11'}\n",
            "793 n/a None n/a {'$ref': '#/pictures/11'}\n",
            "794 n/a None n/a {'$ref': '#/pictures/11'}\n",
            "795 List-item None List-item {'$ref': '#/pictures/11'}\n",
            "796 185660 None 185660 {'$ref': '#/pictures/11'}\n",
            "797 17.19 None 17.19 {'$ref': '#/pictures/11'}\n",
            "798 13.34 None 13.34 {'$ref': '#/pictures/11'}\n",
            "799 15.82 None 15.82 {'$ref': '#/pictures/11'}\n",
            "800 87-88 None 87-88 {'$ref': '#/pictures/11'}\n",
            "801 74-83 None 74-83 {'$ref': '#/pictures/11'}\n",
            "802 90-92 None 90-92 {'$ref': '#/pictures/11'}\n",
            "803 97-97 None 97-97 {'$ref': '#/pictures/11'}\n",
            "804 81-85 None 81-85 {'$ref': '#/pictures/11'}\n",
            "805 75-88 None 75-88 {'$ref': '#/pictures/11'}\n",
            "806 93-95 None 93-95 {'$ref': '#/pictures/11'}\n",
            "807 Page- None Page- {'$ref': '#/pictures/11'}\n",
            "808 footer None footer {'$ref': '#/pictures/11'}\n",
            "809 70878 None 70878 {'$ref': '#/pictures/11'}\n",
            "810 6.51 None 6.51 {'$ref': '#/pictures/11'}\n",
            "811 5.58 None 5.58 {'$ref': '#/pictures/11'}\n",
            "812 6.00 None 6.00 {'$ref': '#/pictures/11'}\n",
            "813 93-94 None 93-94 {'$ref': '#/pictures/11'}\n",
            "814 88-90 None 88-90 {'$ref': '#/pictures/11'}\n",
            "815 95-96 None 95-96 {'$ref': '#/pictures/11'}\n",
            "816 100 None 100 {'$ref': '#/pictures/11'}\n",
            "817 92-97 None 92-97 {'$ref': '#/pictures/11'}\n",
            "818 100 None 100 {'$ref': '#/pictures/11'}\n",
            "819 96-98 None 96-98 {'$ref': '#/pictures/11'}\n",
            "820 Page- None Page- {'$ref': '#/pictures/11'}\n",
            "821 header None header {'$ref': '#/pictures/11'}\n",
            "822 58022 None 58022 {'$ref': '#/pictures/11'}\n",
            "823 5.10 None 5.10 {'$ref': '#/pictures/11'}\n",
            "824 6.70 None 6.70 {'$ref': '#/pictures/11'}\n",
            "825 5.06 None 5.06 {'$ref': '#/pictures/11'}\n",
            "826 85-89 None 85-89 {'$ref': '#/pictures/11'}\n",
            "827 66-76 None 66-76 {'$ref': '#/pictures/11'}\n",
            "828 90-94 None 90-94 {'$ref': '#/pictures/11'}\n",
            "829 98-100 None 98-100 {'$ref': '#/pictures/11'}\n",
            "830 91-92 None 91-92 {'$ref': '#/pictures/11'}\n",
            "831 97-99 None 97-99 {'$ref': '#/pictures/11'}\n",
            "832 81-86 None 81-86 {'$ref': '#/pictures/11'}\n",
            "833 Picture None Picture {'$ref': '#/pictures/11'}\n",
            "834 45976 None 45976 {'$ref': '#/pictures/11'}\n",
            "835 4.21 None 4.21 {'$ref': '#/pictures/11'}\n",
            "836 2.78 None 2.78 {'$ref': '#/pictures/11'}\n",
            "837 5.31 None 5.31 {'$ref': '#/pictures/11'}\n",
            "838 69-71 None 69-71 {'$ref': '#/pictures/11'}\n",
            "839 56-59 None 56-59 {'$ref': '#/pictures/11'}\n",
            "840 82-86 None 82-86 {'$ref': '#/pictures/11'}\n",
            "841 69-82 None 69-82 {'$ref': '#/pictures/11'}\n",
            "842 80-95 None 80-95 {'$ref': '#/pictures/11'}\n",
            "843 66-71 None 66-71 {'$ref': '#/pictures/11'}\n",
            "844 59-76 None 59-76 {'$ref': '#/pictures/11'}\n",
            "845 Section- None Section- {'$ref': '#/pictures/11'}\n",
            "846 header None header {'$ref': '#/pictures/11'}\n",
            "847 142884 None 142884 {'$ref': '#/pictures/11'}\n",
            "848 12.60 None 12.60 {'$ref': '#/pictures/11'}\n",
            "849 15.77 None 15.77 {'$ref': '#/pictures/11'}\n",
            "850 12.85 None 12.85 {'$ref': '#/pictures/11'}\n",
            "851 83-84 None 83-84 {'$ref': '#/pictures/11'}\n",
            "852 76-81 None 76-81 {'$ref': '#/pictures/11'}\n",
            "853 90-92 None 90-92 {'$ref': '#/pictures/11'}\n",
            "854 94-95 None 94-95 {'$ref': '#/pictures/11'}\n",
            "855 87-94 None 87-94 {'$ref': '#/pictures/11'}\n",
            "856 69-73 None 69-73 {'$ref': '#/pictures/11'}\n",
            "857 78-86 None 78-86 {'$ref': '#/pictures/11'}\n",
            "858 Table None Table {'$ref': '#/pictures/11'}\n",
            "859 34733 None 34733 {'$ref': '#/pictures/11'}\n",
            "860 3.20 None 3.20 {'$ref': '#/pictures/11'}\n",
            "861 2.27 None 2.27 {'$ref': '#/pictures/11'}\n",
            "862 3.60 None 3.60 {'$ref': '#/pictures/11'}\n",
            "863 77-81 None 77-81 {'$ref': '#/pictures/11'}\n",
            "864 75-80 None 75-80 {'$ref': '#/pictures/11'}\n",
            "865 83-86 None 83-86 {'$ref': '#/pictures/11'}\n",
            "866 98-99 None 98-99 {'$ref': '#/pictures/11'}\n",
            "867 58-80 None 58-80 {'$ref': '#/pictures/11'}\n",
            "868 79-84 None 79-84 {'$ref': '#/pictures/11'}\n",
            "869 70-85 None 70-85 {'$ref': '#/pictures/11'}\n",
            "870 Text None Text {'$ref': '#/pictures/11'}\n",
            "871 510377 None 510377 {'$ref': '#/pictures/11'}\n",
            "872 45.82 None 45.82 {'$ref': '#/pictures/11'}\n",
            "873 49.28 None 49.28 {'$ref': '#/pictures/11'}\n",
            "874 45.00 None 45.00 {'$ref': '#/pictures/11'}\n",
            "875 84-86 None 84-86 {'$ref': '#/pictures/11'}\n",
            "876 81-86 None 81-86 {'$ref': '#/pictures/11'}\n",
            "877 88-93 None 88-93 {'$ref': '#/pictures/11'}\n",
            "878 89-93 None 89-93 {'$ref': '#/pictures/11'}\n",
            "879 87-92 None 87-92 {'$ref': '#/pictures/11'}\n",
            "880 71-79 None 71-79 {'$ref': '#/pictures/11'}\n",
            "881 87-95 None 87-95 {'$ref': '#/pictures/11'}\n",
            "882 Title None Title {'$ref': '#/pictures/11'}\n",
            "883 5071 None 5071 {'$ref': '#/pictures/11'}\n",
            "884 0.47 None 0.47 {'$ref': '#/pictures/11'}\n",
            "885 0.30 None 0.30 {'$ref': '#/pictures/11'}\n",
            "886 0.50 None 0.50 {'$ref': '#/pictures/11'}\n",
            "887 60-72 None 60-72 {'$ref': '#/pictures/11'}\n",
            "888 24-63 None 24-63 {'$ref': '#/pictures/11'}\n",
            "889 50-63 None 50-63 {'$ref': '#/pictures/11'}\n",
            "890 94-100 None 94-100 {'$ref': '#/pictures/11'}\n",
            "891 82-96 None 82-96 {'$ref': '#/pictures/11'}\n",
            "892 68-79 None 68-79 {'$ref': '#/pictures/11'}\n",
            "893 24-56 None 24-56 {'$ref': '#/pictures/11'}\n",
            "894 Total None Total {'$ref': '#/pictures/11'}\n",
            "895 1107470 None 1107470 {'$ref': '#/pictures/11'}\n",
            "896 941123 None 941123 {'$ref': '#/pictures/11'}\n",
            "897 99816 None 99816 {'$ref': '#/pictures/11'}\n",
            "898 66531 None 66531 {'$ref': '#/pictures/11'}\n",
            "899 82-83 None 82-83 {'$ref': '#/pictures/11'}\n",
            "900 71-74 None 71-74 {'$ref': '#/pictures/11'}\n",
            "901 79-81 None 79-81 {'$ref': '#/pictures/11'}\n",
            "902 89-94 None 89-94 {'$ref': '#/pictures/11'}\n",
            "903 86-91 None 86-91 {'$ref': '#/pictures/11'}\n",
            "904 71-76 None 71-76 {'$ref': '#/pictures/11'}\n",
            "905 68-85 None 68-85 {'$ref': '#/pictures/11'}\n",
            "906 3 None 3 {'$ref': '#/body'}\n",
            "907 , None , {'$ref': '#/body'}\n",
            "908 government offices, None government offices, {'$ref': '#/body'}\n",
            "909 We reviewed the col- None We reviewed the col- {'$ref': '#/body'}\n",
            "910 , None , {'$ref': '#/body'}\n",
            "911 Page- None Page- {'$ref': '#/body'}\n",
            "912 Title and None Title and {'$ref': '#/body'}\n",
            "913 . None . {'$ref': '#/body'}\n",
            "914 page. Specificity ensures that the choice of label is not ambiguous, None page. Specificity ensures that the choice of label is not ambiguous, {'$ref': '#/body'}\n",
            "915 tables\" None tables\" {'$ref': '#/pictures/12'}\n",
            "916 \"#~cols\" None \"#~cols\" {'$ref': '#/pictures/12'}\n",
            "917 14 None 14 {'$ref': '#/pictures/12'}\n",
            "918 data\" None data\" {'$ref': '#/pictures/12'}\n",
            "919 \"bbox\" None \"bbox\" {'$ref': '#/pictures/12'}\n",
            "920 10 None 10 {'$ref': '#/pictures/12'}\n",
            "921 12 None 12 {'$ref': '#/pictures/12'}\n",
            "922 13 None 13 {'$ref': '#/pictures/12'}\n",
            "923 14 None 14 {'$ref': '#/pictures/12'}\n",
            "924 spans None spans {'$ref': '#/pictures/12'}\n",
            "925 [ None [ {'$ref': '#/pictures/12'}\n",
            "926 28 None 28 {'$ref': '#/pictures/12'}\n",
            "927 text\" None text\" {'$ref': '#/pictures/12'}\n",
            "928 29 None 29 {'$ref': '#/pictures/12'}\n",
            "929 col\" None col\" {'$ref': '#/pictures/12'}\n",
            "930 30 None 30 {'$ref': '#/pictures/12'}\n",
            "931 \"col-span\" None \"col-span\" {'$ref': '#/pictures/12'}\n",
            "932 Figure 3: Corpus Conversion Service annotation user inter- None Figure 3: Corpus Conversion Service annotation user inter- {'$ref': '#/pictures/12'}\n",
            "933 face. The PDF page is shown in the background, with over- None face. The PDF page is shown in the background, with over- {'$ref': '#/pictures/12'}\n",
            "934 laid text-cells (in darker shades). The annotation boxes can None laid text-cells (in darker shades). The annotation boxes can {'$ref': '#/pictures/12'}\n",
            "935 be drawn by dragging a rectangle over each segment with None be drawn by dragging a rectangle over each segment with {'$ref': '#/pictures/12'}\n",
            "936 the respective label from the palette on the right. None the respective label from the palette on the right. {'$ref': '#/pictures/12'}\n",
            "937 company websites as well as data directory services for financial None company websites as well as data directory services for financial {'$ref': '#/pictures/12'}\n",
            "938 reports and patents. Scanned documents were excluded wherever None reports and patents. Scanned documents were excluded wherever {'$ref': '#/pictures/12'}\n",
            "939 possible because they can be rotated or skewed. This would not None possible because they can be rotated or skewed. This would not {'$ref': '#/pictures/12'}\n",
            "940 allow us to perform annotation with rectangular bounding-boxes None allow us to perform annotation with rectangular bounding-boxes {'$ref': '#/pictures/12'}\n",
            "941 and therefore complicate the annotation process. None and therefore complicate the annotation process. {'$ref': '#/pictures/12'}\n",
            "942 Preparation work included uploading and parsing the sourced None Preparation work included uploading and parsing the sourced {'$ref': '#/pictures/12'}\n",
            "943 PDF documents in the Corpus Conversion Service (CCS) [22], a None PDF documents in the Corpus Conversion Service (CCS) [22], a {'$ref': '#/pictures/12'}\n",
            "944 cloud-native platform which provides a visual annotation interface None cloud-native platform which provides a visual annotation interface {'$ref': '#/pictures/12'}\n",
            "945 and allows for dataset inspection and analysis. The annotation in- None and allows for dataset inspection and analysis. The annotation in- {'$ref': '#/pictures/12'}\n",
            "946 terface of CCS is shown in Figure 3. The desired balance of pages None terface of CCS is shown in Figure 3. The desired balance of pages {'$ref': '#/pictures/12'}\n",
            "947 between the different document categories was achieved by se- None between the different document categories was achieved by se- {'$ref': '#/pictures/12'}\n",
            "948 lective subsampling of pages with certain desired properties. For None lective subsampling of pages with certain desired properties. For {'$ref': '#/pictures/12'}\n",
            "949 example, we made sure to include the title page of each document None example, we made sure to include the title page of each document {'$ref': '#/pictures/12'}\n",
            "950 and bias the remaining page selection to those with figures or None and bias the remaining page selection to those with figures or {'$ref': '#/pictures/12'}\n",
            "951 tables. The latter was achieved by leveraging pre-trained object None tables. The latter was achieved by leveraging pre-trained object {'$ref': '#/pictures/12'}\n",
            "952 detection models from PubLayNet, which helped us estimate how None detection models from PubLayNet, which helped us estimate how {'$ref': '#/pictures/12'}\n",
            "953 many figures and tables a given page contains. None many figures and tables a given page contains. {'$ref': '#/pictures/12'}\n",
            "954 Phase 2: Label selection and guideline. None Phase 2: Label selection and guideline. {'$ref': '#/pictures/12'}\n",
            "955 lected documents and identified the most common structural fea- None lected documents and identified the most common structural fea- {'$ref': '#/pictures/12'}\n",
            "956 tures they exhibit. This was achieved by identifying recurrent layout None tures they exhibit. This was achieved by identifying recurrent layout {'$ref': '#/pictures/12'}\n",
            "957 elements and lead us to the definition of 11 distinct class labels. None elements and lead us to the definition of 11 distinct class labels. {'$ref': '#/pictures/12'}\n",
            "958 These 11 class labels are None These 11 class labels are {'$ref': '#/pictures/12'}\n",
            "959 Caption None Caption {'$ref': '#/pictures/12'}\n",
            "960 , None , {'$ref': '#/pictures/12'}\n",
            "961 Footnote None Footnote {'$ref': '#/pictures/12'}\n",
            "962 , None , {'$ref': '#/pictures/12'}\n",
            "963 Formula List-item None Formula List-item {'$ref': '#/pictures/12'}\n",
            "964 , None , {'$ref': '#/pictures/12'}\n",
            "965 footer None footer {'$ref': '#/pictures/12'}\n",
            "966 , None , {'$ref': '#/pictures/12'}\n",
            "967 Page-header None Page-header {'$ref': '#/pictures/12'}\n",
            "968 , None , {'$ref': '#/pictures/12'}\n",
            "969 Picture None Picture {'$ref': '#/pictures/12'}\n",
            "970 , None , {'$ref': '#/pictures/12'}\n",
            "971 Section-header None Section-header {'$ref': '#/pictures/12'}\n",
            "972 , None , {'$ref': '#/pictures/12'}\n",
            "973 Table None Table {'$ref': '#/pictures/12'}\n",
            "974 , None , {'$ref': '#/pictures/12'}\n",
            "975 Text None Text {'$ref': '#/pictures/12'}\n",
            "976 , None , {'$ref': '#/pictures/12'}\n",
            "977 Critical factors that were considered for the choice of these class None Critical factors that were considered for the choice of these class {'$ref': '#/pictures/12'}\n",
            "978 labels were (1) the overall occurrence of the label, (2) the specificity None labels were (1) the overall occurrence of the label, (2) the specificity {'$ref': '#/pictures/12'}\n",
            "979 of the label, (3) recognisability on a single page (i.e. no need for None of the label, (3) recognisability on a single page (i.e. no need for {'$ref': '#/pictures/12'}\n",
            "980 context from previous or next page) and (4) overall coverage of the None context from previous or next page) and (4) overall coverage of the {'$ref': '#/pictures/12'}\n",
            "981 C None C {'$ref': '#/pictures/12'}\n",
            "982 we distributed the annotation workload and performed continuous be annotated. We refrained from class labels that are very specific None we distributed the annotation workload and performed continuous be annotated. We refrained from class labels that are very specific {'$ref': '#/body'}\n",
            "983 only. For phases three and four, a group of 40 dedicated annotators while coverage ensures that all meaningful items on a page can None only. For phases three and four, a group of 40 dedicated annotators while coverage ensures that all meaningful items on a page can {'$ref': '#/body'}\n",
            "984 quality controls. Phase one and two required a small team of experts to a document category, such as None quality controls. Phase one and two required a small team of experts to a document category, such as {'$ref': '#/body'}\n",
            "985 Abstract in the None Abstract in the {'$ref': '#/body'}\n",
            "986 Scientific Articles were assembled and supervised. None Scientific Articles were assembled and supervised. {'$ref': '#/body'}\n",
            "987 category. We also avoided class labels that are tightly linked to the None category. We also avoided class labels that are tightly linked to the {'$ref': '#/body'}\n",
            "988 Phase 1: Data selection and preparation. None Phase 1: Data selection and preparation. {'$ref': '#/body'}\n",
            "989 Our inclusion cri- None Our inclusion cri- {'$ref': '#/body'}\n",
            "990 Author None Author {'$ref': '#/body'}\n",
            "991 Affiliation None Affiliation {'$ref': '#/body'}\n",
            "992 teria for documents were described in Section 3. A large effort went into ensuring that all documents are free to use. The data sources in DocBank, are often only distinguishable by discriminating on 3 https://arxiv.org/ Figure 4: Table 1 from the DocLayNet paper in the original PDF (A), as rendered Markdown (B) and in JSON representation (C). Spanning table cells, such as the multi-column header 'triple interannotator mAP@0.5-0.95 (%)', is repeated for each column in the Markdown representation (B), which guarantees that every data point can be traced back to row and column headings only by its grid coordinates in the table. In the JSON representation, the span information is reflected in the fields of each table cell (C). None teria for documents were described in Section 3. A large effort went into ensuring that all documents are free to use. The data sources in DocBank, are often only distinguishable by discriminating on 3 https://arxiv.org/ Figure 4: Table 1 from the DocLayNet paper in the original PDF (A), as rendered Markdown (B) and in JSON representation (C). Spanning table cells, such as the multi-column header 'triple interannotator mAP@0.5-0.95 (%)', is repeated for each column in the Markdown representation (B), which guarantees that every data point can be traced back to row and column headings only by its grid coordinates in the table. In the JSON representation, the span information is reflected in the fields of each table cell (C). {'$ref': '#/body'}\n",
            "993 semantics of the text. Labels such as and None semantics of the text. Labels such as and {'$ref': '#/body'}\n",
            "994 , None , {'$ref': '#/body'}\n",
            "995 as seen None as seen {'$ref': '#/body'}\n",
            "996 9 None 9 {'$ref': '#/body'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EDdQH633Optx"
      },
      "id": "EDdQH633Optx",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}